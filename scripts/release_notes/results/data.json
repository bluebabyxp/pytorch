{"88232b4cee": {"title": "Fix ENABLE_RECORD_KERNEL_FUNCTION_DTYPE build (#65370)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65370\n\nForgot a wrapping 'namespace at' here!  And no contbuilds to test it.\nghstack-source-id: 138565579\n\nTest Plan:\n```\nbuck build --show-output -c pt.disable_per_op_profiling=0 -c pt.enable_record_kernel_dtype=1 -c pt.has_backtraces=1 fbsource//xplat/caffe2/fb/model_tracer:model_tracer\n```\n\nReviewed By: JacobSzwejbka\n\nDifferential Revision: D31065923\n\nfbshipit-source-id: ed4563fbd8f3c29f6b10ac8999c9010bd4359c97", "pr_number": "65370", "files_changed": ["aten/src/ATen/Dispatch.h", "tools/build_variables.bzl"], "labels": ["Merged", "cla signed", "ciflow/default"]}, "6d7bc34b67": {"title": "Make new_empty/new_ones/new_zeros/new_full respect subclass (#65169)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65169\n\nPreviously these composite functions created a new tensor\nusing at::empty (or some other factory function) using TensorOptions\nwhich doesn't preserve Python subclass.  Making new_empty a\nnon-composite op and then routing everyone through it makes it\nrespect subclass.  We could also make all of these non-composite\nbut this reduces the number of derivatives.yaml entries I have to\nmake and allows you to trace the fill calls.\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nReviewed By: albanD\n\nDifferential Revision: D31003713\n\nPulled By: ezyang\n\nfbshipit-source-id: 19f906f1404a6b724769c49f48d123f407a561ff", "pr_number": "65169", "files_changed": ["aten/src/ATen/native/TensorFactories.cpp", "aten/src/ATen/native/native_functions.yaml", "test/test_python_dispatch.py", "tools/autograd/derivatives.yaml"], "labels": ["Merged", "cla signed", "ciflow/default"]}, "00b732e98b": {"title": "Remove orphan from cuDNN persistent note (#65160)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/60009.\n\nAs the document is properly [included](https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/rnn.py#L799), and [`:orphan:` doesn't need to be used in included documents](https://github.com/sphinx-doc/sphinx/issues/6787#issuecomment-549256840), and no warning is emitted in my local build when removing it, I think it can be removed.\n\nThe artifact reported in https://github.com/pytorch/pytorch/issues/60009 can be seen in 3 pages: [torch.nn.RNN](https://pytorch.org/docs/stable/generated/torch.nn.RNN.html#torch.nn.RNN), [torch.nn.LSTM](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html#torch.nn.LSTM), and [torch.nn.GRU](https://pytorch.org/docs/stable/generated/torch.nn.GRU.html#torch.nn.GRU).\n\ncc ezyang suo\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65160\n\nReviewed By: bdhirsh\n\nDifferential Revision: D31020280\n\nPulled By: ezyang\n\nfbshipit-source-id: 6c3541e5a856a91cf1ce1d2db4d04f5d13118ee4", "pr_number": "65160", "files_changed": ["docs/source/cudnn_persistent_rnn.rst"], "labels": ["open source", "Merged", "cla signed"]}, "28bfdbb066": {"title": "OpInfo for `nn.functional.batch_norm` (#63218)", "body": "Summary:\nAddresses https://github.com/facebookresearch/functorch/issues/78 and https://github.com/pytorch/pytorch/issues/54261.\n\n* There exists `torch.batch_norm` but it takes an extra arg: `cudnn_enabled` (not there in functional variant). This is passed from the functional variant to `torch.batch_norm` here: https://github.com/pytorch/pytorch/blob/master/torch/nn/functional.py#L2282. `test_variant_consistency_jit` fails with an error: (when passed an alias)\n    ```python\n    File \"/home/krshrimali/Documents/Projects/Quansight/pytorch/test/test_ops.py\", line 457, in _test_consistency_helper\n    variant_forward = variant(cloned,\n    TypeError: batch_norm() missing 1 required positional arguments: \"cudnn_enabled\"\n    ```\n    * I'm not sure of a solution to this, as AFIK - there is no way to pass a lambda wrapper for an alias. Hence, I've skipped adding this as an alias there.\n    * On second thought, is this even an alias?\n\ncc: mruberry zou3519 kshitij12345\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/63218\n\nReviewed By: bdhirsh\n\nDifferential Revision: D31019785\n\nPulled By: zou3519\n\nfbshipit-source-id: 2a834d05835da975289efc544a7ad7e98c99438f", "pr_number": "63218", "files_changed": ["torch/testing/_internal/common_device_type.py", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["triaged", "open source", "Merged", "cla signed", "module: testing", "ci/windows", "ci/master", "ciflow/slow-gradcheck", "ciflow/scheduled"]}, "8bab468943": {"title": "Reduce test size for max_pool (#65336)", "body": "Summary:\nFixe OOM in slow gradcheck tests.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65336\n\nReviewed By: malfet\n\nDifferential Revision: D31059007\n\nPulled By: albanD\n\nfbshipit-source-id: 2dd6967d88663558e37f8c0836ad33333c92dfb5", "pr_number": "65336", "files_changed": ["torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "ciflow/slow-gradcheck"]}, "1fec9cd76b": {"title": "[Fixed] Enable Half, BFloat16, and Complex dtypes for coo-coo sparse matmul [CUDA] (#59980)", "body": "Summary:\nThis PR enables Half, BFloat16, ComplexFloat, and ComplexDouble support for matrix-matrix multiplication of COO sparse matrices.\nThe change is applied only to CUDA 11+ builds.\n\n`cusparseSpGEMM` also supports `CUDA_C_16F` (complex float16) and `CUDA_C_16BF` (complex bfloat16). PyTorch also supports the complex float16 dtype (`ScalarType::ComplexHalf`), but there is no convenient dispatch, so this dtype is omitted in this PR.\n\ncc nikitaved pearu cpuhrsch IvanYashchuk ezyang anjali411 dylanbespalko mruberry Lezcano\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/59980\n\nReviewed By: ngimel\n\nDifferential Revision: D30994115\n\nPulled By: cpuhrsch\n\nfbshipit-source-id: 4f55b99e8e25079d6273b4edf95ad6fa85aeaf24", "pr_number": "59980", "files_changed": ["aten/src/ATen/cuda/CUDADataType.h", "aten/src/ATen/native/sparse/cuda/SparseMatMul.cu", "test/test_sparse.py", "torch/testing/_internal/common_cuda.py", "torch/utils/hipify/cuda_to_hip_mappings.py"], "labels": ["module: sparse", "triaged", "module: complex", "open source", "Merged", "cla signed", "ciflow/default", "with-ssh", "ciflow/win"]}, "3f5f721ab3": {"title": "Pass through allow-list from prepare_qat into propagate_qconfig_ to allow custom mapping and custom QAT module (#65119)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65119\n\nPytorch Quantization: allow prepare_qat to include custom module by passing allow_list into the prepare_qat.\n\nWhen we are implementing custom module and custom mapping for Quantization Aware Training (QAT), we need to add the custom module to the mappings and to the allow_list during prepare_qat. The allow_list needs to be surfaced to the  propagate_qconfig_.\n\nTest Plan: relying on general unit test\n\nReviewed By: supriyar\n\nDifferential Revision: D30982060\n\nfbshipit-source-id: 1114115b6a3b853238d33d72b5cbaafc60f463e0", "pr_number": "65119", "files_changed": ["torch/ao/quantization/quantize.py"], "labels": ["fb-exported", "Merged", "cla signed"]}, "f90d9b48db": {"title": "test_neg_view: preseve sign of sample input (#63010)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/63010\n\nThis changes `test_neg_view` to call the operator with the same numeric values as the original sample input.\n\nTest Plan: Imported from OSS\n\nReviewed By: pbelevich\n\nDifferential Revision: D31082824\n\nPulled By: anjali411\n\nfbshipit-source-id: 7d50f99dc0d1343247e366cbe9b0ca081bd0a9b1", "pr_number": "63010", "files_changed": ["test/test_ops.py", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["open source", "Merged", "cla signed", "ciflow/default"]}, "9324d682fd": {"title": "Fix autograd engine checks and update InputMetadata (#65235)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65235\n\n1. Updated the legacy type checks in `torch/csrc/autograd/engine.cpp` to individually validate the dtype, device, and layout equality for grad and tensor.\n2. Removed device field from `InputMetadata` since it's already stored via storing options. Also, added `dtype()` and `layout()` methods to `InputMetadata`. To make this change, some calls had to be updated due to the change in constructor.\n3. To fix https://github.com/pytorch/pytorch/issues/65016:\n     a. Added a `is_tensor_subclass` field in `InputMetadata` to skip device checks for grad and tensor when the tensor has\n         python key set on it (tensor subclass).\n\nTest Plan: Imported from OSS\n\nReviewed By: pbelevich\n\nDifferential Revision: D31082693\n\nPulled By: anjali411\n\nfbshipit-source-id: cb551cd438c6ca40b0f18a4d0009e0861cf0fd4e", "pr_number": "65235", "files_changed": ["test/test_python_dispatch.py", "torch/csrc/autograd/engine.cpp", "torch/csrc/autograd/function.h", "torch/csrc/autograd/input_metadata.h", "torch/csrc/autograd/variable.cpp"], "labels": ["Merged", "cla signed"]}, "bcc6e3ab5e": {"title": "add python API to print all operators that have kernels registered to a particular DispatchKey (#63575)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/63575\n\nTest Plan: Imported from OSS\n\nReviewed By: ezyang, Chillee\n\nDifferential Revision: D30426919\n\nPulled By: bdhirsh\n\nfbshipit-source-id: b0e487e48dfe02f7b9d678403f0a2b5bfe146f4e", "pr_number": "63575", "files_changed": ["aten/src/ATen/core/dispatch/Dispatcher.cpp", "aten/src/ATen/core/dispatch/Dispatcher.h", "aten/src/ATen/core/dispatch/OperatorEntry.h", "aten/src/ATen/core/op_registration/op_registration_test.cpp", "c10/core/DispatchKey.cpp", "c10/core/DispatchKey.h", "test/test_dispatch.py", "torch/csrc/utils/python_dispatch.cpp", "torch/testing/_internal/common_utils.py"], "labels": ["Merged", "cla signed"]}, "7c9a278895": {"title": "fix trailing newlines (#65474)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/65474\n\nTest Plan: Imported from OSS\n\nReviewed By: ezyang\n\nDifferential Revision: D31114952\n\nPulled By: suo\n\nfbshipit-source-id: 3b8cde2098635450c3e22571a401f78e4e54e9e0", "pr_number": "65474", "files_changed": ["torch/csrc/Exceptions.cpp"], "labels": ["Merged", "cla signed", "ciflow/default"]}, "152f0236c3": {"title": "Revert D31082693: Fix autograd engine checks and update InputMetadata", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD31082693 (https://github.com/pytorch/pytorch/commit/9324d682fdb87102d049d0579b433c759e7f998a)\n\nOriginal commit changeset: cb551cd438c6\n\nfbshipit-source-id: fc60f86b80fc70058984df6bccbf240d27f5843e", "pr_number": null, "files_changed": ["test/test_python_dispatch.py", "torch/csrc/autograd/engine.cpp", "torch/csrc/autograd/function.h", "torch/csrc/autograd/input_metadata.h", "torch/csrc/autograd/variable.cpp"], "labels": []}, "b3ec88f41f": {"title": "ugh (#65477)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/65477\n\nTest Plan: Imported from OSS\n\nReviewed By: zhouzhuojie\n\nDifferential Revision: D31115936\n\nPulled By: suo\n\nfbshipit-source-id: fb16911a683713fdc2393bfe7150fc29c7d6814f", "pr_number": "65477", "files_changed": ["torch/csrc/Exceptions.cpp"], "labels": ["Merged", "cla signed", "ciflow/default"]}, "db4b68b3ac": {"title": "Back out \"Eagerly populate python_error::what() when TORCH_SHOW_CPP_STACKTRACES=1\"", "body": "Summary: Original commit changeset: 9cfda47cafb3\n\nTest Plan: unland\n\nReviewed By: ezyang\n\nDifferential Revision: D31116643\n\nfbshipit-source-id: 631eea446ed48c63ca39281d24163a2eadbe8d12", "pr_number": null, "files_changed": ["torch/csrc/Exceptions.cpp", "torch/csrc/Exceptions.h"], "labels": []}, "158393e1a1": {"title": "Fix autograd engine checks and update InputMetadata (#65235)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65235\n\n1. Updated the legacy type checks in `torch/csrc/autograd/engine.cpp` to individually validate the dtype, device, and layout equality for grad and tensor.\n2. Removed device field from `InputMetadata` since it's already stored via storing options. Also, added `dtype()` and `layout()` methods to `InputMetadata`. To make this change, some calls had to be updated due to the change in constructor.\n3. To fix https://github.com/pytorch/pytorch/issues/65016:\n     a. Added a `is_tensor_subclass` field in `InputMetadata` to skip device checks for grad and tensor when the tensor has\n         python key set on it (tensor subclass).\n\nTest Plan: Imported from OSS\n\nReviewed By: jbschlosser\n\nDifferential Revision: D31117318\n\nPulled By: anjali411\n\nfbshipit-source-id: 825401df98695c48bf9b320be54585f6aff500bd", "pr_number": "65235", "files_changed": ["test/test_python_dispatch.py", "torch/csrc/autograd/engine.cpp", "torch/csrc/autograd/function.h", "torch/csrc/autograd/input_metadata.h", "torch/csrc/autograd/variable.cpp"], "labels": ["Merged", "cla signed"]}, "70a545b21e": {"title": "Add Tensor._make_wrapper_subclass (#65340)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65340\n\nI thought about a few possible ways of doing this.  The main hazard is\nthat if I create a CPU tensor that doesn't have any real storage, the\nmoment I actually try to access the data on the tensor I will segfault.\nSo I don't want to use _make_subclass on a \"cpu meta tensor\" because\nthe CPU meta tensor (with no subclass) is radioactive: printing it\nwill immediately cause a segfault.  So instead, I have to create\nthe CPU meta tensor AND subclass all in one go, and that means I need\nanother function for it.  One downside to doing it this way is\nI need another overload for explicit strides, and in general it is\ndifficult to get the view relationships to all work out properly;\ntracked at https://github.com/pytorch/pytorch/issues/65339\n\nFixes https://github.com/pytorch/pytorch/issues/62972\nFixes https://github.com/pytorch/pytorch/issues/62730\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nReviewed By: albanD\n\nDifferential Revision: D31057231\n\nPulled By: ezyang\n\nfbshipit-source-id: 73522769e093ae8a1bf0c7f7e594659bfb827b28", "pr_number": "65340", "files_changed": ["test/test_python_dispatch.py", "torch/csrc/DynamicTypes.cpp", "torch/csrc/autograd/python_variable.cpp", "torch/overrides.py"], "labels": ["Merged", "cla signed", "ciflow/default"]}, "0fe86ac6c6": {"title": "Fix torch.any documentation (#65310)", "body": "Summary:\nCurrently, the description of torch.any would be parsed like\n\n```\nparam input\nthe input tensor.\n```\n\nHowever, it should be\n\n```\nTests if any element in input evaluates to True.\n```\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65310\n\nReviewed By: ezyang\n\nDifferential Revision: D31102918\n\nPulled By: soulitzer\n\nfbshipit-source-id: 678ade20ba16ad2643639fbd2420c8b36fcd8bd7", "pr_number": "65310", "files_changed": ["torch/_torch_docs.py"], "labels": ["open source", "Merged", "cla signed"]}, "cbc3db8274": {"title": "Create test for builtin tensorrt module in torch deploy (#63819)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/63819\n\nghstack-source-id: 138521664\n\nTest Plan:\nbuck test mode/dev-nosan caffe2/torch/csrc/deploy:test_deploy_gpu\n\nbuck test mode/opt-split-dwarf caffe2/torch/csrc/deploy:test_deploy_gpu\n\nReviewed By: wconstab\n\nDifferential Revision: D30499301\n\nfbshipit-source-id: 0bc165b4ed5be28ebb0becc65f292cf26368692f", "pr_number": "63819", "files_changed": ["torch/csrc/deploy/example/generate_examples.py", "torch/csrc/deploy/example/tensorrt_example.py", "torch/csrc/deploy/test_deploy_gpu.cpp"], "labels": ["Merged", "cla signed"]}, "2898ef7549": {"title": "Minor ScanKernels.cu cleanup (#65350)", "body": "Summary:\n- Replace THCNumerics with `at::_isnan`\n- Replace `contiguous` with `expect_contiguous`\n- Don't use `contiguous` on output tensors. Instead skip the copy and\n  just create a new empty tensor.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65350\n\nReviewed By: ezyang\n\nDifferential Revision: D31103501\n\nPulled By: ngimel\n\nfbshipit-source-id: 9030869e28d6c570fad074fd0502076de8e2ab09", "pr_number": "65350", "files_changed": ["aten/src/ATen/native/cuda/ScanKernels.cu"], "labels": ["triaged", "open source", "Merged", "cla signed", "ciflow/default"]}, "32f0387ee8": {"title": "Bug in CosineAnnealingWarmRestarts in optim/lr_scheduler.py (#64758)", "body": "Summary:\n## {emoji:1f41b} Bug\n'CosineAnnealingWarmRestarts'  object has no attribute 'T_cur'.\nIn the Constructor of the CosineAnnealingWarmRestarts, we're calling the constructor of the Parent class (_LRScheduler) which inturn calls the step method of the CosineAnnealingWarmRestarts.\nThe called method tries to update the object's attribute  'T_cur' which is not defined yet. So it raises the error.\nThis only holds, when we give the value for last_epoch argument as 0 or greater than 0 to the 'CosineAnnealingWarmRestarts', while initializing the object.\n\n![Bug_in_CosineAnnealingWarmRestarts](https://user-images.githubusercontent.com/77477328/132552212-70abc8b5-0357-4c35-90a9-832648bac607.png)\n## To Reproduce\n\nSteps to reproduce the behavior:\n\n1. Give the value for the last_epoch argument as zero OR\n1. Give the value for the last_epoch argument as a Positive integer.\n\n## Expected behavior\n\nI only expected the 'CosineAnnealingWarmRestarts' object to be initialized.\n\n## Environment\n\nPyTorch version: 1.9.0+cpu\nIs debug build: False\nCUDA used to build PyTorch: None\nROCM used to build PyTorch: N/A\nOS: Ubuntu 20.04.2 LTS (x86_64)\nGCC version: (Ubuntu 9.3.0-17ubuntu1~20.04) 9.3.0\nClang version: Could not collect\nCMake version: version 3.21.2\nLibc version: glibc-2.31\nPython version: 3.8.10  [GCC 9.4.0] (64-bit runtime)\nPython platform: Linux-5.8.0-59-generic-x86_64-with-glibc2.29\nIs CUDA available: False\nCUDA runtime version: No CUDA\n\n## Additional context\nWe can able to solve this bug by moving the line 'self.T_cur = self.last_epoch' above the 'super(CosineAnnealingWarmRestarts,self).__init__()' line. Since we've initialized the \"self.T_cur\" to the object.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/64758\n\nReviewed By: ezyang\n\nDifferential Revision: D31113694\n\nPulled By: jbschlosser\n\nfbshipit-source-id: 98c0e292291775895dc3566fda011f2d6696f721", "pr_number": "64758", "files_changed": ["torch/optim/lr_scheduler.py"], "labels": ["triaged", "open source", "Merged", "cla signed"]}, "7e7be526c9": {"title": "Add TORCH_SHOW_CPP_STACKTRACES to Contributing.md (#64052)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/64052\n\nReviewed By: ezyang\n\nDifferential Revision: D31107779\n\nPulled By: Chillee\n\nfbshipit-source-id: 2ad8ad40cd48e54fe711863c3c74df884a2e2de7", "pr_number": "64052", "files_changed": ["CONTRIBUTING.md"], "labels": ["Merged", "cla signed"]}, "97b535dabd": {"title": "[PyTorch] add fastToString for infer_schema (#64823)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/64823\n\nWe seem to spend noticable time in vfprintf for this, and the number of arguments is almost always small enough to do this in just a few instructions.\nghstack-source-id: 138623354\n\nTest Plan: Profile schema parsing, saw less time in vfprintf\n\nReviewed By: ezyang, dhruvbird\n\nDifferential Revision: D30860716\n\nfbshipit-source-id: 09ef085cd6f93dc1eaa78790dde918ac60e67450", "pr_number": "64823", "files_changed": ["aten/src/ATen/core/op_registration/infer_schema.cpp"], "labels": ["Merged", "cla signed"]}, "c731be8066": {"title": "[BE] Use `DispatchKeySet` in `check_base_legacy_new` (#65535)", "body": "Summary:\nRefactor:\n```\nTORCH_CHECK ( key == a ||\n              key == b ||\n              key == c,\n              \"expected key to be in \", a, \" or \", b , \" or \", c,\n              \" but got \", key);\n```\ninto\n```\nTORCH_CHECK( key_set.has(key),\n            \"expected key to be in \", key_set,\n            \" but got \", key );\n```\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65535\n\nReviewed By: wconstab\n\nDifferential Revision: D31144239\n\nPulled By: malfet\n\nfbshipit-source-id: 68a053041a38f043e688e491889dd7ee258f3db3", "pr_number": "65535", "files_changed": ["torch/csrc/utils/tensor_new.cpp"], "labels": ["Merged", "cla signed", "ciflow/default"]}, "7e772e7685": {"title": "Update link to tutorial on defining NN modules (#65534)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/65527. Please, see my comment in the issue: https://github.com/pytorch/pytorch/issues/65527#issuecomment-925863193. The file was renamed in https://github.com/pytorch/tutorials/commit/ce58d5904c04c4be10561447e41a153f573a3f93#diff-e5ef486bd89eb38de15752211d9437953681b8caa8f44d7c86bb820d13151df2, but the link in this repository was not updated.\n\nIt doesn't change the fact that the old link is still working, but I guess this has to be fixed in [pytorch/tutorials](https://github.com/pytorch/tutorials) instead of here.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65534\n\nReviewed By: soulitzer\n\nDifferential Revision: D31144269\n\nPulled By: H-Huang\n\nfbshipit-source-id: f70744a21113b7dc84510e2992d87f0fed793985", "pr_number": "65534", "files_changed": ["docs/source/notes/modules.rst"], "labels": ["open source", "Merged", "cla signed"]}, "1f0f246fe2": {"title": "Automated submodule update: FBGEMM (#65360)", "body": "Summary:\nThis is an automated pull request to update the first-party submodule for [pytorch/FBGEMM](https://github.com/pytorch/FBGEMM).\n\nNew submodule commit: https://github.com/pytorch/FBGEMM/commit/0108d4f5527ee262bc92a4949b6ec6239ded9d4d\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65360\n\nTest Plan: Ensure that CI jobs succeed on GitHub before landing.\n\nReviewed By: jspark1105\n\nDifferential Revision: D31061552\n\nfbshipit-source-id: 8bce5157a281e38cad5d5d0e9dcd123beda39735", "pr_number": "65360", "files_changed": ["third_party/fbgemm"], "labels": ["triaged", "open source", "Merged", "cla signed"]}, "36485d36b6": {"title": "Docathon: Add docs for nn.functional.*d_max_pool (#63264)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/63264\n\nAdding docs to max_pool to resolve docathon issue #60904\n\nTest Plan: Imported from OSS\n\nReviewed By: malfet\n\nDifferential Revision: D31071491\n\nPulled By: Gamrix\n\nfbshipit-source-id: f4f6ec319c62ff1dfaeed8bb6bb0464b9514a7e9", "pr_number": "63264", "files_changed": ["torch/nn/functional.py"], "labels": ["Merged", "cla signed", "hackathon"]}, "c73f0e457e": {"title": "Tensor and device is_hpu methods (#65408)", "body": "Summary:\nAdd is_hpu() methods for Aten tensor and device\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65408\n\nReviewed By: malfet\n\nDifferential Revision: D31144227\n\nPulled By: wconstab\n\nfbshipit-source-id: 115f4df4b8d54e6913dd51af7b6d4cacf6dd43c5", "pr_number": "65408", "files_changed": ["aten/src/ATen/core/TensorBase.h", "c10/core/Device.h", "c10/core/TensorImpl.h"], "labels": ["triaged", "open source", "Merged", "cla signed"]}, "963ae25e41": {"title": "Migrate THCAtomics to ATen (#65470)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/65470\n\nTest Plan: Imported from OSS\n\nReviewed By: H-Huang\n\nDifferential Revision: D31148184\n\nPulled By: ngimel\n\nfbshipit-source-id: aaac3dfb5f2c6f88e9bd922b3a56d0a16a861e17", "pr_number": "65470", "files_changed": ["aten/src/ATen/cuda/Atomic.cuh", "aten/src/ATen/cuda/CUDAApplyUtils.cuh", "aten/src/ATen/native/cuda/AdaptiveAveragePooling.cu", "aten/src/ATen/native/cuda/AdaptiveAveragePooling3d.cu", "aten/src/ATen/native/cuda/AdaptiveMaxPooling2d.cu", "aten/src/ATen/native/cuda/AdaptiveMaxPooling3d.cu", "aten/src/ATen/native/cuda/AveragePool3d.cu", "aten/src/ATen/native/cuda/DilatedMaxPool3d.cu", "aten/src/ATen/native/cuda/EmbeddingBackwardKernel.cu", "aten/src/ATen/native/cuda/EmbeddingBackwardKernel.cuh", "aten/src/ATen/native/cuda/EmbeddingBag.cu", "aten/src/ATen/native/cuda/FractionalMaxPool2d.cu", "aten/src/ATen/native/cuda/FractionalMaxPool3d.cu", "aten/src/ATen/native/cuda/FunctionOfAMatrixUtilsKernel.cu", "aten/src/ATen/native/cuda/Indexing.cu", "aten/src/ATen/native/cuda/KernelUtils.cuh", "aten/src/ATen/native/cuda/LossCTC.cu", "aten/src/ATen/native/cuda/NLLLoss2d.cu", "aten/src/ATen/native/cuda/RNN.cu", "aten/src/ATen/native/cuda/ReflectionPad.cu", "aten/src/ATen/native/cuda/ReplicationPadding.cu", "aten/src/ATen/native/cuda/ScatterGatherKernel.cu", "aten/src/ATen/native/cuda/SortingRadixSelect.cuh", "aten/src/ATen/native/cuda/SummaryOps.cu", "aten/src/ATen/native/cuda/UpSample.cuh", "aten/src/ATen/native/cuda/UpSampleLinear1d.cu", "aten/src/ATen/native/cuda/UpSampleTrilinear3d.cu", "aten/src/ATen/native/cuda/im2col.cuh", "aten/src/ATen/test/cuda_atomic_ops_test.cu", "aten/src/THC/THCAtomics.cuh"], "labels": ["module: porting", "open source", "Merged", "cla signed", "ciflow/default"]}, "8c7caedbb8": {"title": "avoid re-allocation of view_shape for every tensor in `torch.meshgrid` (#62908)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/62908\n\nReviewed By: mruberry\n\nDifferential Revision: D31064165\n\nPulled By: dagitses\n\nfbshipit-source-id: 3ddc3088e70fc8ef6dcf56ceb67fd20991169af1", "pr_number": "62908", "files_changed": ["aten/src/ATen/native/TensorShape.cpp"], "labels": ["Merged", "cla signed"]}, "d85e12a5bf": {"title": "add OpInfo for `torch.argsort` (#65454)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65454\n\nAddresses facebookresearch/functorch#103.\n\nTest Plan: Imported from OSS\n\nReviewed By: ezyang\n\nDifferential Revision: D31111700\n\nPulled By: zou3519\n\nfbshipit-source-id: ec4babd2fcdcea856ba0ee8db0fd8f42b87269f3", "pr_number": "65454", "files_changed": ["torch/testing/_internal/common_methods_invocations.py"], "labels": ["open source", "Merged", "cla signed", "module: testing", "ciflow/default", "ciflow/all"]}, "fd24e1b61f": {"title": "add `OpInfo` for `torch.repeat_interleave` (#65455)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65455\n\nAddresses facebookresearch/functorch#103.\n\nTest Plan: Imported from OSS\n\nReviewed By: ezyang\n\nDifferential Revision: D31111696\n\nPulled By: zou3519\n\nfbshipit-source-id: 4fa73708fa915cb21adbba9cb8fd2b8f75bcd3e0", "pr_number": "65455", "files_changed": ["torch/testing/_internal/common_methods_invocations.py"], "labels": ["open source", "Merged", "cla signed", "module: testing", "ciflow/default", "ciflow/all"]}, "e742839f0e": {"title": "Fix autograd engine test in python_dispatch (#65567)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/65567\n\nTest Plan: Imported from OSS\n\nReviewed By: soulitzer\n\nDifferential Revision: D31158090\n\nPulled By: albanD\n\nfbshipit-source-id: 651b78016ad978c7419343554ce7ceffd54aef1b", "pr_number": "65567", "files_changed": ["test/test_python_dispatch.py"], "labels": ["Merged", "cla signed"]}, "b858993c97": {"title": "Fix engine check for case where grad is a subclass (#65568)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/65568\n\nTest Plan: Imported from OSS\n\nReviewed By: soulitzer\n\nDifferential Revision: D31158089\n\nPulled By: albanD\n\nfbshipit-source-id: 2a77df9b6340107de02a043b57a36cb7ae68df34", "pr_number": "65568", "files_changed": ["test/test_python_dispatch.py", "torch/csrc/autograd/engine.cpp"], "labels": ["Merged", "cla signed"]}, "f3587f6bfa": {"title": "Remove THC ScalarConvert (#65471)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/65471\n\nTest Plan: Imported from OSS\n\nReviewed By: H-Huang\n\nDifferential Revision: D31148182\n\nPulled By: ngimel\n\nfbshipit-source-id: bbf74e36a3d91a7be3e47199981440c68a2f645f", "pr_number": "65471", "files_changed": ["aten/src/ATen/native/cuda/AdaptiveAveragePooling.cu", "aten/src/ATen/native/cuda/AveragePool2d.cu", "aten/src/ATen/native/cuda/AveragePool3d.cu", "aten/src/ATen/native/cuda/DilatedMaxPool2d.cu", "aten/src/ATen/native/cuda/EmbeddingBag.cu", "aten/src/ATen/native/cuda/NaiveDilatedConvolution.cu", "aten/src/ATen/native/cuda/RNN.cu", "aten/src/ATen/native/cuda/SortUtils.cuh", "aten/src/ATen/native/cuda/TensorTopK.cu", "aten/src/ATen/native/cuda/WeightNorm.cu", "aten/src/ATen/native/cuda/im2col.cuh", "aten/src/THC/THCNumerics.cuh"], "labels": ["module: porting", "open source", "Merged", "cla signed", "ciflow/default"]}, "760aefd34d": {"title": "Fix nullptr addition (#65548)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65548\n\nFixes\ncaffe2/test:jit - test_unsupported_nn_functional_pad_circular_cpu_float32 (test_jit_fuser_te.TestNNCOpInfoCPU)\n\nTest Plan: Sandcastle\n\nReviewed By: ngimel\n\nDifferential Revision: D31148405\n\nfbshipit-source-id: 4c8c693a45229ab4e59b0b0ec5326d3ac114dbaf", "pr_number": "65548", "files_changed": ["c10/core/TensorImpl.h"], "labels": ["fb-exported", "cla signed"]}, "eca4f14b6c": {"title": "[PyTorch] Add C10_ prefix to MPARK_* macros in variant.h (#65589)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65589\n\nWithout this prefix, the include guards interfere with attempts to indirectly include both c10::variant and the original mpark variant in the same translation unit.\nghstack-source-id: 138901838\n\nTest Plan: Temporarily `#include <c10/util/variant.h>` in ivalue.h and buck build //data_preproc/preproc:preproc_adapter_utils mode/no-gpu -- this delayed D31101962 (https://github.com/pytorch/pytorch/commit/01720d6a2352fc1c16f41753c3929f9c12dac528) from fixing S244170\n\nReviewed By: bhosmer\n\nDifferential Revision: D31159414\n\nfbshipit-source-id: 234c5ed37ca853702bcdf3263e4f185b95ac1d08", "pr_number": "65589", "files_changed": ["c10/util/variant.h"], "labels": ["cla signed", "ciflow/default"]}, "640a615150": {"title": "[easy] [PyTorch Edge] Remove double pragma once directive in the generated code (#65620)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65620\n\nThis was bothering me for a while.\n\nghstack-source-id: 138914860\n\nTest Plan: Sandcastle\n\nReviewed By: beback4u\n\nDifferential Revision: D31162648\n\nfbshipit-source-id: 72c47ea34d40c772bb53da721fcb36365b5dbaf3", "pr_number": "65620", "files_changed": ["tools/lite_interpreter/gen_selected_mobile_ops_header.py"], "labels": ["cla signed", "ciflow/default"]}, "15724bcc03": {"title": "[TensorExpr] Re-enable a float16 test. (#65632)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/65632\n\nTest Plan: Imported from OSS\n\nReviewed By: huiguoo\n\nDifferential Revision: D31181798\n\nPulled By: ZolotukhinM\n\nfbshipit-source-id: 1a57d0a878d44f8b73f3c24eef7ba707ce18fb70", "pr_number": "65632", "files_changed": ["test/test_tensorexpr.py"], "labels": ["cla signed"]}, "10d0dbc6d9": {"title": "Avoid storage access for HPU tensors (#65409)", "body": "Summary:\nAdd is_hpu() methods for Aten tensor and device\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65409\n\nReviewed By: wconstab, H-Huang\n\nDifferential Revision: D31134422\n\nPulled By: malfet\n\nfbshipit-source-id: 181ebb67dce8e05a0723ef3c82f23e39228841ee", "pr_number": "65409", "files_changed": ["torch/_tensor.py"], "labels": ["triaged", "open source", "cla signed"]}, "cd2656a2e5": {"title": "[package] add some docs describing how to debug dependencies (#65704)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65704\n\nAs title.\n\nTest Plan: Imported from OSS\n\nReviewed By: tugsbayasgalan\n\nDifferential Revision: D31209866\n\nPulled By: suo\n\nfbshipit-source-id: 4c8ec1d5418ea75b71c4b9a498b86f0ef5383544", "pr_number": "65704", "files_changed": ["docs/source/package.rst"], "labels": ["cla signed", "ciflow/default"]}, "ea546e20fd": {"title": "[Reland] nn.functional.linear OpInfo (#65498)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/65498\n\nTest Plan: Imported from OSS\n\nReviewed By: gchanan\n\nDifferential Revision: D31171149\n\nPulled By: zou3519\n\nfbshipit-source-id: badb06af08a772397b0280189385723c0175200b", "pr_number": "65498", "files_changed": ["torch/testing/_internal/common_methods_invocations.py"], "labels": ["cla signed", "ci/windows", "ci/master", "ciflow/slow-gradcheck", "ciflow/default", "ciflow/all"]}, "fea32be964": {"title": "Add HPU type for check_base_legacy_new (#65410)", "body": "Summary:\nFixes #{issue number}\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65410\n\nReviewed By: H-Huang\n\nDifferential Revision: D31143754\n\nPulled By: malfet\n\nfbshipit-source-id: 32abfbae4f7c09924c7dfa16758d64a2215ec636", "pr_number": "65410", "files_changed": ["torch/csrc/utils/tensor_new.cpp"], "labels": ["triaged", "open source", "cla signed"]}, "f5b4e369f6": {"title": "Sparse SoftMax: Remove unused variables (#65539)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65539\n\nThis function doesn't directly use thrust so these are simply unused variables.\n\nTest Plan: Imported from OSS\n\nReviewed By: gchanan\n\nDifferential Revision: D31193191\n\nPulled By: malfet\n\nfbshipit-source-id: 231b6a197c9f1bd5a61e46cb858e8eedc85b2818", "pr_number": "65539", "files_changed": ["aten/src/ATen/native/sparse/cuda/SoftMax.cu"], "labels": ["open source", "cla signed", "ciflow/default"]}, "7c62b6e973": {"title": "add deepcopy support to subclasses (#65584)", "body": "Summary:\nHappy to get any feedback on how to make this code cleaner!\n\nThis:\n- Fix Tensor attribute deepcopy BC-breaking?\n- Add a test for Tensor attribute deepcopy\n- Fix subclass deepcopy\n- Moves the subclass serialization tests into their own class not to interfere with other serialization test logic\n- Add a test for subclass deepcopy\n\ncc ezyang gchanan\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65584\n\nReviewed By: gchanan\n\nDifferential Revision: D31206590\n\nPulled By: albanD\n\nfbshipit-source-id: 74a8f0767f4933b9c941fbea880a8fd1b893ea2f", "pr_number": "65584", "files_changed": ["test/test_serialization.py", "test/test_torch.py", "torch/_tensor.py"], "labels": ["module: bc-breaking", "cla signed"]}, "6a6ee92e36": {"title": "[quant] Add op benchmark for CPU FakeQuantizePerChannel with float zero_points (#65241)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/65241\n\nTest Plan: Imported from OSS\n\nReviewed By: jingsh\n\nDifferential Revision: D31150087\n\nPulled By: b-koopman\n\nfbshipit-source-id: a00d4995841eee81305d0007c908473cc3d5a727", "pr_number": "65241", "files_changed": ["benchmarks/operator_benchmark/pt/quantization_test.py"], "labels": ["cla signed"]}, "6a99053515": {"title": "Added sparse-tensor copy logic to dispatcher (#65304)", "body": "Summary:\n- Only ported copy for sparse tensor to dispatcher. Everything else is the same\n- Duplicated code for named tensor handling in sparse tensor copy\n\t- Might change it later to handle named tensors using dispatcher\n\nIssue https://github.com/pytorch/pytorch/issues/61122\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65304\n\nReviewed By: gchanan\n\nDifferential Revision: D31176720\n\nPulled By: ezyang\n\nfbshipit-source-id: 56757a3b0fb56c3d05c16dd935428a0cd91ea766", "pr_number": "65304", "files_changed": ["aten/src/ATen/native/Copy.cpp", "aten/src/ATen/native/native_functions.yaml", "aten/src/ATen/native/sparse/SparseTensor.cpp"], "labels": ["open source", "cla signed"]}, "3324bae5f1": {"title": "Remove THCTensor.cu and THCTensorCopy.cu copy (#65491)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65491\n\nThe only user of any of this code is THCStorage_copy, so I've\nmigrated that to call `Tensor.copy_` directly.\n\nTest Plan: Imported from OSS\n\nReviewed By: H-Huang\n\nDifferential Revision: D31148183\n\nPulled By: ngimel\n\nfbshipit-source-id: 92bab71306c84bc481c47a0615ebb811af2c2875", "pr_number": "65491", "files_changed": ["aten/src/THC/CMakeLists.txt", "aten/src/THC/THC.h", "aten/src/THC/THCStorageCopy.cpp", "aten/src/THC/THCStorageCopy.cu", "aten/src/THC/THCStorageCopy.h", "aten/src/THC/THCTensor.cpp", "aten/src/THC/THCTensor.cu", "aten/src/THC/THCTensorCopy.cu", "aten/src/THC/THCTensorCopy.h", "aten/src/THC/THCTensorCopy.hpp", "aten/src/THC/generic/THCStorageCopy.cpp", "aten/src/THC/generic/THCStorageCopy.cu", "aten/src/THC/generic/THCTensor.cpp", "aten/src/THC/generic/THCTensor.cu", "aten/src/THC/generic/THCTensorCopy.cu", "aten/src/THC/generic/THCTensorCopy.h"], "labels": ["module: porting", "open source", "cla signed", "ciflow/default"]}, "26e31f76b0": {"title": "`*_solve` methods: implements forward AD (#65546)", "body": "Summary:\nThis PR adds forward AD for `*_solve` methods.\nAdditionally, `cholesky_solve` gets OpInfo + a bug fix when wrong leading dimensions could be passed to LAPACK,\nand `lu_solve` gets forward AD with 2x`lu_solve` instead of 1x`lu_solve` + 2x`triangular_solve`.\n\ncc ezyang albanD zou3519 gqchen pearu nikitaved soulitzer Lezcano Varal7 jianyuh mruberry walterddr IvanYashchuk xwang233\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65546\n\nReviewed By: gchanan\n\nDifferential Revision: D31206837\n\nPulled By: albanD\n\nfbshipit-source-id: 040beda97442e7a88a9df9abc7bb18313ce55bc3", "pr_number": "65546", "files_changed": ["aten/src/ATen/native/BatchLinearAlgebra.cpp", "tools/autograd/derivatives.yaml", "torch/csrc/autograd/FunctionsManual.cpp", "torch/csrc/autograd/FunctionsManual.h", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["module: autograd", "open source", "module: linear algebra", "cla signed", "ciflow/slow-gradcheck", "ciflow/default"]}, "e155e7520f": {"title": "MaxUnpooling: parallel_for not always backed by OMP (#65655)", "body": "Summary:\nUse `c10::optional` + thread_fence  instead of `#pragma omp critical` inside max_unpooling kernels\n\nUsing any OpenMP pragma in `at::parallel_for` body is wrong, as it can\nbe implemented using native treading algorithms such as ptrheads\n\n`c10::optional` sounds like a much better approach to pair of\n`has_error` and `error_index` variables. Use `std::atomic_thread_fence` to ensure error_index value is synchronized.\n\nIt also fixes ICE reported in https://github.com/pytorch/pytorch/issues/65578\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65655\n\nReviewed By: ngimel\n\nDifferential Revision: D31206501\n\nPulled By: malfet\n\nfbshipit-source-id: 93df34530e721777b69509cd6c68f5d713fb2b2a", "pr_number": "65655", "files_changed": ["aten/src/ATen/native/cpu/MaxUnpoolKernel.cpp"], "labels": ["cla signed", "ciflow/default"]}, "87cd658c27": {"title": "Add override to virtual destructor in derived class (#65476)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65476\n\nAs suggested by `-Winconsistent-missing-destructor-override`.\n\nTest Plan: CI\n\nReviewed By: pritamdamania87\n\nDifferential Revision: D31115128\n\nfbshipit-source-id: a4e2441c13704c0c46e3e86f7886fca76c40ca39", "pr_number": "65476", "files_changed": ["c10/core/thread_pool.h"], "labels": ["fb-exported", "cla signed"]}, "c2252b3aa6": {"title": "Port `max` kernel to structured kernels. (#61449)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/61449\n\nTracking issue: #55070\n\nTest Plan: Imported from OSS\n\nReviewed By: saketh-are\n\nDifferential Revision: D29741714\n\nPulled By: bdhirsh\n\nfbshipit-source-id: 6c8c17d20f578ab0af8a969d103a19ccd8d51842", "pr_number": "61449", "files_changed": ["aten/src/ATen/native/ReduceOpsUtils.h", "aten/src/ATen/native/TensorCompare.cpp", "aten/src/ATen/native/TensorCompare.h", "aten/src/ATen/native/cpu/TensorCompareKernel.cpp", "aten/src/ATen/native/cuda/ReduceMinMaxKernel.cu", "aten/src/ATen/native/native_functions.yaml"], "labels": ["open source", "cla signed"]}, "c829cb6840": {"title": "Port `min` kernel to structured kernels. (#61450)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/61450\n\nTracking issue: #55070\n\nTest Plan: Imported from OSS\n\nReviewed By: saketh-are\n\nDifferential Revision: D29741713\n\nPulled By: bdhirsh\n\nfbshipit-source-id: 2c107752a90fd39cfb55e08aaf3541bd484a5fc3", "pr_number": "61450", "files_changed": ["aten/src/ATen/native/ReduceOps.cpp", "aten/src/ATen/native/ReduceOpsUtils.h", "aten/src/ATen/native/TensorCompare.cpp", "aten/src/ATen/native/TensorCompare.h", "aten/src/ATen/native/cpu/TensorCompareKernel.cpp", "aten/src/ATen/native/cuda/ReduceMinMaxKernel.cu", "aten/src/ATen/native/native_functions.yaml", "test/test_autograd.py", "test/test_sparse.py"], "labels": ["open source", "cla signed"]}, "a90912ecc5": {"title": "[sparsity] Remove the pack_param from the sparsifier state_dict (#65292)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65292\n\nThat was the original design, that we decided to simplify by removing the packing in the sparsifier.\nThe state of the sparsifier is saved directly, and the old behavior accidentally bled through to the current version.\nThis change removes the `_pack_params` method, and changes the state_dict to include the state directly.\nWe don't have to change the load_state_dict, as it will work with either the old or the new format.\n\nThe main reason for this PR is the simplification. The original design didn't achieve anything useful by packing the sparsification parameters.\n\nTest Plan: Imported from OSS\n\nReviewed By: gchanan\n\nDifferential Revision: D31186826\n\nPulled By: z-a-f\n\nfbshipit-source-id: 4ad72a7e669f048d2f2d269269ee11b63fa169db", "pr_number": "65292", "files_changed": ["test/ao/sparsity/test_parametrization.py", "test/ao/sparsity/test_sparsifier.py", "torch/ao/sparsity/sparsifier/base_sparsifier.py", "torch/ao/sparsity/sparsifier/utils.py"], "labels": ["cla signed"]}, "92ee5cc2e2": {"title": "[sparsity] Fix for accumulation bug in WeightNormSparsifier (#65293)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65293\n\nThis fixes a bug in the WeightNormSparsifier, where the mask is being multiplied by the newly computed mask.\nBecause the mask elements are binary 0/1, this accumulates the mask over every iteration, eventually collapsing the mask to zero.\nThis bug accidentally bled through from old versions.\n\nTest Plan: Imported from OSS\n\nReviewed By: gchanan\n\nDifferential Revision: D31186829\n\nPulled By: z-a-f\n\nfbshipit-source-id: 3f5b2c833148ab0bd8084e7410ce398f1252e65e", "pr_number": "65293", "files_changed": ["test/ao/sparsity/test_sparsifier.py", "torch/ao/sparsity/sparsifier/weight_norm_sparsifier.py"], "labels": ["cla signed"]}, "609384c056": {"title": "[sparsity][doc] Docstring for WeightNormSparsifier (#65294)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65294\n\nThis adds the docstring documentation to the WeightNormSparsifier and adds the typehints for the constructor args.\nNote, this does not require testing as only the doc is changed.\n\nTest Plan: Imported from OSS\n\nReviewed By: gchanan\n\nDifferential Revision: D31186827\n\nPulled By: z-a-f\n\nfbshipit-source-id: c5010c9bba25b074c4cc6c88f251474b758f950d", "pr_number": "65294", "files_changed": ["torch/ao/sparsity/sparsifier/weight_norm_sparsifier.py"], "labels": ["cla signed"]}, "0d7036fdaf": {"title": "don't leak build time path name to runtime for frozen python modules (#65715)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65715\n\nHere is how we freeze a python module:\n- we call python builtin compile method with the source code of the modules and the path. This method returns a python code object\n- we call marshal.dumps to serialize the code object to bytes.\n\nThe code_object.co_filename actually matches the one passed in to the compile method. We can simply replace that with a marker\nto avoid leak build time path to runtime.\n\nThis works on nested code objects as well:\n```\n#!/bin/env python3.8\nimport marshal\n\ncode_str = \"\"\"\nprint(\"hello\")\n\nclass MyCls:\n    def __init__(self):\n        pass\n\"\"\"\nco = compile(code_str, \"<Generated by torch::deploy>\", \"exec\")\ncobytes = marshal.dumps(co)\nimport pdb; pdb.set_trace()\n```\n\nChecking `co`:\n```\n(Pdb) co.co_filename\n'<Generated by torch::deploy>'\n(Pdb) co.co_consts\n('hello', <code object MyCls at 0x7f0e8670bbe0, file \"<Generated by torch::deploy>\", line 4>, 'MyCls', None)\n(Pdb) co.co_consts[1].co_filename\n'<Generated by torch::deploy>'\n```\n\nTest Plan:\nFind the serialized frozenmodule for torch.nn.modules.linear module in the generated bytecode_x.c file. Put the content to /tmp/linear.bytecode\n\nRun the testing script:\n```\nimport marshal\nco_bytes = bytes(eval(\"[{}]\".format(\"\".join(open('/tmp/linear.bytecode').readlines()).replace('\\n', '').replace('\\t', ''))))\nco = marshal.loads(co_bytes)\nprint(co)\n\n```\n\nThe output for the paste without the change:\n```\n<code object <module> at 0x7f39ca7f07c0, file \"/data/users/shunting/fbsource/fbcode/buck-out/opt/gen/caffe2/gen_frozen_torchpython_src__srcs/torch/nn/modules/linear.py\", line 1>\n```\n\nThe output for the paste with the change:\n```\n<code object <module> at 0x7f05a765d710, file \"<Generated by torch::deploy>\", line 1>\n````\n\nNote that the file part is changed as expected.\n\nReviewed By: suo\n\nDifferential Revision: D31214555\n\nfbshipit-source-id: 56958e0a7352f8c30a3377f83209efe7db61f0fb", "pr_number": "65715", "files_changed": ["test/test_deploy.py", "torch/csrc/deploy/interpreter/CMakeLists.txt", "torch/csrc/deploy/interpreter/freeze.py", "torch/utils/_freeze.py"], "labels": ["fb-exported", "cla signed"]}, "8a247fb418": {"title": "LLVM-12 fix for shm_mutex (#65781)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65781\n\nFixes\n```\nstderr: In file included from caffe2/caffe2/contrib/shm_mutex/shm_mutex.cc:1:\ncaffe2/caffe2/contrib/shm_mutex/shm_mutex.h:334:28: error: anonymous non-C-compatible type given name for linkage purposes by alias declaration; add a tag name here [-Werror,-Wnon-c-typedef-for-linkage]\nusing TicketStruct = struct : ShmBaseHeader {\n                           ^\n                            TicketStruct\ncaffe2/caffe2/contrib/shm_mutex/shm_mutex.h:334:31: note: type is not C-compatible due to this base class\nusing TicketStruct = struct : ShmBaseHeader {\n                              ^~~~~~~~~~~~~\ncaffe2/caffe2/contrib/shm_mutex/shm_mutex.h:334:7: note: type is given name 'TicketStruct' for linkage purposes by this alias declaration\nusing TicketStruct = struct : ShmBaseHeader {\n      ^\n1 error generated.\nCannot execute a rule out of process. On RE worker. Thread: Thread[main,5,main]\nCommand failed with exit code 1.\n```\n\nTest Plan: Sandcastle\n\nReviewed By: ngimel\n\nDifferential Revision: D31248938\n\nfbshipit-source-id: 47342fecc72ada9397a1b7bd6fcabfccf988dd3e", "pr_number": "65781", "files_changed": ["caffe2/contrib/shm_mutex/shm_mutex.h"], "labels": ["fb-exported", "cla signed"]}, "f9c2dc860d": {"title": "make layout check optional in torch.testing.assert_close() (#65419)", "body": "Summary:\nIn case the inputs have a different layout, `assert_close(..., check_layout=False)` converts them to strided before comparison. This is helpful if you just want to compare the values of sparse COO / CSR tensor against a strided reference.\n\nThis keeps BC, since the default `check_layout=True` was the old, hard-coded behavior.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65419\n\nReviewed By: H-Huang\n\nDifferential Revision: D31133629\n\nPulled By: mruberry\n\nfbshipit-source-id: ca8918af81fb0e0ba263104836a4c2eeacdfc7e6", "pr_number": "65419", "files_changed": ["test/test_testing.py", "torch/testing/_asserts.py"], "labels": ["open source", "cla signed", "module: testing", "ciflow/default"]}, "0a0564a347": {"title": "Revert D31206837: [pytorch][PR] `*_solve` methods: implements forward AD", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD31206837 (https://github.com/pytorch/pytorch/commit/26e31f76b0a6257d40b8dbcd7fe393acea49f988)\n\nOriginal commit changeset: 040beda97442\n\nfbshipit-source-id: f28091327357af9f54f367eda6606240924b93ac", "pr_number": null, "files_changed": ["aten/src/ATen/native/BatchLinearAlgebra.cpp", "tools/autograd/derivatives.yaml", "torch/csrc/autograd/FunctionsManual.cpp", "torch/csrc/autograd/FunctionsManual.h", "torch/testing/_internal/common_methods_invocations.py"], "labels": []}, "07d5d7b5cc": {"title": "move kernel launch checks from `torch.testing` to `torch.testing._internal.check_kernel_launches` (#60862)", "body": "Summary:\nThe fact that these functions are only used in a single test might be a good enough reason to move them to that module.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/60862\n\nReviewed By: H-Huang\n\nDifferential Revision: D31141354\n\nPulled By: mruberry\n\nfbshipit-source-id: 6ce1f721b88620c5f46222ad1b942bc689f0a3e0", "pr_number": "60862", "files_changed": ["test/test_kernel_launch_checks.py", "torch/testing/__init__.py", "torch/testing/_check_kernel_launches.py", "torch/testing/_internal/check_kernel_launches.py"], "labels": ["open source", "cla signed", "module: testing", "ciflow/default"]}, "f63150fd1d": {"title": "[PyTorch Edge] Reduce the cost of computing isIncludedInAlias() (#65735)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65735\n\nCurrently, `isIncludedInAlias()` calls `getRuntimeDispatchKeySet()` which creates a new `DispatchKeySet` object from an enumerated list of dispatch keys. `isIncludedInAlias()` then checks if a single dispatch key is part of this set. Instead, just pass in the key one wishes to check. This is marginally faster.\n\nghstack-source-id: 139281528\n\nTest Plan:\nSee these 2 AI Bench Runs on the Milan-FFF-11-30 device.\n\n### Before\n[AI Bench](https://www.internalfb.com/intern/aibench/details/237302972704466), [Flamegraph](https://interncache-all.fbcdn.net/manifold/aibench/tree/mobile/pt/profiling_reports/speech_transducer_v25_perf_1632804218329.html)\n\n### After\n[AI Bench](https://www.internalfb.com/intern/aibench/details/606320012968375), [Flamegraph](https://interncache-all.fbcdn.net/manifold/aibench/tree/mobile/pt/profiling_reports/speech_transducer_v25_perf_1632807348803.html)\n\nCheck the the flamegraphs, and focus on any kernel registration code path during library initialization.\n\nReviewed By: swolchok\n\nDifferential Revision: D31228062\n\nfbshipit-source-id: 7a986e3593c30ded7919cd3b564ec579dc97ab5f", "pr_number": "65735", "files_changed": ["aten/src/ATen/test/CMakeLists.txt", "aten/src/ATen/test/dispatch_key_set_test.cpp", "c10/core/DispatchKeySet.cpp", "c10/core/DispatchKeySet.h"], "labels": ["cla signed", "ciflow/default"]}, "1d681c1ab2": {"title": "Migrate THCThrustAllocator to ATen (#65492)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/65492\n\nTest Plan: Imported from OSS\n\nReviewed By: H-Huang\n\nDifferential Revision: D31148180\n\nPulled By: ngimel\n\nfbshipit-source-id: d5e4902036493517ca97c3442713b5e0e79229f9", "pr_number": "65492", "files_changed": ["aten/src/ATen/cuda/ThrustAllocator.h", "aten/src/ATen/native/cuda/EmbeddingBackwardKernel.cu", "aten/src/ATen/native/cuda/LegacyThrustHelpers.cu", "aten/src/ATen/native/cuda/SoftMax.cu", "aten/src/ATen/native/cuda/SpectralOps.cu", "aten/src/ATen/native/cuda/TensorModeKernel.cu", "aten/src/ATen/native/cuda/Unique.cu", "aten/src/ATen/native/cudnn/Conv_v7.cpp", "aten/src/ATen/native/miopen/Conv_miopen.cpp", "aten/src/ATen/native/sparse/cuda/SoftMax.cu", "aten/src/ATen/native/sparse/cuda/SparseCUDATensor.cu", "aten/src/ATen/native/sparse/cuda/SparseCUDATensorMath.cu", "aten/src/ATen/native/sparse/cuda/SparseCsrTensorMath.cu", "aten/src/ATen/native/sparse/cuda/SparseMatMul.cu", "aten/src/THC/CMakeLists.txt", "aten/src/THC/THCGeneral.cpp", "aten/src/THC/THCGeneral.h.in", "aten/src/THC/THCStorage.cu", "aten/src/THC/THCThrustAllocator.cuh", "aten/src/THC/generic/THCStorage.cu"], "labels": ["module: porting", "open source", "cla signed", "ciflow/default"]}, "2670cacfc2": {"title": "LLVM-12 fix for tensor_new.cpp (#65785)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65785\n\nFixes offset to nullptr at fbcode/caffe2/torch/csrc/utils/tensor_new.cpp:206\n\nTest Plan: Sandcastle\n\nReviewed By: ngimel\n\nDifferential Revision: D31250995\n\nfbshipit-source-id: 56c7761787e732180a2537a8aa4346a39e7399a8", "pr_number": "65785", "files_changed": ["torch/csrc/utils/tensor_new.cpp"], "labels": ["fb-exported", "cla signed"]}, "9b40eaaaab": {"title": "Revert D31193205: [pytorch][PR] CMake: Limit python include directories to only python libraries", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD31193205 (https://github.com/pytorch/pytorch/commit/971c57f1d094806eab5340263dd59dd817267994)\n\nOriginal commit changeset: 5c1b554a59d0\n\nfbshipit-source-id: 5719b7df987ded6e7e212749a438db947656df87", "pr_number": null, "files_changed": ["caffe2/CMakeLists.txt", "cmake/Dependencies.cmake", "torch/CMakeLists.txt"], "labels": []}, "6c2f235d36": {"title": "common_utils.py: Add ASAN as a platform for which you can disable tests (#65791)", "body": "Summary:\nCould be useful for the future.\n\nNext steps: document it.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65791\n\nReviewed By: suo\n\nDifferential Revision: D31254115\n\nPulled By: janeyx99\n\nfbshipit-source-id: 715c18b4505f2be6328aa0be25976116d6956b25", "pr_number": "65791", "files_changed": ["torch/testing/_internal/common_utils.py"], "labels": ["cla signed", "ciflow/default"]}, "a84feeeade": {"title": "[PyTorch Edge] Conditionally trim dispatch key set to save heap memory at runtime (#65732)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65732\n\nFor certain on-device uses, runtime memory comes at a premium. On-device deployments won't use all the available dispatch keys, so it makes sense to keep only the on-device specific ones around for such uses to reduce runtime heap memory allocated.\n\nThis change keeps just 10 dispatch keys (the ones that used on-device), guarded under the `C10_MOBILE_TRIM_DISPATCH_KEYS` macro. it tries to keep the other code-paths unaffected and uses `constexpr` for use in the `array` declaration, and simple inline functions to ensure that the compiler is able to optimize these for server builds.\n\nTest Plan:\nBuild and check mobile models end to end.\n\n```\nbuck build -c \"pt.enable_milan_dispatch_keys_trimming\"=1 //xplat/caffe2/fb/lite_predictor:lite_predictor\n```\n\nReviewed By: ezyang\n\nDifferential Revision: D31185407\n\nfbshipit-source-id: e954765606373dea6ee9466a851dca7684167b0b", "pr_number": "65732", "files_changed": ["CMakeLists.txt", "aten/src/ATen/core/dispatch/OperatorEntry.cpp", "aten/src/ATen/core/dispatch/OperatorEntry.h", "c10/core/DispatchKey.h"], "labels": ["cla signed", "ciflow/default"]}, "0dd1b74a5b": {"title": "Migrate THCScanUtils to ATen (#65743)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/65743\n\nTest Plan: Imported from OSS\n\nReviewed By: navahgar\n\nDifferential Revision: D31257938\n\nfbshipit-source-id: 273b22df41bb7f2a0ab605ec1f6322c2937e7472", "pr_number": "65743", "files_changed": ["aten/src/ATen/cuda/AsmUtils.cuh", "aten/src/ATen/cuda/ScanUtils.cuh", "aten/src/ATen/native/cuda/SortingRadixSelect.cuh", "aten/src/ATen/native/cuda/TensorTopK.cu", "aten/src/THC/CMakeLists.txt", "aten/src/THC/THCAsmUtils.cuh", "aten/src/THC/THCScanUtils.cuh"], "labels": ["module: porting", "open source", "cla signed", "ciflow/default"]}, "20374c991b": {"title": "slow_conv2d_forward: avoid calling dispatcher in parallel region (#65724)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65724\n\nSee gh-56794\n\nAvoid dispatch inside of parallel_for by:\n1. Replacing Tensor slicing with TensorAccessor\n2. Copy bias into output only once, outside of the parallel region\n3. Replaces `addmm`_ with a direct call to gemm.\n\nTechnically this also adds a new requirement that the output always be\ncontiguous, but the out argument version isn't exposed or used\nanywhere in the `torch.nn` API. So that should be fine.\n\nTest Plan: Imported from OSS\n\nReviewed By: saketh-are\n\nDifferential Revision: D31257875\n\nPulled By: ngimel\n\nfbshipit-source-id: 84d2b39e7f65334bdfcc2c4719f93ee3c514ca32", "pr_number": "65724", "files_changed": ["aten/src/ATen/native/ConvolutionMM2d.cpp", "aten/src/ATen/native/Unfold2d.h", "aten/src/ATen/native/cpu/Unfold2d.cpp"], "labels": ["open source", "cla signed", "ciflow/default"]}, "ad85b582da": {"title": "Remove THCDeviceTensor (#65744)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65744\n\nThis is just dead code.\n\nTest Plan: Imported from OSS\n\nReviewed By: navahgar\n\nDifferential Revision: D31257940\n\nfbshipit-source-id: 6c02264106c2dcbadd332f24b95bc9351a04fd9e", "pr_number": "65744", "files_changed": ["aten/src/THC/CMakeLists.txt", "aten/src/THC/THCDeviceTensor-inl.cuh", "aten/src/THC/THCDeviceTensor.cuh", "aten/src/THC/THCDeviceTensorUtils-inl.cuh", "aten/src/THC/THCDeviceTensorUtils.cuh"], "labels": ["module: porting", "open source", "cla signed", "ciflow/default"]}, "91611fe1d1": {"title": "Decouple forward AD checks from backward AD in OpInfo tests and gradcheck (#65040)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/64999\n\n- Adds a flag to gradcheck `check_backward_ad` that can be used to disable gradcheck for backward ad\n  - This is a bit bc-breaking in terms of positional args, but I prefer this ordering\n- In OpInfo tests for forward ad:\n  - set `check_backward_ad` False\n- In test_ops treat `supports_autograd` as if it is `supports_backward_ad` (it basically already is)\n  - the only modification needed is to no longer skip forward ad tests if `supports_autograd` is false\n  - test_dtype, test_variant_consistency, etc behave correctly as-is\n  - In a follow-up PR, we can rename it to actually be `supports_backward_ad`\n- Testing\n  - https://github.com/pytorch/pytorch/pull/65060\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65040\n\nReviewed By: albanD\n\nDifferential Revision: D31238177\n\nPulled By: soulitzer\n\nfbshipit-source-id: f068d4cbe7ffb094930b16cddb210583b9b7b2c4", "pr_number": "65040", "files_changed": ["test/test_autograd.py", "test/test_ops.py", "torch/autograd/gradcheck.py", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["cla signed"]}, "c7ef620a14": {"title": "[quant] Add imports to the torch/ao/quantization/__init__.py (#64911)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/64911\n\nThe import statements that involve the `quantize.py` were not added to the module level __init__ file. Those imports are necessary to mimic the behavior of the old import locations. Otherwise, the user would need to change their import statements to `from torch.ao.quantization.quantize import quantize` (instead of `from torch.ao.quantization import quantize`.\n\nAnother change in this diff is that we don't use `__all__` anymore. The all dunder was never used in quantization anyway, and just creates a potential bug when using `from ... import *`.\nghstack-source-id: 139342483\n\nTest Plan: `buck test mode/dev //caffe2/test:quantization`\n\nReviewed By: vkuzo\n\nDifferential Revision: D30897663\n\nfbshipit-source-id: a7b4919a191755e3ba690a79ce3362889f416689", "pr_number": "64911", "files_changed": ["torch/ao/quantization/__init__.py"], "labels": ["cla signed"]}, "5349ea921b": {"title": "Migrate THCIntegerDivider.cuh to ATen (#65745)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/65745\n\nTest Plan: Imported from OSS\n\nReviewed By: navahgar\n\nDifferential Revision: D31257937\n\nfbshipit-source-id: 283693525859b7a77a116df0c227653763911a42", "pr_number": "65745", "files_changed": ["aten/src/ATen/cuda/detail/IntegerDivider.cuh", "aten/src/ATen/cuda/detail/OffsetCalculator.cuh", "aten/src/ATen/native/cuda/SpectralOps.cu", "aten/src/ATen/test/cuda_integer_divider_test.cu", "aten/src/THC/CMakeLists.txt", "aten/src/THC/THCIntegerDivider.cuh"], "labels": ["module: porting", "open source", "cla signed", "ciflow/default"]}, "2c29ec2a41": {"title": "Remove \"SciPioneer\" from PT Distributed code owners (#65862)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65862\n\nghstack-source-id: 139378782\n\nTest Plan: N/A\n\nReviewed By: rohan-varma\n\nDifferential Revision: D31291340\n\nfbshipit-source-id: 65d6a82c57dd50d8a4241e9442d73002590989d9", "pr_number": "65862", "files_changed": ["CODEOWNERS"], "labels": ["cla signed"]}, "ea776fa034": {"title": "Update CODEOWNERS for optim (#65773)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/65773\n\nReviewed By: mrshenli\n\nDifferential Revision: D31269749\n\nPulled By: albanD\n\nfbshipit-source-id: 1ec35d2396797b8e97a7122e2b3a9021f8fcf0a0", "pr_number": "65773", "files_changed": ["CODEOWNERS"], "labels": ["cla signed"]}, "541eb1db63": {"title": "Add cuSPARSE descriptors and update CSR addmm (#60838)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/60838\n\nRewrote `addmm_out_sparse_csr_dense_cuda` implementation using new cusparse descriptors.\n\n`addmm` now works without conversions with both 32-bit and 64-bit indices.\nThe dense tensors can have a row- or column-major layout. If the dense tensors are a contiguous slice of a larger tensor, the storage is used directly without temporary copies.\n\nTest Plan: Imported from OSS\n\nReviewed By: pbelevich\n\nDifferential Revision: D30643191\n\nPulled By: cpuhrsch\n\nfbshipit-source-id: 5555f5b59b288daa3a3987d322a93dada63b46c8", "pr_number": "60838", "files_changed": ["BUILD.bazel", "aten/src/ATen/cuda/CUDADataType.h", "aten/src/ATen/cuda/CUDASparse.h", "aten/src/ATen/cuda/CUDASparseDescriptors.cpp", "aten/src/ATen/cuda/CUDASparseDescriptors.h", "aten/src/ATen/cuda/CuSparseHandlePool.cpp", "aten/src/ATen/native/LinearAlgebraUtils.h", "aten/src/ATen/native/sparse/cuda/SparseBlasImpl.cpp", "aten/src/ATen/native/sparse/cuda/SparseBlasImpl.h", "aten/src/ATen/native/sparse/cuda/SparseCsrTensorMath.cu", "test/test_sparse_csr.py", "torch/testing/_internal/common_cuda.py"], "labels": ["module: sparse", "open source", "cla signed", "ciflow/default", "ciflow/cuda", "ciflow/win"]}, "24f59fa20b": {"title": "[ci] fix softmax bc check (#65952)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/65952\n\nTest Plan: Imported from OSS\n\nReviewed By: soulitzer\n\nDifferential Revision: D31320441\n\nPulled By: suo\n\nfbshipit-source-id: ddd2ccca523d7ed31b231d924fbd6206525f16cf", "pr_number": "65952", "files_changed": ["test/backward_compatibility/check_backward_compatibility.py"], "labels": ["ciflow/default"]}, "8f3983254b": {"title": "[MicroBench] Added a micro benchmark for prefix sum (#65790)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65790\n\nHere are the results of the benchmark:\n\n* ATen - version that calls `at::cumsum`\n* NNC - a simple prefix-sum loop implemented in NNC (not vectorized)\n* Local - a C++ implementation of the simple prefix-sum loop\n* LocalAVX2 - a vectorized C++ implementation of prefix-sum, only using AVX2\n* LocalAVX512 - a vectorized C++ implementation of prefix-sum, using AVX512.\n\nThe vectorized implementations are from the paper \"Parallel Prefix Sum with SIMD\" in ADMS' 20.\n\n```\n$ OMP_NUM_THREADS=1 ./buck-out/opt/gen/caffe2/benchmarks/cpp/tensorexpr/tensorexpr_bench --benchmark_filter=PrefixSumBench\nRun on (36 X 1601 MHz CPU s)\n2021-09-28 23:13:12\n------------------------------------------------------------------------------------------\nBenchmark                                   Time           CPU Iterations UserCounters...\n------------------------------------------------------------------------------------------\nPrefixSumBench/ATen/64                   1289 ns       1289 ns     543199 GB/s=397.069M/s\nPrefixSumBench/ATen/256                  1867 ns       1867 ns     374232 GB/s=1096.8M/s\nPrefixSumBench/ATen/1024                 4169 ns       4169 ns     167889 GB/s=1.9649G/s\nPrefixSumBench/ATen/4096                14137 ns      14136 ns      49266 GB/s=2.31806G/s\nPrefixSumBench/ATen/16384               49887 ns      49883 ns      13988 GB/s=2.6276G/s\nPrefixSumBench/ATen/65536              193742 ns     193686 ns       3628 GB/s=2.7069G/s\nPrefixSumBench/ATen/262144             764803 ns     764774 ns        917 GB/s=2.74219G/s\nPrefixSumBench/ATen/1048576           3040653 ns    3040277 ns        231 GB/s=2.75916G/s\nPrefixSumBench/Local/64                   586 ns        586 ns    1197003 GB/s=873.244M/s\nPrefixSumBench/Local/256                 1077 ns       1077 ns     646265 GB/s=1.90143G/s\nPrefixSumBench/Local/1024                3050 ns       3050 ns     229458 GB/s=2.68579G/s\nPrefixSumBench/Local/4096               11910 ns      11910 ns      58953 GB/s=2.75132G/s\nPrefixSumBench/Local/16384              43204 ns      43202 ns      16081 GB/s=3.03393G/s\nPrefixSumBench/Local/65536             167966 ns     167966 ns       4154 GB/s=3.12139G/s\nPrefixSumBench/Local/262144            667631 ns     667613 ns       1048 GB/s=3.14127G/s\nPrefixSumBench/Local/1048576          2654785 ns    2654631 ns        264 GB/s=3.15999G/s\nPrefixSumBench/NNC/64                     642 ns        642 ns    1095277 GB/s=797.442M/s\nPrefixSumBench/NNC/256                   1139 ns       1138 ns     617214 GB/s=1.799G/s\nPrefixSumBench/NNC/1024                  3103 ns       3103 ns     225531 GB/s=2.63979G/s\nPrefixSumBench/NNC/4096                 12053 ns      12052 ns      58084 GB/s=2.71883G/s\nPrefixSumBench/NNC/16384                43227 ns      43225 ns      16192 GB/s=3.03231G/s\nPrefixSumBench/NNC/65536               168065 ns     168056 ns       4153 GB/s=3.11972G/s\nPrefixSumBench/NNC/262144              668974 ns     668921 ns       1045 GB/s=3.13513G/s\nPrefixSumBench/NNC/1048576            2657464 ns    2657341 ns        263 GB/s=3.15677G/s\nPrefixSumBench/LocalAVX2/64               523 ns        523 ns    1351308 GB/s=979.537M/s\nPrefixSumBench/LocalAVX2/256              755 ns        755 ns     927762 GB/s=2.71159G/s\nPrefixSumBench/LocalAVX2/1024            1759 ns       1759 ns     400355 GB/s=4.65609G/s\nPrefixSumBench/LocalAVX2/4096            6708 ns       6706 ns     103959 GB/s=4.88649G/s\nPrefixSumBench/LocalAVX2/16384          22143 ns      22142 ns      31229 GB/s=5.91951G/s\nPrefixSumBench/LocalAVX2/65536          83649 ns      83642 ns       8350 GB/s=6.26828G/s\nPrefixSumBench/LocalAVX2/262144        330433 ns     330427 ns       2133 GB/s=6.34679G/s\nPrefixSumBench/LocalAVX2/1048576      1302301 ns    1302179 ns        537 GB/s=6.44198G/s\nPrefixSumBench/LocalAVX512/64             474 ns        474 ns    1459151 GB/s=1080.8M/s\nPrefixSumBench/LocalAVX512/256            576 ns        576 ns    1217442 GB/s=3.55524G/s\nPrefixSumBench/LocalAVX512/1024           994 ns        994 ns     703387 GB/s=8.24434G/s\nPrefixSumBench/LocalAVX512/4096          3642 ns       3641 ns     190646 GB/s=8.99857G/s\nPrefixSumBench/LocalAVX512/16384        10140 ns      10140 ns      68947 GB/s=12.9267G/s\nPrefixSumBench/LocalAVX512/65536        35739 ns      35736 ns      19567 GB/s=14.6711G/s\nPrefixSumBench/LocalAVX512/262144      156415 ns     156413 ns       4467 GB/s=13.4078G/s\nPrefixSumBench/LocalAVX512/1048576     613952 ns     613876 ns       1144 GB/s=13.665G/s\n```\n\nTest Plan: Imported from OSS\n\nReviewed By: bertmaher\n\nDifferential Revision: D31253849\n\nPulled By: navahgar\n\nfbshipit-source-id: f33e7be787c86a09e90babddd66b16e2e0777eb4", "pr_number": "65790", "files_changed": ["benchmarks/cpp/tensorexpr/CMakeLists.txt", "benchmarks/cpp/tensorexpr/bench_prefix_sum.cpp"], "labels": ["cla signed"]}, "70f9f58a71": {"title": "Add __module__ to torch.dtype.__dict__ (#65182)", "body": "Summary:\ntorch.dtype.__reduce__ returns a string, which causes Pickle to look\nup the object by module and name. In order to find the right module,\nPickle looks for __module__ on the object; if it doesn't find that, it\nfalls back to searching sys.modules.\n\nPreviously, torch.dtype instances did not have a `__module__`\nattribute, so pickling dtypes would fall back to a search of\nsys.module.\n\nInstances of normal Python objects have a `__module__` attribute\nbecause normal Python classes have a `__module__` key in their\n`__dict__`. Imitate that by populating one in `torch.dtype`.\n\nWe set the field in `tp_dict` before calling `PyType_Ready` (instead\nof afterwards) because of the doc warning against mutating a type's\ndictionary once initialized:\nhttps://docs.python.org/3/c-api/typeobj.html#c.PyTypeObject.tp_dict\n\nfixes https://github.com/pytorch/pytorch/issues/65077\n\n ---\n\nI didn't add any tests because I didn't see any obvious places with similar tests for pickling or dtype objects. Let me know if I missed the right place, or should start one.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65182\n\nReviewed By: mrshenli\n\nDifferential Revision: D31310530\n\nPulled By: ezyang\n\nfbshipit-source-id: 20cd713ce175a709d6ce47459c3891162ce29d77", "pr_number": "65182", "files_changed": ["torch/csrc/Dtype.cpp"], "labels": ["triaged", "open source", "cla signed"]}, "6285348f06": {"title": "Implement n-dimensional hermitian FFTs (#63890)", "body": "Summary:\nCloses https://github.com/pytorch/pytorch/issues/59127\n\ncc mruberry peterbell10 walterddr\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/63890\n\nReviewed By: ngimel\n\nDifferential Revision: D30761909\n\nPulled By: mruberry\n\nfbshipit-source-id: 06e1e4dc65726f35c99a74f18b9fa36eb7d694a5", "pr_number": "63890", "files_changed": ["aten/src/ATen/native/SpectralOps.cpp", "aten/src/ATen/native/native_functions.yaml", "test/test_spectral_ops.py", "torch/fft/__init__.py", "torch/overrides.py", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["triaged", "open source", "module: fft", "cla signed", "ci/master", "ciflow/slow-gradcheck", "ciflow/default"]}, "b3da2afebe": {"title": "Clarified difference in behavior of `empty_strided` and `as_strided` (#64568)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/64568\n\nFix: #64389\n\nTest Plan: Imported from OSS\n\nReviewed By: mrshenli\n\nDifferential Revision: D31299999\n\nPulled By: mruberry\n\nfbshipit-source-id: dd538ffa7cc1267ab6472806f4216b170dd0faad", "pr_number": "64568", "files_changed": ["torch/_torch_docs.py"], "labels": ["open source", "cla signed"]}, "f6dfac6974": {"title": "Migrate THCCachingHostAllocator to ATen (#65746)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65746\n\nThis also removes the cudaHostAllocator field on THCState, since there\ndoesn't seem to be an API anywhere for customizing it.\n\nTest Plan: Imported from OSS\n\nReviewed By: mrshenli\n\nDifferential Revision: D31236630\n\nPulled By: ngimel\n\nfbshipit-source-id: 2a8e756222ae70565e77f8e7139d60ec5be32276", "pr_number": "65746", "files_changed": ["BUILD.bazel", "aten/src/ATen/cuda/CachingHostAllocator.cpp", "aten/src/ATen/cuda/CachingHostAllocator.h", "aten/src/ATen/cuda/PinnedMemoryAllocator.cpp", "aten/src/ATen/cuda/PinnedMemoryAllocator.h", "aten/src/ATen/native/cuda/Copy.cu", "aten/src/THC/CMakeLists.txt", "aten/src/THC/THC.h", "aten/src/THC/THCCachingHostAllocator.cpp", "aten/src/THC/THCCachingHostAllocator.h", "aten/src/THC/THCGeneral.cpp", "aten/src/THC/THCGeneral.h.in", "aten/src/THC/THCGeneral.hpp", "torch/csrc/cuda/Module.cpp"], "labels": ["module: porting", "open source", "cla signed", "ciflow/default"]}, "08df4c2b3c": {"title": "slow_conv2d grad_input: avoid dispatch in parallel region (#65725)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65725\n\nSee gh-56794\n\nAvoid dispatch inside of parallel_for by:\n1. Replacing Tensor slicing with TensorAccessor\n2. Call `grad_input.zero_()` only once, outside of the parallel region\n3. Replace `at::mm` with a `gemm` call\n\nTest Plan: Imported from OSS\n\nReviewed By: saketh-are\n\nDifferential Revision: D31257876\n\nPulled By: ngimel\n\nfbshipit-source-id: f2902edeccd161431c1dfb1ab3e165d039ec259d", "pr_number": "65725", "files_changed": ["aten/src/ATen/native/ConvolutionMM2d.cpp"], "labels": ["open source", "cla signed", "ciflow/default"]}, "ea0de37d2e": {"title": "[PyTorch] Avoid string construction from const char* and speedup empty string creation if error messages are suppressed (#65939)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65939\n\nThis change includes 2 separate optimizations.\n\n1. Provide an overload of `debugString(const char*, ...)` in addition to `debugString(std::string, ...)` for cases where `const char*` is passed in to avoid `std::string` construction in cases where `STRIP_ERROR_MESSAGES` is also defined and the caller is passing in a `const char*`\n2. Return `std::string(\"\", 0)` instead of `\"\"` since the former triggers no call to `std::basic_string`'s constructor whereas the latter does. [Godbolt Link](https://godbolt.org/z/oTExed5h8). However, I'm surprosed by this since the man page for [std::basic_string](https://en.cppreference.com/w/cpp/string/basic_string/basic_string) clearly states that the constexpr overload is since C++20, and I am building using `-Os -std=c++17`\n\nGodbolt Screenshot:\n\n{F667311023}\n\nghstack-source-id: 139507542\n\nTest Plan:\nCI and local build via:\n\n```\nbuck build //xplat/caffe2/fb/lite_predictor:lite_predictor\n```\n\nReviewed By: swolchok\n\nDifferential Revision: D31312942\n\nfbshipit-source-id: aa24abbfe1c16419f235d037595321982614c5ea", "pr_number": "65939", "files_changed": ["aten/src/ATen/core/library.cpp"], "labels": ["cla signed", "ciflow/default"]}, "8b1aa85388": {"title": "[sparsity] Change API to take FQNs as configuration (#65296)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65296\n\nThe original API described in the https://github.com/pytorch/pytorch/issues/59835\nassumed that the per-layer configuration would take a module/layer\nreference. However, a more useful approach is to refer to the layers\nby their fully qualified names (FQN). That allows us to store the\nconfiguration in a file without serializing the models.\n\nWe define a layer's FQN as it's \"path\" within a model. For example,\nif one can refer to a model using `model.layer0.sublayerX`, the FQN\nof the sublayerX is `'layer0.sublayerX'`.\n\nTest Plan:\n```\npython test/test_ao_sparsity.py -- TestBaseSparsifier\nbuck test mode/opt //caffe2:test -- TestBaseSparsifier\n```\n\nReviewed By: gchanan\n\nDifferential Revision: D31186830\n\nPulled By: z-a-f\n\nfbshipit-source-id: d8d87f1c054e5c10d470e67837476a11e0a9b1d4", "pr_number": "65296", "files_changed": ["test/ao/sparsity/test_sparsifier.py", "torch/ao/sparsity/sparsifier/base_sparsifier.py"], "labels": ["cla signed"]}, "c27b427cd9": {"title": "[sparsity] Add m-out-of-n support in the WeightNormSparsifier (#65295)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65295\n\nThe m-out-of-n is implemented as follows:\n\n1. Compute the blocks that need to be sparsified using the weight-norm criterion\n2. Within each block below the threshold find the smallest absolute value elements\n3. Zero out only the smallest values within each block\n\nm-out-of-n describes sparsification scheme where in a block with \"n\" elements, only \"m\" of them would be zeroed-out.\nBlock sparsity, with the whole block being all zeros, is a special case of m-out-n: If m==n, the whole block is reset.\n\nThis echoes the implementation described in the https://github.com/pytorch/pytorch/issues/59835,\nas well as meets the support of the nVidia cusparselt requirements.\nTo support the CUDA sparsity (2/4), one would need to set the sparsity_level to 1.0.\nThat translates to all blocks of shape 1x4 within a tensor will sprasify with 2-out-4 scheme.\n\nTest Plan: Imported from OSS\n\nReviewed By: vkuzo\n\nDifferential Revision: D31186828\n\nPulled By: z-a-f\n\nfbshipit-source-id: 7bd3e2707915b90f4831859781fc6e25f716c618", "pr_number": "65295", "files_changed": ["test/ao/sparsity/test_sparsifier.py", "torch/ao/sparsity/sparsifier/weight_norm_sparsifier.py"], "labels": ["cla signed"]}, "dac35b3592": {"title": "pytorch quantization ao migration phase 2: torch/jit (#65829)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65829\n\nRenames `torch.quantization` to `torch.ao.quantization` in `torch/jit` folder.\n\n```\nfind caffe2/torch/jit/ -type f -name \"*.py\" -print0 | xargs -0 sed -i \"s/torch\\.quantization/torch.ao.quantization/g\"\n```\n\nTest Plan: CI\n\nReviewed By: z-a-f\n\nDifferential Revision: D31273365\n\nfbshipit-source-id: 350eb116148d91b967d428b54413caee4fd68438", "pr_number": "65829", "files_changed": ["torch/jit/_recursive.py", "torch/jit/quantized.py"], "labels": ["oncall: jit", "fb-exported", "cla signed", "ciflow/default"]}, "8595b6eeed": {"title": "Avoid UB when indexing into size-0 tensors (#65878)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65878\n\nIf we attempt to compute an offset into an empty tensor we trigger UB, since\nwe'd be adding an offset to a nullptr, which is UB\n(https://reviews.llvm.org/D67122) even if we never use the pointer.\n\nSince indexing into an empty tensor yields an empty tensor anyways, let's just\nreturn the underlying (null) data ptr in this case.\n\nghstack-source-id: 139448496\n\nTest Plan:\nr-barnes originally pointed this out to me in a failing TE fuser test:\nhttps://www.internalfb.com/intern/testinfra/diagnostics/5910974579561425.281475022329152.1632898053/\n```\nbuck test mode/dev //caffe2/test:jit -- --exact 'caffe2/test:jit - test_unsupported_nn_functional_pad_circular_cpu_float32 (test_jit_fuser_te.TestNNCOpInfoCPU)'\n```\n\nBut it turns out it's easily triggered by anything that tries to operate on a\nslice of a size-0 tensor:\n```\ndef test_pad(self):\n    F.pad(torch.ones(0, 3, 3), (1, 2), 'circular')\n\ndef test_index(self):\n    input = torch.zeros(0, 3, 3)\n    out = torch.zeros(0, 3, 6)\n    out[..., 1:4] = input[..., 0:3]\n\ndef test_add(self):\n    torch.ones(0, 2)[:, 1] + torch.ones(0, 1)\n```\n\nWhat's the right place for these sort of operator corner-case tests?  Should\nthey be/are they part of OpInfo?\n\nReviewed By: jamesr66a\n\nDifferential Revision: D31296914\n\nfbshipit-source-id: 0ef52ad311dceeed985498f8d9390bc6fbaefbfc", "pr_number": "65878", "files_changed": ["c10/core/TensorImpl.h"], "labels": ["cla signed"]}, "53c0d91db9": {"title": "Make autograd codegen for differentiable outputs safer to use (#65823)", "body": "Summary:\nThis PR adds raising an error when `len(output_differentiability) != len(outputs)`\n\nNotes in derivatives.yml tell that\n> 'output_differentiability' and value a list of the same length as the number of outputs from the forward function.\n\nbut it was not enforced in codegen leading to confusion and unexpected bugs https://github.com/pytorch/pytorch/issues/65061#issuecomment-930271126.\n\ncc ezyang albanD zou3519 gqchen pearu nikitaved soulitzer Lezcano Varal7\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65823\n\nReviewed By: mrshenli\n\nDifferential Revision: D31307312\n\nPulled By: albanD\n\nfbshipit-source-id: caeb949e9249310dffd237e77871e6d0d784e298", "pr_number": "65823", "files_changed": ["tools/autograd/derivatives.yaml", "tools/codegen/api/autograd.py"], "labels": ["module: autograd", "open source", "module: codegen", "cla signed", "ciflow/default"]}, "383c0a3858": {"title": "Fix internal assert failure for torch.all and torch.any with requires_grad=True (#65714)", "body": "Summary:\nThis PR fixes https://github.com/pytorch/pytorch/issues/58547.\nI added an OpInfo-based test that fails on master and passes with the\nproposed changes.\n\ncc ezyang albanD zou3519 gqchen pearu nikitaved soulitzer Lezcano Varal7 mruberry\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65714\n\nReviewed By: saketh-are, mruberry\n\nDifferential Revision: D31248307\n\nPulled By: albanD\n\nfbshipit-source-id: 041eaa9b744c3043f78dd8ae5f457f67c311df4f", "pr_number": "65714", "files_changed": ["test/test_ops.py", "tools/autograd/derivatives.yaml", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["module: autograd", "module: tests", "triaged", "open source", "cla signed", "ciflow/default"]}, "21eebc9fd6": {"title": "[PyTorch][easy] Use copy-and-move instead of copy-and-swap in IValue::operator= (#65826)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65826\n\nShould be marginally more efficient.\nghstack-source-id: 139315050\n\nTest Plan: CI\n\nReviewed By: ezyang\n\nDifferential Revision: D31272489\n\nfbshipit-source-id: 7c309d67a0ec0ada35a5b62497bac374538394a9", "pr_number": "65826", "files_changed": ["aten/src/ATen/core/ivalue.h"], "labels": ["cla signed", "ciflow/default"]}, "6e8ffd191e": {"title": "Fix typo in name of LayerNormBackwardCUDAKernel (#66000)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66000\n\nSaw this in nvprof and I'm just a little too nitpicky to let it slide!\nghstack-source-id: 139547271\n\nTest Plan: CI\n\nReviewed By: xiaomengy\n\nDifferential Revision: D31340262\n\nfbshipit-source-id: ab48dc99c34a74585e66800b4bbcccc6aabbaff2", "pr_number": "66000", "files_changed": ["aten/src/ATen/native/cuda/layer_norm_kernel.cu", "caffe2/operators/layer_norm_op.cu"], "labels": ["cla signed"]}, "ad889d0b5e": {"title": "Revert D30634700: [pytorch][PR] Fix typo in tensor docs", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD30634700 (https://github.com/pytorch/pytorch/commit/d937473709956ddb87257360445f9b2d0b2fec55)\n\nOriginal commit changeset: e8952be20966\n\nfbshipit-source-id: b18694e332023abcdf17ec1900b81b00d21f1014", "pr_number": null, "files_changed": ["docs/source/tensors.rst"], "labels": []}, "d9a95e66f0": {"title": "Upload test failures to RDS (#65873)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/65873\n\nTest Plan: Imported from OSS\n\nReviewed By: janeyx99\n\nDifferential Revision: D31296520\n\nPulled By: driazati\n\nfbshipit-source-id: 0bd3fb6b62e49c7177199001fda0e7b124a22ab2", "pr_number": "65873", "files_changed": ["tools/stats/print_test_stats.py"], "labels": ["cla signed", "ciflow/default"]}, "23caeb3f71": {"title": "model_dump: Add a helper to produce html with a single call (#66005)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66005\n\nghstack-source-id: 139342091\n\nTest Plan: Unit test, and used in a notebook.\n\nReviewed By: dhruvbird\n\nDifferential Revision: D31281091\n\nfbshipit-source-id: 1e4d0713b9796a3d182de9e676c3b3c3b1610d6e", "pr_number": "66005", "files_changed": ["test/test_model_dump.py", "torch/utils/model_dump/__init__.py"], "labels": ["cla signed", "ciflow/default"]}, "e1d963e8fc": {"title": "model_dump: Fix memory computation when both constants and data tensors are present (#66006)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66006\n\nPreviously, this was resulting in a key collision and a crash.\nghstack-source-id: 139342089\n\nTest Plan: Ran webdriver test locally.\n\nReviewed By: dhruvbird\n\nDifferential Revision: D31281092\n\nfbshipit-source-id: f31311726c681d6d7e0504ff8e84c888af9054f0", "pr_number": "66006", "files_changed": ["test/test_model_dump.py", "torch/utils/model_dump/code.js"], "labels": ["cla signed", "ciflow/default"]}, "10f6294281": {"title": "Fix shape inference dim_type for Clip, Mean, Div (#65996)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/65996\n\nTest Plan:\nFacebook\n```\nbuck build caffe2/caffe2/opt:bound_shape_inference_test && ./buck-out/gen/caffe2/caffe2/opt/bound_shape_inference_test --gtest_filter=*Clip*\n```\n```\nbuck build caffe2/caffe2/opt:bound_shape_inference_test && ./buck-out/gen/caffe2/caffe2/opt/bound_shape_inference_test --gtest_filter=*Div*\n```\n```\nbuck build caffe2/caffe2/opt:bound_shape_inference_test && ./buck-out/gen/caffe2/caffe2/opt/bound_shape_inference_test --gtest_filter=*Mean*\n```\n\nReviewed By: yinghai\n\nDifferential Revision: D31121298\n\nfbshipit-source-id: f366d8f4d4d0be159b62bfaafc42ca924c05e022", "pr_number": "65996", "files_changed": ["caffe2/opt/bound_shape_inference_test.cc", "caffe2/opt/bound_shape_inferencer.cc", "caffe2/opt/bound_shape_inferencer.h"], "labels": ["fb-exported", "cla signed"]}, "eb3b9fe719": {"title": "[XROS][ML] System specific adjustments for UTs to work. (#65245)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65245\n\nBuilding and running c10 and qnnpack tests on XROS.\n\nNotable changes:\n- Adding #if define(_XROS_) in few places not supported by XROS\n- Changing Threadpool to abstract class\nghstack-source-id: 139513579\n\nTest Plan: Run c10 and qnnpack tests on XROS.\n\nReviewed By: veselinp, iseeyuan\n\nDifferential Revision: D30137333\n\nfbshipit-source-id: bb6239b935187fac712834341fe5a8d3377762b1", "pr_number": "65245", "files_changed": ["c10/core/GeneratorImpl.cpp", "c10/macros/Macros.h", "c10/util/Logging.cpp", "c10/util/Type.cpp", "caffe2/utils/threadpool/ThreadPool.cc", "caffe2/utils/threadpool/ThreadPool.h", "caffe2/utils/threadpool/pthreadpool-cpp.cc", "caffe2/utils/threadpool/pthreadpool.h", "caffe2/utils/threadpool/pthreadpool_impl.cc"], "labels": ["cla signed"]}, "5ef350d7cc": {"title": "Revert D31359010: [pytorch][PR] Fix cang-tidy regressions caused by #65954", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD31359010 (https://github.com/pytorch/pytorch/commit/c269f471f45be536bdbdc41ac681ccb730877ece)\n\nOriginal commit changeset: dce4b91a9891\n\nfbshipit-source-id: 085417432b6748d3672b9b7141460f47d1c17a7f", "pr_number": null, "files_changed": ["CMakeLists.txt", "test/benchmark_utils/test_benchmark_utils.py", "torch/csrc/deploy/loader.cpp", "torch/csrc/deploy/test_deploy.cpp", "torch/utils/benchmark/utils/timeit_template.cpp", "torch/utils/benchmark/utils/valgrind_wrapper/timer_callgrind_template.cpp"], "labels": []}, "0fc6bd2e47": {"title": "[gpu ne eval] disable adam decay unit test for gpu (#66056)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66056\n\nkeep running into this unrelated failure when landing diffs regarding the gpu inference project,\ndisabling this operator unit test in gpu because it doesn't exist\n\nRuntimeError: [enforce fail at operator.cc:277] op. Cannot create operator of type 'SmartDecaySparseAdam' on the device 'CUDA'. Verify that implementation for the corresponding device exist. It might also happen if the binary is not linked with the operator implementation code. If Python frontend is used it might happen if dyndep.InitOpsLibrary call is missing. Operator def: input: \"param\" input: \"mom1\" input: \"mom2\" input: \"last_seen\" input: \"indices\" input: \"grad\" input: \"lr\" input: \"iter\" output: \"param\" output: \"mom1\" output: \"mom2\" output: \"last_seen\" name: \"\" type: \"SmartDecaySparseAdam\" arg { name: \"beta1\" f: 0 } arg { name: \"beta2\" f: 0.9 } arg { name: \"epsilon\" f: 1e-05 } device_option { device_type: 1 }\n\nhttps://www.internalfb.com/intern/testinfra/diagnostics/5910974579962988.562949996565057.1633122845/\n\nTest Plan: sandcastle\n\nReviewed By: jianyuh\n\nDifferential Revision: D31364731\n\nfbshipit-source-id: 7fbd994cbe7f6ca116f5f34506a1ed7f14759bdf", "pr_number": "66056", "files_changed": ["caffe2/python/operator_test/adam_test.py"], "labels": ["fb-exported", "cla signed", "ciflow/default"]}, "c7748fc172": {"title": "Added validation of mode parameter in AveragedModel (#65921)", "body": "Summary:\nDiscussion: https://github.com/pytorch/pytorch/pull/65495#issuecomment-930460469\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65921\n\nReviewed By: albanD\n\nDifferential Revision: D31310105\n\nPulled By: prabhat00155\n\nfbshipit-source-id: 417691832a7c793744830c11e0ce53e3972d21a3", "pr_number": "65921", "files_changed": ["torch/optim/swa_utils.py"], "labels": ["cla signed"]}, "40948a935d": {"title": "Fix LLVM-12 UB in generate_proposals_op.cc (#66009)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66009\n\nFixes\n```\ntest_trace_c10_ops (jit.test_tracer.TestTracer) ... third-party-buck/platform009/build/eigen/include/Eigen/src/Core/Block.h:374:24: runtime error: applying non-zero offset 4 to null pointer\n    #0 0x7f5228f72227 in Eigen::internal::BlockImpl_dense<Eigen::Map<Eigen::Array<float, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> >, -1, -1, false, true>::BlockImpl_dense(Eigen::Map<Eigen::Array<float, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> >&, long, long, long, long) third-party-buck/platform009/build/eigen/include/Eigen/src/Core/Block.h:374\n    #1 0x7f5228f7212c in Eigen::BlockImpl<Eigen::Map<Eigen::Array<float, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> >, -1, -1, false, Eigen::Dense>::BlockImpl(Eigen::Map<Eigen::Array<float, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> >&, long, long, long, long) third-party-buck/platform009/build/eigen/include/Eigen/src/Core/Block.h:166\n    #2 0x7f5228f720dc in Eigen::Block<Eigen::Map<Eigen::Array<float, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> >, -1, -1, false>::Block(Eigen::Map<Eigen::Array<float, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> >&, long, long, long, long) third-party-buck/platform009/build/eigen/include/Eigen/src/Core/Block.h:142\n    #3 0x7f5229b0e059 in Eigen::DenseBase<Eigen::Map<Eigen::Array<float, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> > >::FixedBlockXpr<internal::get_fixed_value<int>::value, internal::get_fixed_value<long>::value>::Type Eigen::DenseBase<Eigen::Map<Eigen::Array<float, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> > >::block<int, long>(long, long, int, long) third-party-buck/platform009/build/eigen/include/Eigen/src/Core/../plugins/BlockMethods.h:98\n    #4 0x7f5229b0c5ca in caffe2::GenerateProposalsOp<caffe2::CPUContext>::RunOnDevice() caffe2/caffe2/operators/generate_proposals_op.cc:348\n```\nAlso cleans up some data type and const issues around the area.\n\nTest Plan: Sandcastle\n\nReviewed By: xush6528\n\nDifferential Revision: D31343046\n\nfbshipit-source-id: fd9096c8e47a0aad529c72fd313f64ca98dcb80b", "pr_number": "66009", "files_changed": ["caffe2/operators/generate_proposals_op.cc"], "labels": ["fb-exported", "cla signed", "ciflow/default"]}, "89ed9bdaee": {"title": "[Static Runtime] Fix bug of creating output aliases in aten::embedding_bag (#65516)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65516\n\nThis change fixes a bug that Static Runtime's `aten::embedding_bag` out variant implementation creates aliases in its managed output tensors.\n\nManaged output tensors should never be an alias with each other since writing to them can illegally overwrite others' contents unintentionally, and this exact problem was causing the bug at T97393697, causing SR to return wrong return values.\n\nThis bug is detected in inline_cvr/remote_ro by a DCHECK, `verify_no_memory_overlap` (introduced by D30211705 (https://github.com/pytorch/pytorch/commit/3fb33b38b9edfb294b32b61e6c6133822e48f215)), but wasn't found so far since our testing didn't include running the model in the debug mode. Fortunately this bug is not hitting production since the aliases outputs are not used in production.\n\nThis change fixes the root cause from `_embedding_bag_cpu_impl_out`  by replacing alias creation with copying.\n\nNote that this change also includes a fundamental change in Static Runtime's unit testing: `testStaticRuntime` exercises the given graph 3 times:\n 1. profile run\n 2. run using the profile to allocate managed tensors\n 3. reuse the managed tensors -- newly added\n\nAdding 3 reveals this bug with a new unittest `EmbeddingBagWithManagedOutput`.\n\nTest Plan:\n- Confirmed that the crash experienced by `StaticRuntime.EmbeddingBagWithManagedOutput` disappears with this change (crash paste: P459807248).\n\n- Added `StaticRuntime.EmbeddingBagWithManagedOutput` to detect the same problem in the future.\n\nReviewed By: hlu1\n\nDifferential Revision: D31104345\n\nfbshipit-source-id: 7bddf9cd82b400d18d8ce1bf15e29b815ef9ba8f", "pr_number": "65516", "files_changed": ["aten/src/ATen/native/EmbeddingBag.cpp", "benchmarks/static_runtime/test_static_runtime.cc", "benchmarks/static_runtime/test_utils.cc"], "labels": ["oncall: jit", "fb-exported", "cla signed"]}, "b6d5f1ee70": {"title": "Allow None to pass through for vmap (#65565)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65565\n\nDoes jax allow this?\n\nTest Plan: Imported from OSS\n\nReviewed By: mrshenli\n\nDifferential Revision: D31236258\n\nPulled By: soulitzer\n\nfbshipit-source-id: 80460b355fc32ecbba8151e1f3179f076a927f9d", "pr_number": "65565", "files_changed": ["torch/_vmap_internals.py"], "labels": ["cla signed", "ciflow/default"]}, "73901b099d": {"title": "Add batched_grad parameter to `autograd.grad` (#65564)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65564\n\n- wrap the call into engine with vmap if `batched_grad` is `True`\n- improves the comment on the call to engine (somewhat addressing https://github.com/pytorch/pytorch/issues/41659)\n- borrows the message from functional.jacobian's vectorized argument concerning usage of the vmap feature\n- adds basic test (further testing is done when we replace the usage in vectorized jacobian computation)\n\nTODO:\n - create an issue tracking this\n\nTest Plan: Imported from OSS\n\nReviewed By: albanD\n\nDifferential Revision: D31236259\n\nPulled By: soulitzer\n\nfbshipit-source-id: b33e6b26ea98fa9f70c44da08458fc54ba4df0f7", "pr_number": "65564", "files_changed": ["test/test_autograd.py", "torch/autograd/__init__.py"], "labels": ["cla signed", "ciflow/default"]}, "8f5631b859": {"title": "Refactor functional api vectorized jacobian to use batched grad parameter (#65566)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65566\n\nThis doesn't simplify vectorized jacobian computation, but is good to consolidate logic and helps us to test the logic\n\nTest Plan: Imported from OSS\n\nReviewed By: albanD\n\nDifferential Revision: D31236257\n\nPulled By: soulitzer\n\nfbshipit-source-id: 00ca0aa6519bed5f9ee2c7be4daa8872af5e92cd", "pr_number": "65566", "files_changed": ["torch/autograd/functional.py"], "labels": ["cla signed", "ciflow/default"]}, "df475aa1dc": {"title": "Update Vulkan runner in benchmark binary to handle non-tensor inputs (#66123)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66123\n\nSome models may take in a list of tensors as inputs, thus the bundled inputs will contain `IValues` that are of the type `c10::List`. For Vulkan models, every tensor in the `IValue` list has to be converted to a vulkan tensor first, and this case is not currently handled by the Vulkan model wrapper in the benchmark binary.\n\nThis diff introduces `IValue` type checking to the input processor of the Vulkan model wrapper, and adds support for Tensor and List types.\n\nTest Plan:\n```\n# Build the binary\ncd ~/fbsource\nbuck build -c ndk.custom_libcxx=false -c pt.enable_qpl=0 //xplat/caffe2:ptmobile_compareAndroid\\#android-arm64 --show-output\n# Push it to the device\nadb push buck-out/gen/xplat/caffe2/ptmobile_compareAndroid\\#android-arm64 /data/local/tmp/compare_models\n\n# Run the benchmark binary\nBENCH_CMD=\"/data/local/tmp/compare_models\"\nBENCH_CMD+=\" --model=$PATH_TO_MODEL\"\nBENCH_CMD+=\" --refmodel=$PATH_TO_REFERENCE_MODEL\"\nBENCH_CMD+=\" --input_type=float --input_dims=$MODEL_INPUT_SIZE\"\nBENCH_CMD+=\" --iter=100\"\nBENCH_CMD+=\" --tolerance 1e-5\"\n```\n\nReviewed By: beback4u\n\nDifferential Revision: D31276862\n\nfbshipit-source-id: 1d9abf958963da6ecad641202f0458402bee5ced", "pr_number": "66123", "files_changed": ["binaries/speed_benchmark_torch.cc"], "labels": ["fb-exported", "cla signed", "ciflow/default"]}, "aa80f05d2d": {"title": "Remove sync in Embedding caused by unique (#66091)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/66091\n\nReviewed By: albanD\n\nDifferential Revision: D31385576\n\nPulled By: ngimel\n\nfbshipit-source-id: e656d4d9c38b705c71853ca295f977d1cddc61a1", "pr_number": "66091", "files_changed": ["aten/src/ATen/native/cuda/Embedding.cu"], "labels": ["open source", "cla signed", "ciflow/default"]}, "1db78c30c9": {"title": "Fix LLVM-12 concat_split_op.h error (#66060)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66060\n\nFixes\n```\ntestTumHistoryAdditionalLaser (caffe2.caffe2.fb.layers.tests.tum_history_test.TestTumHistory) ... caffe2/caffe2/operators/concat_split_op.h:363:74: runtime error: applying non-zero offset 8 to null pointer\n    #0 0x7f8f39d29795 in caffe2::ConcatOp<caffe2::CPUContext>::RunOnDevice() caffe2/caffe2/operators/concat_split_op.h:363\n    #1 0x7f8f39c4978d in caffe2::Operator<caffe2::CPUContext>::Run(int) caffe2/caffe2/core/operator.h:987\n    #2 0x7f8f381fe9c9 in caffe2::SimpleNet::Run() caffe2/caffe2/core/net_simple.cc:67\n    #3 0x7f8f38ee488e in caffe2::Workspace::RunNet(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) caffe2/caffe2/core/workspace.cc:289\n```\n\nTest Plan: Sandcastle\n\nReviewed By: dzhulgakov, xush6528\n\nDifferential Revision: D31366205\n\nfbshipit-source-id: 566aa519677c9d371189e4b1f81d595732861efc", "pr_number": "66060", "files_changed": ["caffe2/operators/concat_split_op.h"], "labels": ["fb-exported", "cla signed", "ciflow/default"]}, "bda3230b62": {"title": "slow_conv2d grad_weight: call gemm directly (#65726)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65726\n\nThis PR isn't strictly necessary since grad_weight doesn't use\nparallel_for. However, this does reduce the function overhead and will\nmake it easier to parallelize in the future.\n\nTest Plan: Imported from OSS\n\nReviewed By: albanD\n\nDifferential Revision: D31257877\n\nPulled By: ngimel\n\nfbshipit-source-id: d8ea97cc1f43d8d9dfff355ae27c9d982838b57e", "pr_number": "65726", "files_changed": ["aten/src/ATen/native/ConvolutionMM2d.cpp"], "labels": ["open source", "cla signed", "ciflow/default"]}, "92d0b7e99c": {"title": "[deploy] fix typo in `registerModuleSource` (#66107)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66107\n\nlol\n\nTest Plan: Imported from OSS\n\nReviewed By: zdevito\n\nDifferential Revision: D31385631\n\nPulled By: suo\n\nfbshipit-source-id: a3307e2862f7951c160776eb8edb18329c937ed1", "pr_number": "66107", "files_changed": ["torch/csrc/deploy/deploy.cpp", "torch/csrc/deploy/deploy.h", "torch/csrc/deploy/test_deploy.cpp", "torch/csrc/deploy/test_deploy_python_ext.cpp"], "labels": ["cla signed", "ciflow/default"]}, "0d020effab": {"title": "[quant] Fix the parts that were missing after initial migration (#66058)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66058\n\nAfter the initial migration from `torch.quantization` to `torch.ao.quantization`, some of the files did not change.\nThis happened because the migration was done in parallel, and some of the files were landed while the others were still in the original location.\nThis is the last fix in the AO migration phase 1, which completely enables the ao.quantization namespace.\n\nTest Plan: `python test/test_quantization.py`\n\nReviewed By: vkuzo\n\nDifferential Revision: D31366066\n\nPulled By: z-a-f\n\nfbshipit-source-id: bf4a74885be89d098df2d87e685795a2a64026c5", "pr_number": "66058", "files_changed": ["torch/ao/__init__.py", "torch/ao/nn/sparse/quantized/dynamic/linear.py", "torch/ao/ns/_numeric_suite.py", "torch/ao/ns/_numeric_suite_fx.py", "torch/ao/ns/fx/graph_matcher.py", "torch/ao/ns/fx/graph_passes.py", "torch/ao/ns/fx/pattern_utils.py", "torch/ao/ns/fx/utils.py", "torch/ao/quantization/_correct_bias.py", "torch/ao/quantization/_learnable_fake_quantize.py", "torch/ao/quantization/fake_quantize.py", "torch/ao/quantization/fuse_modules.py", "torch/ao/quantization/fx/_equalize.py", "torch/ao/quantization/fx/prepare.py", "torch/ao/quantization/fx/qconfig_utils.py", "torch/ao/quantization/fx/quantization_patterns.py", "torch/ao/quantization/observer.py", "torch/ao/quantization/qconfig.py", "torch/ao/quantization/quantize.py", "torch/ao/quantization/quantize_fx.py", "torch/ao/quantization/quantize_jit.py"], "labels": ["cla signed", "ciflow/default", "fx"]}, "588c1787ba": {"title": "Update link to example pytorch/examples (#66095)", "body": "Summary:\n`https://github.com/goldsborough/examples/tree/cpp/cpp` -> `https://github.com/pytorch/examples/tree/master/cpp`\nAs C++ examples in  https://github.com/pytorch/examples are more update\n\nPartially addresses https://github.com/pytorch/pytorch/issues/65388\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66095\n\nReviewed By: janeyx99\n\nDifferential Revision: D31382888\n\nPulled By: malfet\n\nfbshipit-source-id: 8884c7795386249dea07cbe66783fa1dd963e07c", "pr_number": "66095", "files_changed": ["docs/cpp/source/frontend.rst", "docs/cpp/source/index.rst"], "labels": ["cla signed", "ciflow/default"]}, "a3bbaf227c": {"title": "Revert D31227448: [pytorch][PR] fixing sorting in stride indices", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD31227448 (https://github.com/pytorch/pytorch/commit/da0e29edd4def41c9d305468765db3d92e31419b)\n\nOriginal commit changeset: 51e3cd903757\n\nfbshipit-source-id: a752a4df70281aa0eaaeb1afdd88395b08276da8", "pr_number": null, "files_changed": ["aten/src/ATen/core/type.cpp", "aten/src/ATen/test/CMakeLists.txt", "aten/src/ATen/test/stride_properties_test.cpp"], "labels": []}, "e7747795c9": {"title": "[PyTorch Edge] Reduce dispatch table size further for a trimmed build (#66112)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66112\n\nEliminate Metal and Vulkan Dispatch Keys.\n\nTest Plan: Build + Sandcastle\n\nDifferential Revision: D31298307\n\nfbshipit-source-id: 31302fc626382db7997e5058750fa85458c9cbc1", "pr_number": "66112", "files_changed": ["c10/core/DispatchKey.h"], "labels": ["cla signed", "ciflow/default"]}, "7452b65144": {"title": "Remove unused `dump` method from VSX vec256 methods (#66085)", "body": "Summary:\nFollow up after https://github.com/pytorch/pytorch/pull/63533\n\nProbably fixes https://github.com/pytorch/pytorch/issues/65956\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66085\n\nReviewed By: ngimel\n\nDifferential Revision: D31382898\n\nPulled By: malfet\n\nfbshipit-source-id: f3d97b0f2c7f1207827773ae85e2739f1d54b9c7", "pr_number": "66085", "files_changed": ["aten/src/ATen/cpu/vec/vec256/vsx/vec256_complex_double_vsx.h", "aten/src/ATen/cpu/vec/vec256/vsx/vec256_complex_float_vsx.h", "aten/src/ATen/cpu/vec/vec256/vsx/vec256_double_vsx.h", "aten/src/ATen/cpu/vec/vec256/vsx/vec256_float_vsx.h", "aten/src/ATen/cpu/vec/vec256/vsx/vec256_quint8_vsx.h"], "labels": ["module: POWER", "cla signed", "ciflow/bazel"]}, "68555339d7": {"title": "test_utils.py: Add another retry to test_download_url_to_file (#66159)", "body": "Summary:\nFixes one of the flakiness concerns mentioned https://github.com/pytorch/pytorch/issues/65439#issuecomment-934686485\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66159\n\nReviewed By: ngimel\n\nDifferential Revision: D31406485\n\nPulled By: janeyx99\n\nfbshipit-source-id: cf7834cdab58360ecef1748075d52969de2e0778", "pr_number": "66159", "files_changed": ["test/test_utils.py"], "labels": ["cla signed", "ciflow/default"]}, "f062def486": {"title": "Revert D31260343: [pytorch][PR] Add hash and int128 utils for Lazy Tensor Core", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD31260343 (https://github.com/pytorch/pytorch/commit/e94fea08d0590e131f9762b000216b44f8c892da)\n\nOriginal commit changeset: 8bb1194188e3\n\nfbshipit-source-id: 3d0d5377d71ed928015bcb2105801be368e38cd8", "pr_number": null, "files_changed": ["BUILD.bazel", "c10/util/int128.cpp", "c10/util/int128.h", "caffe2/CMakeLists.txt", "test/cpp/lazy/CMakeLists.txt", "test/cpp/lazy/test_misc.cpp", "tools/build_variables.bzl", "torch/csrc/lazy/core/hash.cpp", "torch/csrc/lazy/core/hash.h"], "labels": []}, "eeabab03e7": {"title": "[DataParallel] Log API Usage for tracking (#66038)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66038\n\nWill help track workflows for DP deprecation. Tested via standalone DP\nscript.\n\nTest Plan: CI\n\nReviewed By: mrshenli\n\nDifferential Revision: D31356975\n\nfbshipit-source-id: c0a3ac3a1faed794e3362f3f3a19a6fb800587a7", "pr_number": "66038", "files_changed": ["torch/nn/parallel/data_parallel.py"], "labels": ["cla signed", "ciflow/default"]}, "43e26d0086": {"title": "[deploy] Improve error messaging for create_movable (#65955)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65955\n\nThis diff makes sure to give clear error message when user tries to create obj from obj that lives in different session\n\nTest Plan: buck test //caffe2/torch/csrc/deploy:test_deploy\n\nReviewed By: suo\n\nDifferential Revision: D31323045\n\nfbshipit-source-id: e7bd6f76afeb0285847bc11881185a164f80e3f0", "pr_number": "65955", "files_changed": ["torch/csrc/deploy/deploy.cpp", "torch/csrc/deploy/interpreter/interpreter_impl.h", "torch/csrc/deploy/test_deploy.cpp"], "labels": ["fb-exported", "cla signed", "ciflow/default"]}, "931352c68d": {"title": "Make handle_torch_function_no_python_arg_parser public (#66054)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66054\n\nI need this function in functorch to support the ability of custom\njitted kernels to invoke torch_function when applicable.\n\nTest Plan: functorch unit tests\n\nReviewed By: qihqi, ngimel\n\nDifferential Revision: D31416599\n\nPulled By: bertmaher\n\nfbshipit-source-id: 90b57badd6a6b9d505ebfc436869b962b55c66d7", "pr_number": "66054", "files_changed": ["torch/csrc/utils/python_arg_parser.h"], "labels": ["cla signed", "ciflow/default"]}, "747a5782e3": {"title": "[quant][fx] Don't assume bias is a keyword argument (#61647)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/61647\n\n`prepare_fx` currently assumes that bias is always a positional argument to\nconvolutions, and only a keyword argument to other functions. This happens to work\ntoday due to a quirk in how `__torch_function__` is handled for python\nfunctions but shouldn't be considered stable.\n\nInstead, we should support `bias` for both positional and keyword forms.\n\ncc jerryzh168 jianyuh raghuramank100 jamesr66a vkuzo\n\nTest Plan: Imported from OSS\n\nReviewed By: ngimel\n\nDifferential Revision: D31401360\n\nPulled By: albanD\n\nfbshipit-source-id: 1e2f53d80e2176b870f326dc498e251e2386136e", "pr_number": "61647", "files_changed": ["torch/ao/quantization/fx/prepare.py", "torch/ao/quantization/fx/quantization_patterns.py", "torch/ao/quantization/fx/utils.py"], "labels": ["oncall: quantization", "open source", "oncall: fx", "cla signed", "ciflow/default"]}, "b8e1999253": {"title": "[quant] Add op benchmark for GPU FakeQuantizePerChannel with float zero_points (#66183)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66183\n\nAdd a GPU benchmark for fakeQuant, similar to #65241\nghstack-source-id: 139810414\n\nTest Plan: https://pxl.cl/1QjJM\n\nReviewed By: b-koopman\n\nDifferential Revision: D31288158\n\nfbshipit-source-id: 65526248b5c7b70f0bc32a86b08f50b4cbc7a83d", "pr_number": "66183", "files_changed": ["benchmarks/operator_benchmark/pt/quantization_test.py"], "labels": ["cla signed", "ciflow/default"]}, "bfaaac6392": {"title": "Ignore register_rds errors (#66185)", "body": "Summary:\nNetwork communications are flaky by nature, test should be marked as\nskipped if network ops can not be completed for some reason\n\nFixes https://github.com/pytorch/pytorch/issues/66184\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66185\n\nReviewed By: seemethere\n\nDifferential Revision: D31423193\n\nPulled By: malfet\n\nfbshipit-source-id: 96c3a123c65913f44ea78b30a03e8e7eda164afe", "pr_number": "66185", "files_changed": ["test/test_import_stats.py"], "labels": ["cla signed", "ciflow/default"]}, "623ac7eabb": {"title": "slow_conv3d: Avoid dispatch in parallel region (#65737)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65737\n\nSee gh-56794\n\nAvoid dispatch inside of parallel_for by:\n- Replacing Tensor slicing with TensorAccessor\n- Copy bias into output only once, outside of the parallel region\n- Replaces `addmm_` and `baddbmm_` with direct calls to gemm.\n\nTest Plan: Imported from OSS\n\nReviewed By: albanD\n\nDifferential Revision: D31257874\n\nPulled By: ngimel\n\nfbshipit-source-id: 20b94daa13082fb1e39eaa8144bfa4c611b61bab", "pr_number": "65737", "files_changed": ["aten/src/ATen/native/CPUBlas.cpp", "aten/src/ATen/native/CPUBlas.h", "aten/src/ATen/native/ConvolutionMM3d.cpp", "aten/src/ATen/native/Unfold3d.cpp", "aten/src/ATen/native/Unfold3d.h", "aten/src/ATen/native/mkl/LinearAlgebra.cpp", "aten/src/ATen/native/mkl/LinearAlgebra.h"], "labels": ["open source", "cla signed", "ciflow/default"]}, "6d7fab5929": {"title": "[Static Runtime][easy] Clone scripts do not use aten::add (#66161)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66161\n\n`aten::add` is not guaranteed to be bit exact with the JIT interpreter. This was causing non-deterministic test failures on master.\n\nTest Plan: `buck test caffe2/benchmarks/static_runtime:static_runtime_cpptest`\n\nReviewed By: hlu1\n\nDifferential Revision: D31406764\n\nfbshipit-source-id: d968cb1bdb8f33934682ef3712a1341a3aacf18e", "pr_number": "66161", "files_changed": ["benchmarks/static_runtime/test_scripts.h"], "labels": ["fb-exported", "cla signed", "ciflow/default"]}, "90db214d4b": {"title": "support counter-based fused rowwise adagrad (#66177)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66177\n\nAs title, with additional change to enable counter for SparseAdagrad.\n\nTest Plan:\nbuck test //caffe2/caffe2/fb/net_transforms/tests:fuse_sparse_ops_test\n\nTesting with canary packages\n\nbaseline: f297789852\n\ncounter run: f297789912\n\nReviewed By: jspark1105\n\nDifferential Revision: D30903029\n\nfbshipit-source-id: 3ed89a7da409fd820fd0b44950407c20fa2018a5", "pr_number": "66177", "files_changed": ["caffe2/sgd/adagrad_op.h", "caffe2/sgd/rowwise_adagrad_fused.cc", "caffe2/sgd/rowwise_adagrad_fused.h"], "labels": ["fb-exported", "cla signed", "ciflow/default"]}, "7cc121dbcd": {"title": "slow_conv3d grad_input: Avoid dispatch in parallel region (#65757)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65757\n\nSee gh-56794\n\nAvoid dispatch inside of parallel_for by:\n- Replacing Tensor slicing with TensorAccessor\n- Replaces `bmm` and `mm` with direct calls to gemm.\n\nTest Plan: Imported from OSS\n\nReviewed By: albanD\n\nDifferential Revision: D31257878\n\nPulled By: ngimel\n\nfbshipit-source-id: e6aad2d5ae7fa432bd27af2b1a8b0dcef1fc6653", "pr_number": "65757", "files_changed": ["aten/src/ATen/native/ConvolutionMM3d.cpp"], "labels": ["open source", "cla signed", "ciflow/default"]}, "78209b93b3": {"title": "Don't build shared library for AOT Compiler (#66227)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66227\n\nBuilding a shared library for AOT Compiler is not necessary as it's included in libtorch. Also having this built as a shared library was affecting android builds and we don't need to build AOT Compiler for mobile builds\n\nBefore fix:\n```\n(pytorch)  ~/local/pytorch master\n\u2514\u2500 $ ANDROID_NDK=/opt/android_ndk/r20/ BUILD_PYTORCH_MOBILE=1 ANDROID_ABI=armeabi-v7a ./scripts/build_android.sh -DBUILD_BINARY=ON\nBuild with ANDROID_ABI[armeabi-v7a], ANDROID_NATIVE_API_LEVEL[21]\nBash: GNU bash, version 5.0.11(1)-release (x86_64-redhat-linux-gnu)\nPython: 3.9.7 (default, Sep 16 2021, 13:09:58)\n[GCC 7.5.0]\nCaffe2 path: /data/users/priyaramani/pytorch\nUsing Android NDK at /opt/android_ndk/r20/\n.\n.\nFAILED: lib/libaot_compiler.so\n: && /opt/android_ndk/r20/toolchains/llvm/prebuilt/linux-x86_64/bin/clang++ --target=armv7-none-linux-androideabi21 --gcc-toolchain=/opt/android_ndk/r20/toolchains/llvm/prebuilt/linux-x86_64 --sysroot=/opt/and\nroid_ndk/r20/toolchains/llvm/prebuilt/linux-x86_64/sysroot -fPIC -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -fno-addrsig -march=armv7-a -mt\nhumb -Wa,--noexecstack -Wformat -Werror=format-security -frtti -fexceptions  -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DUSE_KINETO -DLIBKINETO_NOCUPTI -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -\nDBUILD_LITE_INTERPRETER -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bound\ns -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -W\nno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -Wno-invalid-partial-specialization -Wno-typedef-redefinition -Wno-unknown-warning-option -Wno-unused-private-field -Wno-inconsistent-miss\ning-override -Wno-aligned-allocation-unavailable -Wno-c++14-extensions -Wno-constexpr-not-const -Wno-missing-braces -Qunused-arguments -fcolor-diagnostics -Wno-unused-but-set-variable -Wno-maybe-uninitialized\n-fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -g0 -Oz -DNDEBUG  -Wl,--exclude-libs,libgcc.a -Wl,--exclude-libs,libatomic.a -static-libstdc++ -Wl,--build-id -Wl,--warn-shared-text\nrel -Wl,--fatal-warnings -Wl,--exclude-libs,libunwind.a -Wl,--no-undefined -Qunused-arguments -Wl,-z,noexecstack  -rdynamic -shared -Wl,-soname,libaot_compiler.so -o lib/libaot_compiler.so caffe2/torch/CMakeFi\nles/aot_compiler.dir/csrc/jit/mobile/nnc/aot_compiler.cpp.o  -latomic -lm && :\ncaffe2/torch/CMakeFiles/aot_compiler.dir/csrc/jit/mobile/nnc/aot_compiler.cpp.o:aot_compiler.cpp:function at::from_blob(void*, c10::ArrayRef<long long>, c10::TensorOptions const&): error: undefined reference t\no 'at::TensorMaker::make_tensor()'\n.\n.\ncaffe2/torch/CMakeFiles/aot_compiler.dir/csrc/jit/mobile/nnc/aot_compiler.cpp.o:aot_compiler.cpp:function torch::jit::mobile::nnc::Function::Function(): error: undefined reference to 'c10::AnyType::get()'\nclang++: error: linker command failed with exit code 1 (use -v to see invocation)\n```\n\nAfter fix:\n```\n(pytorch)  ~/local/pytorch master\n\u2514\u2500 $ ANDROID_NDK=/opt/android_ndk/r20/ BUILD_PYTORCH_MOBILE=1 ANDROID_ABI=armeabi-v7a ./scripts/build_android.sh -DBUILD_BINARY=ON\nBuild with ANDROID_ABI[armeabi-v7a], ANDROID_NATIVE_API_LEVEL[21]\nBash: GNU bash, version 5.0.11(1)-release (x86_64-redhat-linux-gnu)\nPython: 3.9.7 (default, Sep 16 2021, 13:09:58)\n[GCC 7.5.0]\nCaffe2 path: /data/users/priyaramani/pytorch\nUsing Android NDK at /opt/android_ndk/r20/\n.\n.\n-- Build files have been written to: /data/users/priyaramani/pytorch/build_android\nWill install headers and libs to /data/users/priyaramani/pytorch/build_android/install for further Android project usage.\n[2/3] Install the project...\n-- Install configuration: \"Release\"\nInstallation completed, now you can copy the headers/libs from /data/users/priyaramani/pytorch/build_android/install to your Android project directory.\n```\n\nTest Plan: Imported from OSS\n\nReviewed By: ljk53, axitkhurana\n\nDifferential Revision: D31450970\n\nPulled By: priyaramani\n\nfbshipit-source-id: 87e48033f1db46fef112bae1239a09a2365620d2", "pr_number": "66227", "files_changed": ["binaries/CMakeLists.txt", "torch/CMakeLists.txt"], "labels": ["cla signed", "ciflow/default"]}, "1d586e78c6": {"title": "`*_solve` methods: implements forward AD (#65546)", "body": "Summary:\nThis PR adds forward AD for `*_solve` methods.\nAdditionally, `cholesky_solve` gets OpInfo + a bug fix when wrong leading dimensions could be passed to LAPACK,\nand `lu_solve` gets forward AD with 2x`lu_solve` instead of 1x`lu_solve` + 2x`triangular_solve`.\n\ncc ezyang albanD zou3519 gqchen pearu nikitaved soulitzer Lezcano Varal7 jianyuh mruberry walterddr IvanYashchuk xwang233\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65546\n\nReviewed By: dagitses\n\nDifferential Revision: D31431847\n\nPulled By: albanD\n\nfbshipit-source-id: 0e343e0d9da3c3d2051fca215fad289d77275251", "pr_number": "65546", "files_changed": ["aten/src/ATen/native/BatchLinearAlgebra.cpp", "tools/autograd/derivatives.yaml", "torch/csrc/autograd/FunctionsManual.cpp", "torch/csrc/autograd/FunctionsManual.h", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["module: autograd", "open source", "module: linear algebra", "cla signed", "ciflow/slow-gradcheck", "ciflow/default"]}, "6c54971cd9": {"title": "Open Registration for torch::deploy Builtins (#65953)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65953\n\nPreviously if people want to add a torch::deploy builtin, they need to change torch::deploy internal code (interpreter_impl.cpp) to register the python part as frozen modules and C++ part as builtin modules. This is not convenient and error prone. We want to add open registration support for torch::deploy builtins so that people only need to add one effective line of code in there *library code* to complete the registration.\n\nHere is an example to registry numpy as torch::deploy builtins:\n  REGISTER_TORCH_DEPLOY_BUILTIN(numpy, numpy_frozen_modules, <list of name, PyInit function pairs>)\n\nThis diff supports open registration of frozen modules. It's the first step to achieve the plan above.\nghstack-source-id: 139888306\n\nTest Plan: Run tests in test_deploy.cpp and test_builtin_registry.cpp\n\nReviewed By: suo\n\nDifferential Revision: D31321562\n\nfbshipit-source-id: 6445bd8869f1bb7126b4c96cf06c31145f0e9445", "pr_number": "65953", "files_changed": ["torch/csrc/deploy/interpreter/builtin_registry.cpp", "torch/csrc/deploy/interpreter/builtin_registry.h", "torch/csrc/deploy/interpreter/interpreter_impl.cpp", "torch/csrc/deploy/interpreter/test_builtin_registry.cpp"], "labels": ["cla signed", "ciflow/default"]}, "3f30526ff2": {"title": "Remove THCAllocator (#65942)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65942\n\nThis one is a bit weird. The class is called `THCIpcDeleter` but it\nactually has nothing IPC-specific. It just converts\n`std::shared_ptr` + `void*` into a `c10::DataPtr`. Instead, moving\nthe `DataPtr` conversion into the actual IPC code allows 2 memory\nallocations to be elided by merging 3 separate deletion contexts\ninto one.\n\nTest Plan: Imported from OSS\n\nReviewed By: dagitses\n\nDifferential Revision: D31386278\n\nPulled By: ngimel\n\nfbshipit-source-id: 5722beed9dcf680f0eb6bbff30405cff47b21962", "pr_number": "65942", "files_changed": ["aten/src/THC/CMakeLists.txt", "aten/src/THC/THC.h", "aten/src/THC/THCAllocator.cpp", "aten/src/THC/THCAllocator.h", "aten/src/THC/THCGeneral.cpp", "torch/csrc/CudaIPCTypes.h", "torch/csrc/generic/StorageSharing.cpp"], "labels": ["module: porting", "open source", "cla signed", "ciflow/default"]}, "1e4bcbdddb": {"title": "[Bootcamp][Pytorch Core] Add test for complex numbers for vanilla SGD (#66230)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66230\n\nAdding test to ensure Vanilla SGD behaves as if complex numbers are two real numbers in R^2 as per issue 65711 on github\nhttps://github.com/pytorch/pytorch/issues/65711\nghstack-source-id: 139918862\n\nTest Plan:\n```buck test mode/dev caffe2/test:optim -- 'test_sgd_complex'```\n\nhttps://pxl.cl/1QHvX\n\nReviewed By: albanD\n\nDifferential Revision: D31449289\n\nfbshipit-source-id: da8b00421085796a23b643e73f96b19b5b560a32", "pr_number": "66230", "files_changed": ["test/test_optim.py"], "labels": ["cla signed", "ciflow/default"]}, "2f1ab477f1": {"title": "Speed up DataTypeToTypeMeta (#66113)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66113\n\nFor a benchmark compiled in opt-mode in which the lookup items were shuffled and then the items were looked up round-robin fashion 10M times (for a total of 140M lookups) compiled in opt-mode we see:\n```\nFunction           Container            Time (ms) Multiplier\nTypeMetaToDataType if-chain                   233         1x\nTypeMetaToDataType std::vector                795      3.41x\nTypeMetaToDataType std::map                  1566      6.72x\nTypeMetaToDataType std::unordered_map        2136      9.17x\n\nDataTypeToTypeMeta switch                     102         1x\nDataTypeToTypeMeta std::vector                666      6.53x\nDataTypeToTypeMeta std::map                  1212      11.9x\nDataTypeToTypeMeta std::unordered_map        1539      15.1x\nDataTypeToTypeMeta folly::F14FastMap         1789      17.5x\n```\nFrom this, we draw two conclusions:\n1. Using a complex container like `std::map` is worse than using a simple vector lookup here (there aren't enough items for the Big-O to assert itself).\n2. Using any container at all is a mistake. (Unless we pull in more exotic reasoning like invalidating the code cache or preventing inlining.)\n\nTest Plan: Sandcastle\n\nReviewed By: dzhulgakov\n\nDifferential Revision: D31375117\n\nfbshipit-source-id: 0b310c6c2e94080d125c82fb7c2b43ab869adbcb", "pr_number": "66113", "files_changed": ["caffe2/core/types.cc", "caffe2/core/types.h"], "labels": ["fb-exported", "cla signed", "ciflow/default"]}, "a8c0b362ce": {"title": "[pytorch][PR] Add hash and int128 utils for Lazy Tensor Core\" (#66181)", "body": "Summary:\nThese utils are prerequisites for Lazy Node base class.\n- set up new torch/csrc/lazy, test/cpp/lazy dirs\n- add source files to build_variables.bzl in new lazy_core_sources var\n- create new test_lazy binary\n\nFixes https://github.com/pytorch/pytorch/issues/65636\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66181\n\nOriginal commit changeset: 3d0d5377d71e\n\nTest Plan:\nRun PyTorch XLA corresponding PR in XLA CI:\nhttps://github.com/pytorch/xla/pull/3148/files\n\nReviewed By: suo\n\nDifferential Revision: D31416438\n\nfbshipit-source-id: 58a6a49c5bc30134bc6bae2e42778f359b9a8f40", "pr_number": "66181", "files_changed": ["BUILD.bazel", "c10/util/int128.cpp", "c10/util/int128.h", "caffe2/CMakeLists.txt", "test/cpp/lazy/CMakeLists.txt", "test/cpp/lazy/test_misc.cpp", "tools/build_variables.bzl", "torch/csrc/lazy/core/hash.cpp", "torch/csrc/lazy/core/hash.h"], "labels": ["fb-exported", "cla signed", "ciflow/default", "ciflow/all"]}, "2e4e5b0264": {"title": "Add inplace_variant for resize_ OpInfo (#66135)", "body": "Summary:\nEnable testing of `torch.Tensor.resize_`.\nThe negative view test is skipped as the test doesn't work with resize_ see\nhttps://github.com/pytorch/pytorch/issues/65945.\n\ncc mruberry\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66135\n\nReviewed By: dagitses\n\nDifferential Revision: D31444263\n\nPulled By: mruberry\n\nfbshipit-source-id: 00c7fe05df28fba01508b31adb3ed4fdcf4d0326", "pr_number": "66135", "files_changed": ["torch/testing/_internal/common_methods_invocations.py"], "labels": ["module: tests", "triaged", "open source", "cla signed", "ciflow/default"]}, "e6a4f746c2": {"title": "slow_conv3d: Use at::sum for grad_bias accumulation (#65758)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65758\n\nThe same change has been made in conv2d, the proper algorithm is both\nfaster and gives more precision.\n\nTest Plan: Imported from OSS\n\nReviewed By: albanD\n\nDifferential Revision: D31257872\n\nPulled By: ngimel\n\nfbshipit-source-id: 6ff3a7a00a05b66f83d45cc820bd0c230cb8de6d", "pr_number": "65758", "files_changed": ["aten/src/ATen/native/ConvolutionMM3d.cpp"], "labels": ["open source", "cla signed", "ciflow/default"]}, "f445ed19b2": {"title": "OpInfo for 2d fft functions (#66128)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66128\n\ncc mruberry peterbell10\n\nTest Plan: Imported from OSS\n\nReviewed By: dagitses\n\nDifferential Revision: D31450217\n\nPulled By: mruberry\n\nfbshipit-source-id: 1952fc60c5d5f454966c43f5710b8b97a9794d0e", "pr_number": "66128", "files_changed": ["test/test_spectral_ops.py", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["open source", "module: fft", "cla signed", "ciflow/default"]}, "5e7d8ec846": {"title": "Support Registering a Variable Length List of Builtin Modules for torch::deploy Builtin Libraries (#66021)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66021\n\nA builtin library consists of a list of frozen modules and a list of builtin modules. For tensorrt, it's quite simple since we only have a single builtin module tensorrt.tensorrt. But it can be complex for libraries like numpy which contains multiple builtin modules (np.core._multiarray_umath, np.random.mtrand etc.) if we want to add it as a torch::deploy builtin. We enhance the macro that registers builtin libraries to accept a variable length of builtin modules. We can use this macro to register frozentorch, frozenpython, tensorrt for now and can also use it to register libraries like numpy later on.\n\nThe enhanced macro now looks as follows. Although we don't need to worry about back-compatibility for now,  but this enhanced version is fully compatible with the previous version. The previous version is just a special case when the library contains no builtin modules.\n\n ```\nREGISTER_TORCH_DEPLOY_BUILTIN(library_name_without_quote, frozen_modules_list,\n    builtin_module_name_1, builtin_module_init_function_1, ...,\n    builtin_module_name_N, builtin_module_init_function_N)\n```\nghstack-source-id: 140007970\n\nTest Plan:\n1. Play around with interactive_embedded_interpreter.cpp to import torch._C, tensorrt.tensorrt etc inside the embedded interpreter.\n2. Enhance test_builtin_registry.cpp\n3. Run test_deploy.cpp and test_deploy_gpu.cpp\n\nReviewed By: suo\n\nDifferential Revision: D31349390\n\nfbshipit-source-id: 70a1fcf660341180fc4d5195aed15ceb07c2bef7", "pr_number": "66021", "files_changed": ["torch/csrc/deploy/interactive_embedded_interpreter.cpp", "torch/csrc/deploy/interpreter/builtin_registry.cpp", "torch/csrc/deploy/interpreter/builtin_registry.h", "torch/csrc/deploy/interpreter/interpreter_impl.cpp", "torch/csrc/deploy/interpreter/test_builtin_registry.cpp"], "labels": ["cla signed", "ciflow/default"]}, "0e2d1b221a": {"title": "[Bootcamp][Pytorch Core] Add testing for complex non-vanilla SGD", "body": "Summary: Adding test to ensure non-Vanilla SGD behaves as if complex numbers are two real numbers in R^2 as per issue 65711 on github\n\nTest Plan:\n```buck test mode/dev caffe2/test:optim -- 'test_sgd_complex'```\n\nhttps://pxl.cl/1QLxw\n\nReviewed By: albanD\n\nDifferential Revision: D31477212\n\nfbshipit-source-id: 500678e561a05ac96759223b4c87a37cab26c6a6", "pr_number": null, "files_changed": ["test/test_optim.py"], "labels": []}, "86de09e49a": {"title": "Upgrade to ubuntu:trusty-20190515 (#63468)", "body": "Summary:\nSecurity Upgrade to ubuntu:trusty-20190515\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/63468\n\nReviewed By: ngimel\n\nDifferential Revision: D31393552\n\nPulled By: malfet\n\nfbshipit-source-id: 4e2399e3cddc1d549c08c82c08015e00569c19bc", "pr_number": "63468", "files_changed": ["caffe2/contrib/docker-ubuntu-14.04/Dockerfile"], "labels": ["triaged", "open source", "cla signed"]}, "20f2e55d4f": {"title": "Rename cuda/Resize.cu to cuda/Resize.cpp (#65943)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65943\n\nThese files don't require nvcc to compile.\n\nTest Plan: Imported from OSS\n\nReviewed By: albanD\n\nDifferential Revision: D31386277\n\nPulled By: ngimel\n\nfbshipit-source-id: 1066ee87fa795e2c7969447fbce1fe2633fb9680", "pr_number": "65943", "files_changed": ["aten/src/ATen/native/Resize.h", "aten/src/ATen/native/cuda/Normalization.cu", "aten/src/ATen/native/cuda/Resize.cpp", "aten/src/ATen/native/cuda/Resize.cu", "aten/src/ATen/native/cuda/Resize.cuh", "aten/src/ATen/native/cuda/Resize.h", "aten/src/ATen/native/cuda/TensorFactories.cu", "aten/src/ATen/native/cuda/TensorShapeCUDA.cpp", "aten/src/THC/THCTensor.cpp"], "labels": ["module: porting", "open source", "cla signed", "ciflow/default"]}, "e1817d895f": {"title": "[BE] Cleanup python_function.cpp (#66296)", "body": "Summary:\n- Delete unused `var_input_idx`\n- Fix `uninitialized variable` clang-tidy warning by setting `PyObject* input` to PyNone\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66296\n\nReviewed By: janeyx99\n\nDifferential Revision: D31491016\n\nPulled By: malfet\n\nfbshipit-source-id: 08267144be0cd049d122580cdf81cf586c3e30a6", "pr_number": "66296", "files_changed": ["torch/csrc/autograd/python_function.cpp"], "labels": ["ciflow/default"]}, "d5033410b1": {"title": "Parallel: Deduplicate parallel functions in different backends (#65326)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65326\n\nparallel_for and parallel_reduce currently share some common code in\nall backends, specifically for detecting if it should run in parallel\nor not. This moves all the backend-specific code into a single\n`internal::invoke_parallel` function and makes the `parallel_`\nfunctions common to all backends.\n\nTest Plan: Imported from OSS\n\nReviewed By: ezyang\n\nDifferential Revision: D31124495\n\nfbshipit-source-id: 65c3d2af42a8860cc4d6349566085c9fa8d8c6f0", "pr_number": "65326", "files_changed": ["aten/src/ATen/Parallel-inl.h", "aten/src/ATen/Parallel.h", "aten/src/ATen/ParallelNative.cpp", "aten/src/ATen/ParallelNative.h", "aten/src/ATen/ParallelNativeTBB.h", "aten/src/ATen/ParallelOpenMP.h"], "labels": ["open source", "cla signed", "ciflow/default", "ciflow/cpu"]}, "bd9eee4e65": {"title": "TBB: Use static partitioner to match OpenMP scheduling (#65327)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65327\n\nShould fix https://github.com/pytorch/pytorch/issues/64571\n\nTest Plan: Imported from OSS\n\nReviewed By: dagitses\n\nDifferential Revision: D31474116\n\nPulled By: malfet\n\nfbshipit-source-id: 8c4264d4778c6caf58261e3f70d72decd134128d", "pr_number": "65327", "files_changed": ["aten/src/ATen/ParallelNativeTBB.h", "test/test_tensor_creation_ops.py", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["open source", "cla signed", "ciflow/default", "ciflow/cpu"]}, "4af913a7cf": {"title": "fixed minor issues for index_add in docs (#65806)", "body": "Summary:\nHi, I'm looking forward to contributing to PyTorch, so starting with a minor fix in the documentation for `index_add`.\n\nCurrently, in the documentation for `index_add_` (please see https://pytorch.org/docs/master/generated/torch.Tensor.index_add_.html#torch.Tensor.index_add_):\n\n1. `tensor` attribute was pointing to `torch.tensor` class, which IMO - is (thought may not be a big deal) unintentional.\n2. `dim` attribute is pointing to `torch.Tensor.dim`, which again IMO - is unintentional.\n\nThis PR suggests a correction for the first point above, to rename `tensor` attribute to `input` so that it doesn't point to `torch.tensor` class. (I've verified that others ops like `scatter` use `input`, so this should not break the consistency in the documentation). I couldn't find an appropriate fix for the second point above, since renaming `dim` to something else will break the consistency (as almost all others op in PyTorch use `dim` as the attribute name).\n\nI may be wrong here, so please let me know if there is any feedback or an alternate fix for this.\n\n_Note:_ I plan to fix this behavior for `index_copy_` (https://pytorch.org/docs/master/generated/torch.Tensor.index_copy_.html#torch.Tensor.index_copy_) once and if this PR is approved.\n\nTo the reviewers, please help me tag the correct person who could help review this PR.\n\ncc: krshrimali mruberry zou3519\n\ncc brianjo mruberry\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65806\n\nReviewed By: dagitses, mruberry\n\nDifferential Revision: D31431182\n\nPulled By: zou3519\n\nfbshipit-source-id: 66ced9677ac3bc71d672d13366f9f567ecea0a2d", "pr_number": "65806", "files_changed": ["torch/_tensor_docs.py"], "labels": ["module: docs", "open source", "cla signed", "ciflow/default"]}, "8d6d448238": {"title": "Add HPU for Autograd Fallback (#65605)", "body": "Summary:\nFixes #{issue number}\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65605\n\nReviewed By: albanD\n\nDifferential Revision: D31373899\n\nPulled By: ezyang\n\nfbshipit-source-id: 894f62dc44b0532f152dc97b839eecfbaed25e8c", "pr_number": "65605", "files_changed": ["aten/src/ATen/core/VariableFallbackKernel.cpp"], "labels": ["triaged", "open source", "cla signed"]}, "dc37547c44": {"title": "Opinfos for avg_pooling (#64214)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/64214\n\nAdded OpInfos for:\n- F.adapative_avg_pool{1, 3}d\n- F.avg_pool{1, 3}d\n\nThe 2d variants already had OpInfos.\n\nTest Plan: - run tests\n\nReviewed By: albanD, mruberry\n\nDifferential Revision: D30667797\n\nPulled By: zou3519\n\nfbshipit-source-id: 53f5cd02070de5b7db4abb017d727376b59288df", "pr_number": "64214", "files_changed": ["torch/testing/_internal/common_methods_invocations.py"], "labels": ["cla signed", "ciflow/default"]}, "ece0221854": {"title": "Rename int to long, add more C++ types. (#66108)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66108\n\nBC-breaking change: intT is now longT (which aligns it more accurately with how\nthe types are referred to in C++).  The benefit for this is we can idiomatically\nexpress all C++ dtypes (with intT now mapping to int32_t).  These types are needed\nfor ufunc codegen in a latter patch.\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nReviewed By: albanD\n\nDifferential Revision: D31385761\n\nPulled By: ezyang\n\nfbshipit-source-id: ec6f3a0953794313470dbe14911f23ac116be425", "pr_number": "66108", "files_changed": ["tools/autograd/gen_autograd_functions.py", "tools/autograd/gen_inplace_or_view_type.py", "tools/autograd/load_derivatives.py", "tools/codegen/api/types.py"], "labels": ["cla signed", "ciflow/default"]}, "0cad2c0615": {"title": "Move intraop_launch_future from Parallel.h (#64166)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/64166\n\nTest Plan: Imported from OSS\n\nReviewed By: gchanan\n\nDifferential Revision: D30728585\n\nPulled By: dagitses\n\nfbshipit-source-id: 75a41418ae9218bec9bac27597051295222b6eee", "pr_number": "64166", "files_changed": ["aten/src/ATen/Parallel-inl.h", "aten/src/ATen/Parallel.h", "aten/src/ATen/ParallelFuture.h", "aten/src/ATen/ParallelNative.cpp", "aten/src/ATen/ParallelNative.h", "aten/src/ATen/ParallelNativeTBB.cpp", "aten/src/ATen/ParallelNativeTBB.h", "aten/src/ATen/ParallelOpenMP.cpp", "aten/src/ATen/test/test_parallel.cpp", "binaries/at_launch_benchmark.cc"], "labels": ["open source", "cla signed", "ciflow/default"]}, "201174cb91": {"title": "Revert D31389480: [pytorch][PR] Allow external CUDA streams to be set as current", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD31389480 (https://github.com/pytorch/pytorch/commit/61f0bb70c1cd63a2dd396c5774d055551914e492)\n\nOriginal commit changeset: 2b2f40e5452c\n\nfbshipit-source-id: c6631e51abcf3819732f981f646cb77b91569c7d", "pr_number": null, "files_changed": ["aten/src/ATen/test/cuda_stream_test.cpp", "c10/cuda/CUDAStream.cpp"], "labels": []}, "0020a151c6": {"title": "slow_conv3d grad_weight: call gemm directly (#65759)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/65759\n\nTest Plan: Imported from OSS\n\nReviewed By: dagitses\n\nDifferential Revision: D31257873\n\nPulled By: ngimel\n\nfbshipit-source-id: 1612c0be10b2aa269c807c7b9f5470172ed68dc1", "pr_number": "65759", "files_changed": ["aten/src/ATen/native/ConvolutionMM3d.cpp"], "labels": ["open source", "cla signed", "ciflow/default"]}, "bc1dec9b81": {"title": "Migrate THCStorage_resizeBytes to ATen (#65944)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/65944\n\nTest Plan: Imported from OSS\n\nReviewed By: albanD\n\nDifferential Revision: D31386276\n\nPulled By: ngimel\n\nfbshipit-source-id: a2b28bc09d11a856fdd3796d3df6f96613f13437", "pr_number": "65944", "files_changed": ["aten/src/ATen/native/cuda/Resize.cpp", "aten/src/ATen/native/cuda/Resize.h", "aten/src/THC/THCStorage.cpp"], "labels": ["module: porting", "open source", "cla signed", "ciflow/default"]}, "0be36d798b": {"title": "Remove Tensor.h include from TensorIterator.h (#64167)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/64167\n\nTest Plan: Imported from OSS\n\nReviewed By: saketh-are\n\nDifferential Revision: D30728579\n\nPulled By: dagitses\n\nfbshipit-source-id: 3888da00c9c8030013c8f4b39d300fe671defb05", "pr_number": "64167", "files_changed": ["aten/src/ATen/MemoryOverlap.cpp", "aten/src/ATen/MemoryOverlap.h", "aten/src/ATen/NamedTensorUtils.h", "aten/src/ATen/TensorIterator.cpp", "aten/src/ATen/TensorIterator.h", "aten/src/ATen/native/DistributionTemplates.h", "aten/src/ATen/native/cpu/RangeFactoriesKernel.cpp", "aten/src/ATen/native/cuda/IndexKernel.cu", "aten/src/ATen/native/cuda/ReduceLogicKernel.cu", "aten/src/ATen/templates/NativeMetaFunctions.h"], "labels": ["open source", "cla signed", "ciflow/default"]}, "8a02d3e5d0": {"title": "Wextra fix for Tensorshape.cpp (#66320)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66320\n\nFixes\n```\nstderr: caffe2/aten/src/ATen/native/TensorShape.cpp:619:36: error: comparison of integers of different signs: 'size_t' (aka 'unsigned long') and 'long' [-Werror,-Wsign-compare]\n    for (size_t offset = 0; offset < numel; offset++) {\n                            ~~~~~~ ^ ~~~~~\nstderr: caffe2/aten/src/ATen/native/TensorShape.cpp:619:36: error: comparison of integers of different signs: 'size_t' (aka 'unsigned long') and 'long' [-Werror,-Wsign-compare]\n    for (size_t offset = 0; offset < numel; offset++) {\n                            ~~~~~~ ^ ~~~~~\n```\n\nTest Plan: Sandcastle\n\nReviewed By: ngimel\n\nDifferential Revision: D31505374\n\nfbshipit-source-id: 0fc393dacd72a8b29c0d82561f730cc047b38f0c", "pr_number": "66320", "files_changed": ["aten/src/ATen/native/TensorShape.cpp"], "labels": ["fb-exported", "ciflow/default"]}, "2daae532bd": {"title": "[ao_migration] torch/nn/qat: torch.quantization -> torch.ao.quantization (#65902)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65902\n\nThis changes the imports in the `caffe2/torch/nn/qat` to include the new import locations.\n\n```\ncodemod -d torch/nn/qat --extensions py 'torch.quantization' 'torch.ao.quantization'\n```\n\nTest Plan: `python test/run_test.py`\n\nReviewed By: jerryzh168\n\nDifferential Revision: D31301196\n\nfbshipit-source-id: ff237790d74cd3b3b5be642a997810f4f439a1d8", "pr_number": "65902", "files_changed": ["torch/nn/qat/modules/conv.py", "torch/nn/qat/modules/linear.py"], "labels": ["cla signed", "ciflow/default"]}, "a28b038af4": {"title": "[ao_migration] torch/nn/intrinsic: torch.quantization -> torch.ao.quantization (#65903)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65903\n\nThis changes the imports in the `caffe2/torch/nn/intrinsic` to include the new import locations.\n\n```\ncodemod -d torch/nn/intrinsic --extensions py 'torch.quantization' 'torch.ao.quantization'\n```\n\nTest Plan: `python test/run_test.py`\n\nReviewed By: albanD\n\nDifferential Revision: D31301195\n\nfbshipit-source-id: a5a9d84cb1ac33df6c90ee03cda3e2f1c5d5ff51", "pr_number": "65903", "files_changed": ["torch/nn/intrinsic/qat/modules/conv_fused.py"], "labels": ["cla signed", "ciflow/default"]}, "4a302a3074": {"title": "Wextra fix for CUDAApplyUtils.cuh (#66323)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66323\n\nFixes\n```\n/data/sandcastle/boxes/eden-trunk-hg-fbcode-fbsource/fbcode/caffe2/aten/src/ATen/cuda/CUDAApplyUtils.cuh:310:48: error: comparison of integers of different signs: 'unsigned long' and 'int' [-Werror,-Wsign-compare]\n  const IndexType bOffset = sizeof...(Offsets) < n ?\n                            ~~~~~~~~~~~~~~~~~~ ^ ~\n/data/sandcastle/boxes/eden-trunk-hg-fbcode-fbsource/fbcode/caffe2/aten/src/ATen/cuda/CUDAApplyUtils.cuh:306:48: error: comparison of integers of different signs: 'unsigned long' and 'int' [-Werror,-Wsign-compare]\n  const IndexType aOffset = sizeof...(Offsets) < n ?\n                            ~~~~~~~~~~~~~~~~~~ ^ ~\n```\n\nTest Plan: Sandcastle\n\nReviewed By: ngimel\n\nDifferential Revision: D31505428\n\nfbshipit-source-id: 326fa8f41f2b200981eddc5cab035b18536cd24e", "pr_number": "66323", "files_changed": ["aten/src/ATen/cuda/CUDAApplyUtils.cuh"], "labels": ["fb-exported", "ciflow/default"]}, "566922bbcd": {"title": "clean up mypy nit in torch/jit/_recursive.py (#66253)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66253\n\nThis was initially broken in #65829 and unbroken in #66003, this PR cleans\nit up by removing the mypy ignore line.\n\nTest Plan:\n```\nmypy torch/jit/_recursive.py --no-incremental\n```\n\nImported from OSS\n\nReviewed By: supriyar\n\nDifferential Revision: D31475100\n\nfbshipit-source-id: 46ab2ede72c08b926f4f9a6b03b1a1375b884c8a", "pr_number": "66253", "files_changed": ["torch/jit/_recursive.py"], "labels": ["ciflow/default"]}, "904fbadaff": {"title": "Fix merge conflict in bc tests (#66356)", "body": "Summary:\nBC test currently borken on trunk\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66356\n\nReviewed By: malfet\n\nDifferential Revision: D31523340\n\nPulled By: janeyx99\n\nfbshipit-source-id: a8d1ff697f017c710f70a76b5bb6a2f89d7637c7", "pr_number": "66356", "files_changed": ["test/backward_compatibility/check_backward_compatibility.py"], "labels": ["ciflow/default"]}, "85b562dd2b": {"title": "Fix type checking errors in fx/utils.py (#66311)", "body": "Summary:\n- [x] Fix the Pyre type checking errors in `torch/quantization/fx/utils.py`\n```\ntorch/quantization/fx/utils.py:490:4 Incompatible variable type [9]: target_module_type is declared to have type `Type[nn.modules.module.Module]` but is used as type `None`.\n```\nFixes the issue: [MLH-Fellowship/pyre-check/issues/75](https://github.com/MLH-Fellowship/pyre-check/issues/75)\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66311\n\nReviewed By: pradeep90\n\nDifferential Revision: D31506399\n\nPulled By: 0xedward\n\nfbshipit-source-id: 3d866fba6005452378d4a2613b8689fa2d7a8b67", "pr_number": "66311", "files_changed": ["torch/ao/quantization/fx/utils.py"], "labels": ["open source", "cla signed", "ciflow/default"]}, "fb5a80ffd8": {"title": "[jit] Don't force refcount bumps from getTypePtr (#66282)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66282\n\nNow that a bunch of the `FooType::get()` functions return a const reference, we can forward that behavior through `getTypePtr()` using return type deduction.\n\nTest Plan: Inspect assembly for List_test.cpp before/after the rest of the change; reference counting is no longer in the happy path.\n\nReviewed By: hlu1, JasonHanwen\n\nDifferential Revision: D31486117\n\nfbshipit-source-id: 863b677bb6685452a5b325d327bdc2a0a09627bf", "pr_number": "66282", "files_changed": ["aten/src/ATen/core/Dict_inl.h", "aten/src/ATen/core/List_inl.h", "aten/src/ATen/core/List_test.cpp", "aten/src/ATen/core/jit_type.h", "aten/src/ATen/core/op_registration/infer_schema.h"], "labels": ["cla signed", "ciflow/default"]}, "1763c25414": {"title": "[PyTorch][jit] Fix excess refcounting in TupleType::compare (#66286)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66286\n\nNo need to take refcount bumps on each comparator call.\n\nTest Plan: CI, review\n\nReviewed By: hlu1, JasonHanwen\n\nDifferential Revision: D31487058\n\nfbshipit-source-id: 98d2447ac27a12695cb0ebe1e279a6b50744ff4f", "pr_number": "66286", "files_changed": ["aten/src/ATen/core/jit_type.h", "aten/src/ATen/core/type.cpp"], "labels": ["cla signed", "ciflow/default"]}, "4cb4d11e0b": {"title": "Disable \"-Wignored-qualifiers\" for vec256_bfloat16.h (#66279)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66279\n\nThis error appears when compiling with \"-Wextra\" and cannot be resolved by fixing the code since the return type of the instrinic being passed to `map` is fixed.\n\nFixes:\n```\ncaffe2/aten/src/ATen/cpu/vec/vec256/vec256_bfloat16.h:204:28: error: 'const' type qualifier on return type has no effect [-Werror,-Wignored-qualifiers]\n  Vectorized<BFloat16> map(const __m256 (*const vop)(__m256)) const {\n                           ^~~~~~\ncaffe2/aten/src/ATen/cpu/vec/vec256/vec256_bfloat16.h:204:28: error: 'const' type qualifier on return type has no effect [-Werror,-Wignored-qualifiers]\n  Vectorized<BFloat16> map(const __m256 (*const vop)(__m256)) const {\n                           ^~~~~~\n```\n\nTest Plan: Sandcastle\n\nReviewed By: ngimel\n\nDifferential Revision: D31480888\n\nfbshipit-source-id: 919c0d48c8ce13ce1106a9df124a077945e36707", "pr_number": "66279", "files_changed": ["aten/src/ATen/cpu/vec/vec256/vec256_bfloat16.h"], "labels": ["fb-exported", "ciflow/default"]}, "109aa135e6": {"title": "Remove apparently unnecessary std::remove_cv_t (#66254)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66254\n\n`std::decay_t` already implies dropping the const\n\nTest Plan: Sandcastle\n\nReviewed By: malfet\n\nDifferential Revision: D31465856\n\nfbshipit-source-id: 851cdb9194354fe9a89b3a37a4463a43dbbcd77a", "pr_number": "66254", "files_changed": ["c10/util/TypeIndex.h"], "labels": ["fb-exported", "ciflow/default"]}, "9539e6216b": {"title": "Quantization docs: add pages for Numeric Suite (Eager and FX) (#66222)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66222\n\nDescription:\n1. creates doc pages for Eager and FX numeric suites\n2. adds a link from main quantization doc to (1)\n3. formats docblocks in Eager NS to render well\n4. adds example code and docblocks to FX numeric suite\n\nTest Plan:\n```\ncd docs\nmake html\npython -m http.server\n// renders well\n```\n\nReviewed By: jerryzh168\n\nDifferential Revision: D31447610\n\nPulled By: vkuzo\n\nfbshipit-source-id: 441170c4a6c3ddea1e7c7c5cc2f1e1cd5aa65f2f", "pr_number": "66222", "files_changed": ["docs/source/quantization.rst", "docs/source/torch.ao.ns._numeric_suite.rst", "docs/source/torch.ao.ns._numeric_suite_fx.rst", "torch/ao/ns/_numeric_suite.py", "torch/ao/ns/_numeric_suite_fx.py", "torch/ao/ns/fx/utils.py"], "labels": ["cla signed", "ciflow/default", "fx"]}, "84326ef059": {"title": "Remove native_functions.yaml dependency from binary ops (#64169)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/64169\n\nTest Plan: Imported from OSS\n\nReviewed By: gchanan\n\nDifferential Revision: D30728586\n\nPulled By: dagitses\n\nfbshipit-source-id: 17d645b6712815d1967b9ff83eecc4d16833ee6b", "pr_number": "64169", "files_changed": ["aten/src/ATen/Context.h", "aten/src/ATen/native/BinaryOps.h", "aten/src/ATen/native/cpu/BinaryOpsKernel.cpp", "aten/src/ATen/native/cuda/BinaryAddSubKernel.cu", "aten/src/ATen/native/cuda/BinaryBitwiseOpsKernels.cu", "aten/src/ATen/native/cuda/BinaryGeometricKernels.cu", "aten/src/ATen/native/cuda/BinaryLogicalOpsKernels.cu", "aten/src/ATen/native/cuda/BinaryMiscOpsKernels.cu", "aten/src/ATen/native/cuda/BinaryMulDivKernel.cu", "aten/src/ATen/native/cuda/BinaryRemainderKernel.cu", "aten/src/ATen/native/cuda/BinaryShiftOpsKernels.cu", "aten/src/ATen/native/cuda/CompareEQKernel.cu", "aten/src/ATen/native/cuda/CompareGEKernel.cu", "aten/src/ATen/native/cuda/CompareGTKernel.cu", "aten/src/ATen/native/cuda/CompareLEKernel.cu", "aten/src/ATen/native/cuda/CompareLTKernel.cu", "aten/src/ATen/native/cuda/CompareNEKernel.cu", "aten/src/ATen/native/cuda/CopysignKernel.cu", "aten/src/ATen/native/cuda/GcdLcmKernel.cu", "aten/src/ATen/native/cuda/IGammaKernel.cu", "aten/src/ATen/native/cuda/LogAddExpKernel.cu", "aten/src/ATen/native/cuda/MaxMinElementwiseKernel.cu", "aten/src/ATen/native/cuda/StepKernel.cu", "aten/src/ATen/native/cuda/ZetaKernel.cu"], "labels": ["open source", "cla signed", "ciflow/default"]}, "27f193af64": {"title": "Automated submodule update: kineto (#59674)", "body": "Summary:\nThis is an automated pull request to update the first-party submodule for [pytorch/kineto](https://github.com/pytorch/kineto).\n\nNew submodule commit: https://github.com/pytorch/kineto/commit/6f9c0eeff519ac7365fec427829404c9741ef391\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/59674\n\nTest Plan: Ensure that CI jobs succeed on GitHub before landing.\n\nReviewed By: larryliu0820\n\nDifferential Revision: D28977762\n\nfbshipit-source-id: d441d4d46a7044cc05eb8b21e59471deee312e02", "pr_number": "59674", "files_changed": ["third_party/kineto"], "labels": ["triaged", "open source", "cla signed", "ciflow/default"]}, "ad0accdecd": {"title": "Revert D31447610: Quantization docs: add pages for Numeric Suite (Eager and FX)", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD31447610 (https://github.com/pytorch/pytorch/commit/9539e6216bc122b604e8fb55c075d1d525c2522b)\n\nOriginal commit changeset: 441170c4a6c3\n\nfbshipit-source-id: b49bff54405cdb8465397077e38506a36b277921", "pr_number": null, "files_changed": ["docs/source/quantization.rst", "docs/source/torch.ao.ns._numeric_suite.rst", "docs/source/torch.ao.ns._numeric_suite_fx.rst", "torch/ao/ns/_numeric_suite.py", "torch/ao/ns/_numeric_suite_fx.py", "torch/ao/ns/fx/utils.py"], "labels": []}, "df1858bea5": {"title": "Revert D31447611: Quantization documentation: move backend section down", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD31447611 (https://github.com/pytorch/pytorch/commit/309a8cf46c2b551d04d03df1d6181e0c350356dc)\n\nOriginal commit changeset: 537b146559bc\n\nfbshipit-source-id: c400aef9a2ea5d18f8076879fe6354be7a6732f1", "pr_number": null, "files_changed": ["docs/source/quantization.rst"], "labels": []}, "037ac2330e": {"title": "Revert D31447616: Quantization docs: consilidate all API references on a single page", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD31447616 (https://github.com/pytorch/pytorch/commit/fe86f0e068e07c0e61d79533afa10b1fcd634db1)\n\nOriginal commit changeset: 2f9c4dac2b2f\n\nfbshipit-source-id: 673368e87399f0a25441688bb9356de5a2f3e66e", "pr_number": null, "files_changed": ["docs/source/quantization-support.rst", "docs/source/quantization.rst"], "labels": []}, "10633460ce": {"title": "Revert D31447614: Create a documentation page for `torch.ao.quantization.QConfig`", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD31447614 (https://github.com/pytorch/pytorch/commit/7332ed13edc509d1fc79e34cb5a83310978b68c5)\n\nOriginal commit changeset: 5d9dd2a4e864\n\nfbshipit-source-id: 6ac15a956222ca61f7fbb75ed36bcc58b23f0f36", "pr_number": null, "files_changed": ["docs/source/quantization.rst", "docs/source/torch.quantization.qconfig.rst", "docs/source/torch.quantization.rst", "torch/ao/quantization/qconfig.py"], "labels": []}, "b85fd4c54f": {"title": "Revert D31447613: Create separate documentation pages for quantization observers and fake_quants", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD31447613 (https://github.com/pytorch/pytorch/commit/f0fa3d1110fdd085b040ef82c8a547b205b98e38)\n\nOriginal commit changeset: 63b4cf518bad\n\nfbshipit-source-id: 67de592d1e12a5149cdb22b0725caad063f94476", "pr_number": null, "files_changed": ["docs/source/quantization.rst", "docs/source/torch.quantization.fake_quantize.rst", "docs/source/torch.quantization.observer.rst", "docs/source/torch.quantization.rst", "torch/ao/quantization/fake_quantize.py", "torch/ao/quantization/observer.py"], "labels": []}, "9971113340": {"title": "Revert D31447612: Create a documentation page for FX graph mode quantization APIs", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD31447612 (https://github.com/pytorch/pytorch/commit/a89ac3138e37d847f1c1e4c5beeb6303768ebedf)\n\nOriginal commit changeset: 07d0a6137f15\n\nfbshipit-source-id: f2cba7d835011500580b4ab9cff72171280ee18b", "pr_number": null, "files_changed": ["docs/source/quantization.rst", "docs/source/torch.quantization.quantize_fx.rst", "torch/ao/quantization/quantize_fx.py"], "labels": []}, "bc06eefebe": {"title": "[reland] Allow external CUDA streams to be set as current (#66324)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66324\n\nFixes https://github.com/pytorch/pytorch/issues/65822.\n\nReland of https://github.com/pytorch/pytorch/pull/65914.\nghstack-source-id: 140105651\n\nTest Plan: Added tests\n\nReviewed By: ngimel\n\nDifferential Revision: D31506134\n\nfbshipit-source-id: ff56203a120befdb282e974309478ac11aa56652", "pr_number": "66324", "files_changed": ["aten/src/ATen/test/cuda_stream_test.cpp", "c10/cuda/CUDAStream.cpp"], "labels": ["cla signed", "ciflow/default", "ciflow/win"]}, "0348148725": {"title": "Update link to qnnpack in quantization doc. (#66226)", "body": "Summary:\nThe old repo has been archived.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66226\n\nReviewed By: vkuzo\n\nDifferential Revision: D31534712\n\nPulled By: ezyang\n\nfbshipit-source-id: 4d7f070c8547aeb25464c72b25ed21f209821bc2", "pr_number": "66226", "files_changed": ["docs/source/quantization.rst"], "labels": ["open source", "cla signed", "ciflow/default"]}, "1b40daac74": {"title": "pinv: forward/backward AD which is Frechet-defined in a rank-preserving neighborhood. (#66092)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/65911. Also enables complex support/tests for `linalg_pinv` in OpInfo.\n\ncc ezyang albanD zou3519 gqchen pearu nikitaved soulitzer Lezcano Varal7 jianyuh mruberry walterddr IvanYashchuk xwang233\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66092\n\nReviewed By: ejguan\n\nDifferential Revision: D31503072\n\nPulled By: albanD\n\nfbshipit-source-id: 52018e826826ae62beaad76becb5edf880be253f", "pr_number": "66092", "files_changed": ["aten/src/ATen/native/native_functions.yaml", "tools/autograd/derivatives.yaml", "tools/autograd/gen_variable_type.py", "torch/csrc/autograd/FunctionsManual.cpp", "torch/csrc/autograd/FunctionsManual.h", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["module: autograd", "open source", "module: linear algebra", "complex_autograd", "cla signed", "ciflow/slow-gradcheck", "ciflow/default"]}, "4775419850": {"title": "[BE] Address feedback from #66296 (#66315)", "body": "Summary:\nAlso use range loop instead of regular one\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66315\n\nReviewed By: albanD\n\nDifferential Revision: D31503730\n\nPulled By: malfet\n\nfbshipit-source-id: f5568f7f28e15a9becd27986dd061a6fcae34651", "pr_number": "66315", "files_changed": ["torch/csrc/autograd/python_function.cpp"], "labels": ["ciflow/default"]}, "d3b29afbb6": {"title": "Remove old code that is unused in test/ (#66331)", "body": "Summary:\n.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66331\n\nReviewed By: gchanan\n\nDifferential Revision: D31533549\n\nPulled By: albanD\n\nfbshipit-source-id: 5addd11edc4199a88f10f0ff236be59ec2289903", "pr_number": "66331", "files_changed": ["test/optim/compare.sh", "test/optim/test.lua", "test/optim/test.py", "test/optim/tests.json"], "labels": ["ciflow/default"]}, "c66847afbe": {"title": "Add workaround for nvcc header dependecies bug (#62550)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/62550\n\nI noticed that running the build twice in a row resulted in ~80 CUDA files being\nrebuilt. Running `ninja -d explain` shows\n```\nninja explain: TH/generic/THStorage.h is dirty\nninja explain: TH/generic/THStorageCopy.h is dirty\nninja explain: THC/generic/THCStorage.h is dirty\nninja explain: THC/generic/THCStorageCopy.h is dirty\nninja explain: TH/generic/THTensor.h is dirty\nninja explain: THC/generic/THCTensor.h is dirty\nninja explain: THC/generic/THCTensorCopy.h is dirty\nninja explain: THC/generic/THCTensorMath.h is dirty\nninja explain: THC/generic/THCTensorMathMagma.h is dirty\nninja explain: THC/generic/THCTensorMathPairwise.h is dirty\nninja explain: THC/generic/THCTensorScatterGather.h is dirty\n```\n\nconsidering `ninja` is working relative to the `build` folder, these files don't\nactually exist. I traced this back to the output of `nvcc -MD` containing\npaths relative to the include directory, instead of being absolute.\n\nThis adds a little script to launch the compiler then resolve any relative paths\nin the `.d` file before `ninja` looks at it. To use it, I run the build with\n```\nexport CMAKE_CUDA_COMPILER_LAUNCHER=\"python;`pwd`/tools/nvcc_fix_deps.py;ccache\"\n```\n\nThere are some possible pit-falls here. The same relative path might work for\ntwo include directories, and the compiler could pick a different one. Or,\nthe compiler might have additional implicit include directories that are needed\nto resolve the path. However, this has worked perfectly in my testing and it's\ncompletely opt-in so should be fine.\n\nTest Plan: Imported from OSS\n\nReviewed By: ejguan\n\nDifferential Revision: D31503351\n\nPulled By: malfet\n\nfbshipit-source-id: b184c4526679d976b93829b5715cafcb1c7db2ae", "pr_number": "62550", "files_changed": ["CONTRIBUTING.md", "tools/nvcc_fix_deps.py"], "labels": ["open source", "cla signed", "ci/no-build", "ciflow/default"]}, "221c308389": {"title": "Wextra fix for LossCTC.cpp (#66381)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66381\n\nFixes\n```\nstderr: caffe2/aten/src/ATen/native/cudnn/LossCTC.cpp:83:37: error: comparison of integers of different signs: 'size_t' (aka 'unsigned long') and 'const long' [-Werror,-Wsign-compare]\n  TORCH_CHECK(input_lengths_.size() == batch_size, \"input_lengths needs to have size to match batch_size\");\n              ~~~~~~~~~~~~~~~~~~~~~ ^  ~~~~~~~~~~\n```\n\nTest Plan: Sandcastle\n\nReviewed By: ngimel\n\nDifferential Revision: D31510217\n\nfbshipit-source-id: e3585e08650950c08d80d347dfae375aedf2ceaf", "pr_number": "66381", "files_changed": ["aten/src/ATen/native/cudnn/LossCTC.cpp"], "labels": ["fb-exported", "ciflow/default"]}, "acb0157a3d": {"title": "Specialization for `c10::util:get_type_index<std::string>` (#66290)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66290\n\nAdd full specialization for std::string type index\n\nIt slightly speeds up compilation as well as solves the ambiguity how template instantiations implemented in inline namespaces are rendered during `__PRETTY_FUNCTION__` computation.\n\nNot sure what `#pragma` controls this behaviour, but when code is compiled by clang-12+ using libstdc++, `__PRETTY_PRINT__`, sometimes resolve `std::string` to `std::basic_string<char>` and sometimes to `std::__cxx11::basic_string<char>`, even though in the object file symbol is always inside `std::__cxx11::` namespace, which might break caffe2 serialization code that depends on dynamic hash generation\n\nTemplate name resolution were debugged using https://gist.github.com/malfet/c83b9ebd35730ebf8bac7af42682ea37\n\n(Note: this ignores all push blocking failures!)\n\nTest Plan: CI\n\nReviewed By: r-barnes\n\nDifferential Revision: D31490050\n\nfbshipit-source-id: 127091574cf6b92c7ec3f972821e4e76f5f626a9", "pr_number": "66290", "files_changed": ["c10/util/TypeIndex.h"], "labels": ["fb-exported", "ciflow/default"]}, "998cb98844": {"title": "[PyTorch][jit] Cache TupleType objects in getTypePtr (#66340)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66340\n\nFor functions that take `std::vector`s with `std::tuple`s in them, `getTypePtr` can get hit on every call, in which case creating a new `TupleType` object every time is expensive.\nghstack-source-id: 140143104\n\nTest Plan: CI\n\nReviewed By: hlu1, JasonHanwen\n\nDifferential Revision: D31514792\n\nfbshipit-source-id: 23652ca90ba1259afc05e953b99ce1fe1bebcc2b", "pr_number": "66340", "files_changed": ["aten/src/ATen/core/jit_type.h"], "labels": ["cla signed", "ciflow/default"]}, "8c468ce00b": {"title": "[PyTorch][JIT] Return a reference from caching specializations of getTypePtr (#66342)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66342\n\n`decltype(auto)` in D31486117 (https://github.com/pytorch/pytorch/commit/fb5a80ffd80997eabf724282e62a7eaf4a78c2ad) wasn't the right choice in these specializations, because it will *still* deduce a copy.\nSee https://godbolt.org/z/GjbcPE1c4 for example.\nghstack-source-id: 140144199\n\nTest Plan: CI, added new static_assert to make sure we got it right for std::tuple in particular\n\nReviewed By: hlu1, JasonHanwen\n\nDifferential Revision: D31514960\n\nfbshipit-source-id: cae722aa34345b590c46eae478229cb5f4b0d7dc", "pr_number": "66342", "files_changed": ["aten/src/ATen/core/jit_type.h", "aten/src/ATen/core/type.cpp"], "labels": ["cla signed", "ciflow/default"]}, "08fab7ae13": {"title": "Wextra fix for Integration.cpp (#66321)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66321\n\nFixes\n```\nstderr: caffe2/aten/src/ATen/native/Integration.cpp:62:27: error: comparison of integers of different signs: 'size_t' (aka 'unsigned long') and 'int64_t' (aka 'long') [-Werror,-Wsign-compare]\n    if (curr_shape.size() >= target_n_dim)\n        ~~~~~~~~~~~~~~~~~ ^  ~~~~~~~~~~~~\nstderr: caffe2/aten/src/ATen/native/Integration.cpp:62:27: error: comparison of integers of different signs: 'size_t' (aka 'unsigned long') and 'int64_t' (aka 'long') [-Werror,-Wsign-compare]\n    if (curr_shape.size() >= target_n_dim)\n        ~~~~~~~~~~~~~~~~~ ^  ~~~~~~~~~~~~\n```\n\nTest Plan: Sandcastle\n\nReviewed By: ngimel\n\nDifferential Revision: D31505347\n\nfbshipit-source-id: 100b76215f78c3ce75bf4a993715a6767189747d", "pr_number": "66321", "files_changed": ["aten/src/ATen/native/Integration.cpp"], "labels": ["fb-exported", "ciflow/default"]}, "49f1605392": {"title": "[RFC] Reduce logging noise from AdagradOptimizer (#66443)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66443\n\nFor some reason, this logging is adding noise to a lot of flow jobs. I am not sure if this is actually needed.\nThis is called from the __init__ so it's logged all the time and logs all key:values the current local symbol.\n\nTest Plan: N/A\n\nReviewed By: chowarfb\n\nDifferential Revision: D31534372\n\nfbshipit-source-id: bed032b66fed548c97a6f66b1b9e905fd2738851", "pr_number": "66443", "files_changed": ["caffe2/python/optimizer.py"], "labels": ["fb-exported", "ciflow/default"]}, "ae5a9a451f": {"title": "Do not enforce unused vars rule for torch_deploy (#66447)", "body": "Summary:\nFollowup after  https://github.com/pytorch/pytorch/pull/66041\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66447\n\nReviewed By: seemethere\n\nDifferential Revision: D31554356\n\nPulled By: malfet\n\nfbshipit-source-id: 6638324dcf658f4b244da285b4360ff2e2e2c013", "pr_number": "66447", "files_changed": ["torch/CMakeLists.txt"], "labels": ["ciflow/default", "ciflow/scheduled"]}, "88ed93c2ca": {"title": "Fix type checking errors in torch/quantization/fx/qconfig_utils.py (#66428)", "body": "Summary:\n- [x] Fix the Pyre type checking errors in `torch/quantization/fx/qconfig_utils.py`\n```\ntorch/quantization/fx/qconfig_utils.py:241:46 Incompatible variable type [9]: prepare_custom_config_dict is declared to have type `Dict[str, typing.Any]` but is used as type `None`.\ntorch/quantization/fx/qconfig_utils.py:267:46 Incompatible variable type [9]: convert_custom_config_dict is declared to have type `Dict[str, typing.Any]` but is used as type `None`.\ntorch/quantization/fx/qconfig_utils.py:284:43 Incompatible variable type [9]: fuse_custom_config_dict is declared to have type `Dict[str, typing.Any]` but is used as type `None`.\n```\nFixes the issue: [MLH-Fellowship/pyre-check/issues/73](https://github.com/MLH-Fellowship/pyre-check/issues/73)\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66428\n\nReviewed By: grievejia\n\nDifferential Revision: D31545215\n\nPulled By: 0xedward\n\nfbshipit-source-id: 767ae7888854c2eec2ecf14855a5b011110b9271", "pr_number": "66428", "files_changed": ["torch/ao/quantization/fx/qconfig_utils.py"], "labels": ["open source", "ciflow/default"]}, "565cf47abf": {"title": "Quantization docs: add pages for Numeric Suite (Eager and FX) (#66380)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66380\n\nDescription:\n1. creates doc pages for Eager and FX numeric suites\n2. adds a link from main quantization doc to (1)\n3. formats docblocks in Eager NS to render well\n4. adds example code and docblocks to FX numeric suite\n\nTest Plan:\n```\ncd docs\nmake html\npython -m http.server\n// renders well\n```\n\nReviewed By: jerryzh168\n\nDifferential Revision: D31543173\n\nPulled By: vkuzo\n\nfbshipit-source-id: feb291bcbe92747495f45165f738631fa5cbffbd", "pr_number": "66380", "files_changed": ["docs/source/quantization.rst", "docs/source/torch.ao.ns._numeric_suite.rst", "docs/source/torch.ao.ns._numeric_suite_fx.rst", "torch/ao/ns/_numeric_suite.py", "torch/ao/ns/_numeric_suite_fx.py", "torch/ao/ns/fx/utils.py"], "labels": ["ciflow/default"]}, "d8532e3524": {"title": "[PyTorch] Split c10 Type.cpp into two files to allow targets to include one of them (#66445)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66445\n\n`Type.cpp` implements `demangle()` function based on the macro `HAS_DEMANGLE`. This diff splits it into two `.cpps` so that we can add either one into the build target. This change follows the patternof `flags_use_no_gflags.cpp` and `flags_use_gflags.cpp`.\n\nTest Plan: Rely on CI\n\nReviewed By: iseeyuan\n\nDifferential Revision: D31551432\n\nfbshipit-source-id: f8b11783e513fa812228ec873459ad3043ff9147", "pr_number": "66445", "files_changed": ["c10/macros/Macros.h", "c10/util/Type.cpp", "c10/util/Type_demangle.cpp", "c10/util/Type_no_demangle.cpp"], "labels": ["fb-exported", "ciflow/default"]}, "2d1552824a": {"title": "Revert D31386275: Migrate THCState to ATen", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD31386275 (https://github.com/pytorch/pytorch/commit/a6774d6e1f3a88f931a3786b7c2617a74bdaa50e)\n\nOriginal commit changeset: 5c1f1bbe8c3d\n\nfbshipit-source-id: bea4e80fb0bdc57e8bb6a8ee781afd224adf4ed0", "pr_number": null, "files_changed": ["aten/src/ATen/cuda/PeerToPeerAccess.cpp", "aten/src/ATen/cuda/PeerToPeerAccess.h", "aten/src/ATen/cuda/detail/CUDAHooks.cpp", "aten/src/ATen/cuda/detail/CUDAHooks.h", "aten/src/ATen/native/cuda/Copy.cu", "aten/src/ATen/native/cuda/Resize.cpp", "aten/src/THC/CMakeLists.txt", "aten/src/THC/THCGeneral.cpp", "aten/src/THC/THCGeneral.h.in", "aten/src/THC/THCGeneral.hpp", "aten/src/THC/THCTensorMathMagma.cpp"], "labels": []}, "18e4688199": {"title": "[Pytorch Edge] Improve bundled inputs name error handling (#65856)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65856\n\nOccasionally functions dont have this __name__ variable set and have name set instead? Not sure why this happens, but this should catch it.\n\nTest Plan: ci\n\nReviewed By: iseeyuan\n\nDifferential Revision: D31286787\n\nfbshipit-source-id: 8a339541215329b6e9ff43ef77363be41f19c5ca", "pr_number": "65856", "files_changed": ["torch/utils/bundled_inputs.py"], "labels": ["fb-exported", "cla signed", "ciflow/default"]}, "1841f76cc0": {"title": "Remove native_functions.yaml dependency from unary ops (#64170)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/64170\n\nTest Plan: Imported from OSS\n\nReviewed By: gchanan, ezyang\n\nDifferential Revision: D30728578\n\nPulled By: dagitses\n\nfbshipit-source-id: 70baa90d0834e68324504c74064a1d1790193483", "pr_number": "64170", "files_changed": ["aten/src/ATen/native/UnaryOps.h", "aten/src/ATen/native/cpu/DistributionKernels.cpp", "aten/src/ATen/native/cpu/UnaryOpsKernel.cpp", "aten/src/ATen/native/cuda/AbsKernel.cu", "aten/src/ATen/native/cuda/UnaryComplexKernels.cu", "aten/src/ATen/native/cuda/UnaryFractionKernels.cu", "aten/src/ATen/native/cuda/UnaryGammaKernels.cu", "aten/src/ATen/native/cuda/UnaryGeometricKernels.cu", "aten/src/ATen/native/cuda/UnaryLogKernels.cu", "aten/src/ATen/native/cuda/UnaryOpsKernel.cu", "aten/src/ATen/native/cuda/UnarySignKernels.cu", "aten/src/ATen/native/cuda/UnarySpecialOpsKernel.cu", "tools/build_variables.bzl"], "labels": ["open source", "cla signed", "ciflow/default"]}, "8674a3c6e3": {"title": "Remove native_functions.yaml dependency from PowKernel (#64171)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/64171\n\nTest Plan: Imported from OSS\n\nReviewed By: gchanan\n\nDifferential Revision: D30728583\n\nPulled By: dagitses\n\nfbshipit-source-id: ea6891a3598eead93daea620b94e50d3a3b248cf", "pr_number": "64171", "files_changed": ["aten/src/ATen/native/Pow.h", "aten/src/ATen/native/cpu/PowKernel.cpp", "aten/src/ATen/native/cuda/PowKernel.cu"], "labels": ["open source", "cla signed", "ciflow/default"]}, "213ac4e59c": {"title": "Remove native_functions.yaml dependency from PointwiseOps (#64172)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/64172\n\nTest Plan: Imported from OSS\n\nReviewed By: gchanan\n\nDifferential Revision: D30728584\n\nPulled By: dagitses\n\nfbshipit-source-id: 2ae9686ac7c312e2d470d26a3cad12afcf7ef47b", "pr_number": "64172", "files_changed": ["aten/src/ATen/native/PointwiseOps.h", "aten/src/ATen/native/cpu/PointwiseOpsKernel.cpp", "aten/src/ATen/native/cuda/PointwiseOpsKernel.cu"], "labels": ["open source", "cla signed", "ciflow/default"]}, "8818dda237": {"title": "Fix lstsq to work with inputs that require grad (#66426)", "body": "Summary:\nI updated `sample_inputs_linalg_lstsq` and `test_nondifferentiable`\nnow correctly reveals the failure. The internal assert error was thrown\nbecause autograd attempts to mark integer tensor as differentiable.\n\nFixes https://github.com/pytorch/pytorch/issues/66420.\n\ncc ezyang albanD zou3519 gqchen pearu nikitaved soulitzer Lezcano Varal7\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66426\n\nReviewed By: ejguan\n\nDifferential Revision: D31550942\n\nPulled By: albanD\n\nfbshipit-source-id: 4a0ca60e62c5e9bb96af5020541da2d09ea3e405", "pr_number": "66426", "files_changed": ["tools/autograd/derivatives.yaml", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["module: autograd", "open source", "ciflow/default"]}, "702fb1de72": {"title": "[fx2trt] open source tests for acc tracer (#66302)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66302\n\nJust move files, ossci can be setup later\n\nTest Plan:\nbuck run //caffe2/test:test_fx_acc_tracer\n\ntestinprod\n\nReviewed By: 842974287\n\nDifferential Revision: D31495087\n\nfbshipit-source-id: f182c7438e3e80ba98924990682cb45a99b9967c", "pr_number": "66302", "files_changed": ["test/fx_acc/test_acc_tracer.py"], "labels": ["fb-exported", "ciflow/default"]}, "17e79bc76c": {"title": "remove is_reference from all is_output_quantized (#66456)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/66456\n\nTest Plan: Imported from OSS\n\nReviewed By: jerryzh168\n\nDifferential Revision: D31562633\n\nPulled By: rahxephon89\n\nfbshipit-source-id: 85c73a23e90ba9c1406f4027d447fbbe4576e39a", "pr_number": "66456", "files_changed": ["torch/ao/quantization/fx/convert.py", "torch/ao/quantization/fx/prepare.py", "torch/ao/quantization/fx/quantization_patterns.py"], "labels": ["ciflow/default"]}, "47c531b6e8": {"title": "[jit] Compare object identity first in ClassType::operator== (#65347)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65347\n\nThis check is much cheaper than anything involving actually inspecting object fields (i.e., the cost is low), and if it succeeds we can skip the expensive (e.g., it involves locking a weak_ptr and then destroying the resulting shared_ptr)  function body. It almost entirely eliminates time spent in this function during model loading according to perf.\nghstack-source-id: 140148561\n\nTest Plan: Specifically I profiled static runtime startup for the ctr_mobile_feed model and saw self time in this function go from 2-3% to 0.36%.\n\nReviewed By: ejguan\n\nDifferential Revision: D31057279\n\nfbshipit-source-id: efb6bdc0957b680112ac282e85dc1b06b1b6c0bd", "pr_number": "65347", "files_changed": ["aten/src/ATen/core/jit_type.h"], "labels": ["cla signed", "ciflow/default"]}, "9984f4bb8b": {"title": "Remove native_functions.yaml dependency from some reduction operators (#64173)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/64173\n\nThis one also required restructuring the code a bit to move the kernel\ncode into seperate files. So, I've mainly focused on CUDA which is\nwhere the real build-time issues are.\n\nTest Plan: Imported from OSS\n\nReviewed By: jbschlosser, ezyang\n\nDifferential Revision: D30728581\n\nPulled By: dagitses\n\nfbshipit-source-id: a69eea5b4100d16165a02660dde200c8f648683d", "pr_number": "64173", "files_changed": ["aten/src/ATen/core/Tensor.cpp", "aten/src/ATen/core/TensorBase.h", "aten/src/ATen/native/LinearAlgebra.h", "aten/src/ATen/native/ReduceAllOps.h", "aten/src/ATen/native/ReduceOps.h", "aten/src/ATen/native/TensorCompare.h", "aten/src/ATen/native/cpu/Reduce.h", "aten/src/ATen/native/cpu/SumKernel.cpp", "aten/src/ATen/native/cuda/Equal.cpp", "aten/src/ATen/native/cuda/LinearAlgebra.cu", "aten/src/ATen/native/cuda/Reduce.cuh", "aten/src/ATen/native/cuda/ReduceLogicKernel.cu", "aten/src/ATen/native/cuda/ReduceMinMaxKernel.cu", "aten/src/ATen/native/cuda/ReduceMomentKernel.cu", "aten/src/ATen/native/cuda/ReduceNormKernel.cu", "aten/src/ATen/native/cuda/ReduceOps.cpp", "aten/src/ATen/native/cuda/ReduceOps.h", "aten/src/ATen/native/cuda/ReduceSumProdKernel.cu", "aten/src/ATen/native/cuda/TensorCompare.cpp", "aten/src/ATen/native/cuda/TensorCompare.cu", "caffe2/CMakeLists.txt"], "labels": ["open source", "cla signed", "ciflow/default"]}, "3ac2c74896": {"title": "Revert D31082208: Use shared CUPTI by default", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD31082208 (https://github.com/pytorch/pytorch/commit/8b0eae5aa86dac184486d0d9481fff8a25aa7853)\n\nOriginal commit changeset: 14f66af92084\n\nfbshipit-source-id: 0faff00832b7f79d476fd1f9f505142a548a76db", "pr_number": null, "files_changed": ["CMakeLists.txt", "cmake/Dependencies.cmake"], "labels": []}, "d32736e317": {"title": "Make permission errors more human readable (#66492)", "body": "Summary:\n`_mkdir_p` feels like a remnant of Python-2 era, add `exist_ok` argument and re-raise OSError to make it more human readable.\n\nAfter the change attempt to build PyTorch in a folder that does not have write permissions will result in:\n```\n% python3.6 setup.py develop\nBuilding wheel torch-1.10.0a0+git9509e8a\n-- Building version 1.10.0a0+git9509e8a\nTraceback (most recent call last):\n  File \"/Users/nshulga/git/pytorch-worktree/tools/setup_helpers/cmake.py\", line 21, in _mkdir_p\n    os.makedirs(d, exist_ok=True)\n  File \"/opt/homebrew/Cellar/python36/3.6.2+_254.20170915/Frameworks/Python.framework/Versions/3.6/lib/python3.6/os.py\", line 220, in makedirs\n    mkdir(name, mode)\nPermissionError: [Errno 13] Permission denied: 'build'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"setup.py\", line 895, in <module>\n    build_deps()\n  File \"setup.py\", line 370, in build_deps\n    cmake=cmake)\n  File \"/Users/nshulga/git/pytorch-worktree/tools/build_pytorch_libs.py\", line 63, in build_caffe2\n    rerun_cmake)\n  File \"/Users/nshulga/git/pytorch-worktree/tools/setup_helpers/cmake.py\", line 225, in generate\n    _mkdir_p(self.build_dir)\n  File \"/Users/nshulga/git/pytorch-worktree/tools/setup_helpers/cmake.py\", line 23, in _mkdir_p\n    raise RuntimeError(f\"Failed to create folder {os.path.abspath(d)}: {e.strerror}\") from e\nRuntimeError: Failed to create folder /Users/nshulga/git/pytorch-worktree/build: Permission denied\n```\n\nFixes https://github.com/pytorch/pytorch/issues/65920\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66492\n\nReviewed By: seemethere\n\nDifferential Revision: D31578820\n\nPulled By: malfet\n\nfbshipit-source-id: afe8240983100ac0a26cc540376b9dd71b1b53af", "pr_number": "66492", "files_changed": ["tools/setup_helpers/cmake.py"], "labels": ["ciflow/default"]}, "40794dbb25": {"title": "add backend_config_dict to checkGraphModeFxOp (#66499)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/66499\n\nTest Plan: Imported from OSS\n\nReviewed By: jerryzh168\n\nDifferential Revision: D31582518\n\nPulled By: rahxephon89\n\nfbshipit-source-id: b8107bb7140517f2dc32bf692c6b916536ea35c3", "pr_number": "66499", "files_changed": ["torch/testing/_internal/common_quantization.py"], "labels": ["ciflow/default"]}, "08f3823647": {"title": "Sparse CSR CUDA: add `addmv_out` (#61407)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/61407\n\nThis PR adds `addmv_out_sparse_csr_cuda`. The operation is used to\ncompute matrix-vector multiplication. Since structured_delegate is used\nwe only need to implement the out variant, the in-place and normal\nvariants are autogenerated.\nWorking on this PR revealed that float16 (and probably bfloat16) inputs\ndo not work correctly in cusparse, therefore for this case `addmm` is\nused with squeezes and unsqueezes.\n\ncc nikitaved pearu cpuhrsch IvanYashchuk ngimel\n\nTest Plan: Imported from OSS\n\nReviewed By: malfet\n\nDifferential Revision: D31584499\n\nPulled By: ngimel\n\nfbshipit-source-id: 4c507791471ada88969116b88eeaaba7a7536431", "pr_number": "61407", "files_changed": ["BUILD.bazel", "aten/src/ATen/cuda/CUDASparseDescriptors.cpp", "aten/src/ATen/cuda/CUDASparseDescriptors.h", "aten/src/ATen/native/Blas.cpp", "aten/src/ATen/native/native_functions.yaml", "aten/src/ATen/native/sparse/cuda/SparseBlas.cpp", "aten/src/ATen/native/sparse/cuda/SparseBlasImpl.cpp", "aten/src/ATen/native/sparse/cuda/SparseBlasImpl.h", "test/test_sparse_csr.py", "torch/testing/_internal/common_device_type.py"], "labels": ["module: sparse", "module: cuda", "open source", "cla signed", "ciflow/default", "ciflow/cuda"]}, "8eb85b5027": {"title": "Remove THCNumerics (#66388)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/66388\n\nTest Plan: Imported from OSS\n\nReviewed By: mruberry\n\nDifferential Revision: D31547710\n\nPulled By: ngimel\n\nfbshipit-source-id: 20710328f2e5fc2e931a3f8ba9b4243acc310d54", "pr_number": "66388", "files_changed": ["aten/src/ATen/NumericUtils.h", "aten/src/ATen/native/cuda/AdaptiveAveragePooling3d.cu", "aten/src/ATen/native/cuda/AdaptiveMaxPooling2d.cu", "aten/src/ATen/native/cuda/AdaptiveMaxPooling3d.cu", "aten/src/ATen/native/cuda/DilatedMaxPool2d.cu", "aten/src/ATen/native/cuda/DilatedMaxPool3d.cu", "aten/src/ATen/native/cuda/FractionalMaxPool2d.cu", "aten/src/ATen/native/cuda/FractionalMaxPool3d.cu", "aten/src/ATen/native/cuda/ReduceMinMaxKernel.cu", "aten/src/ATen/native/cuda/Sorting.cu", "aten/src/ATen/native/cuda/SortingCommon.cuh", "aten/src/ATen/native/cuda/SortingRadixSelect.cuh", "aten/src/ATen/native/cuda/SummaryOps.cu", "aten/src/THC/CMakeLists.txt", "aten/src/THC/THCNumerics.cuh", "aten/src/THC/THCTensorMathReduce.cuh", "test/test_cpp_extensions_jit.py"], "labels": ["module: porting", "open source", "ciflow/default"]}, "80a3619823": {"title": "Remove THCTensorMathReduce.cuh (#66389)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/66389\n\nTest Plan: Imported from OSS\n\nReviewed By: ejguan\n\nDifferential Revision: D31547711\n\nPulled By: ngimel\n\nfbshipit-source-id: c181d14f66536b6873b5b14088312c6c70bf0855", "pr_number": "66389", "files_changed": ["aten/src/ATen/native/cuda/DistanceKernel.cu", "aten/src/ATen/native/cuda/SoftMax.cu", "aten/src/ATen/native/cuda/TensorTopK.cu", "aten/src/ATen/native/cuda/WeightNorm.cu", "aten/src/THC/CMakeLists.txt", "aten/src/THC/THCTensorMathReduce.cuh"], "labels": ["module: porting", "open source", "ciflow/default"]}, "9918fd8305": {"title": "[fx2trt] open source tests for converters (#66361)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66361\n\nossci will be setup later, fbonly ci is ready\n\nTest Plan:\nbuck run caffe2/test:fx2trt_test_linear\n\ntestinprod\n\nReviewed By: 842974287\n\nDifferential Revision: D31511082\n\nfbshipit-source-id: 9e2c50c83fdba822cd2488eb17b5787d8a57f087", "pr_number": "66361", "files_changed": ["test/fx2trt/converters/acc_op/test_adaptive_avgpool.py", "test/fx2trt/converters/acc_op/test_avgpool.py", "test/fx2trt/converters/acc_op/test_batchnorm.py", "test/fx2trt/converters/acc_op/test_binary_ops.py", "test/fx2trt/converters/acc_op/test_cat.py", "test/fx2trt/converters/acc_op/test_clamp.py", "test/fx2trt/converters/acc_op/test_convolution.py", "test/fx2trt/converters/acc_op/test_dequantize.py", "test/fx2trt/converters/acc_op/test_flatten.py", "test/fx2trt/converters/acc_op/test_gelu.py", "test/fx2trt/converters/acc_op/test_getitem.py", "test/fx2trt/converters/acc_op/test_layer_norm.py", "test/fx2trt/converters/acc_op/test_linear.py", "test/fx2trt/converters/acc_op/test_matmul.py", "test/fx2trt/converters/acc_op/test_max.py", "test/fx2trt/converters/acc_op/test_maximum.py", "test/fx2trt/converters/acc_op/test_maxpool.py", "test/fx2trt/converters/acc_op/test_min.py", "test/fx2trt/converters/acc_op/test_minimum.py", "test/fx2trt/converters/acc_op/test_narrow.py", "test/fx2trt/converters/acc_op/test_permute.py", "test/fx2trt/converters/acc_op/test_quantize_per_tensor.py", "test/fx2trt/converters/acc_op/test_relu.py", "test/fx2trt/converters/acc_op/test_reshape.py", "test/fx2trt/converters/acc_op/test_sigmoid.py", "test/fx2trt/converters/acc_op/test_size.py", "test/fx2trt/converters/acc_op/test_softmax.py", "test/fx2trt/converters/acc_op/test_split.py", "test/fx2trt/converters/acc_op/test_squeeze.py", "test/fx2trt/converters/acc_op/test_sum.py", "test/fx2trt/converters/acc_op/test_tanh.py", "test/fx2trt/converters/acc_op/test_tile.py", "test/fx2trt/converters/acc_op/test_topk.py", "test/fx2trt/converters/acc_op/test_unary_ops.py", "test/fx2trt/converters/acc_op/test_unsqueeze.py", "test/fx2trt/converters/vanilla/test_add.py", "test/fx2trt/converters/vanilla/test_convolution.py"], "labels": ["fb-exported", "ciflow/default"]}, "ecb7b38c00": {"title": "[PyTorch] Support additional arguments in Python record function (#65736)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65736\n\nWe ran into some limitations to extract PyTorch operator parameters through hooks or the execution graph. Some of these limitations are not due to the operator not exposing them, rather the inputs for these operators are already fused/processed in some cases (like embedding table). We want to be able to attach some metadata to the user scope record functions allowing the profilers to later extract these information.\n\nThe record function C++ API already supports taking inputs and outputs information. The corresponding Python interface does not support them and only allows a string name as record function parameter.\n\nThis diff adds support for user to optionally to add additional arguments to the record function in two ways.\n1. to remain backward compatible with `record_function_op`, we have added an optional string arg to the interface: `with record_function(name, arg_str)`.\n2. to support data dependency graph, we also have the new `torch.autograd._record_function_with_args_enter` and `torch.autograd._record_function_with_args_exit` functions to provide an interface where we can give additional tensor arguments. For now we imagine this can be used for debugging or analysis purpose. In this form, we currently support some basic data types as inputs: scalars, string, list, and tensor.\n\nExample usage:\n\n```\n# record_function operator with a name and optionally, a string for arguments.\nwith record_function(\"## TEST 1 ##\", \"[1, 2, 3]\"):\n    <actual module or operator>\n\n# more general form of record_function\na = _record_function_with_args_enter(\"## TEST 2 ##\", 1, False, 2.5, [u, u], \"hello\", u)\n<actual module or operator>\n_record_function_with_args_exit(a)\n\n```\nCorresponding outputs in execution graph:\n```\n    {\n      \"name\": \"## TEST 2 ##\", \"id\": 7, \"parent\": 3, \"fw_parent\": 0, \"scope\": 5, \"tid\": 1, \"fw_tid\": 0,\n      \"inputs\": [1,false,2.5,[6,6],\"hello\",6], \"input_shapes\": [[],[],[],[[3,4,5],[3,4,5]],[],[3,4,5]], \"input_types\": [\"Int\",\"Bool\",\"Double\",\"GenericList[Tensor(float),Tensor(float)]\",\"String\",\"Tensor(float)\"],\n      \"outputs\": [], \"output_shapes\": [], \"output_types\": []\n    },\n    {\n      \"name\": \"## TEST 1 ##\", \"id\": 3, \"parent\": 2, \"fw_parent\": 0, \"scope\": 5, \"tid\": 1, \"fw_tid\": 0,\n      \"inputs\": [\"1, 2, 3\"], \"input_shapes\": [[]], \"input_types\": [\"String\"],\n      \"outputs\": [], \"output_shapes\": [], \"output_types\": []\n    },\n```\n\nTest Plan:\n```\n=> buck build caffe2/test:profiler --show-output\n=> buck-out/gen/caffe2/test/profiler#binary.par test_profiler.TestRecordFunction\ntest_record_function (test_profiler.TestRecordFunction) ... Log file: /tmp/libkineto_activities_1651304.json\nNet filter:\nTarget net for iteration count:\nNet Iterations: 3\nINFO:2021-09-27 01:10:15 1651304:1651304 Config.cpp:424] Trace start time: 2021-09-27 01:10:30\nTrace duration: 500ms\nWarmup duration: 5s\nNet size threshold: 0\nGPU op count threshold: 0\nMax GPU buffer size: 128MB\nEnabled activities: cpu_op,user_annotation,external_correlation,cuda_runtime,cpu_instant_event\nManifold bucket: gpu_traces\nManifold object: tree/traces/clientAPI/0/1632730215/devvm2060.ftw0/libkineto_activities_1651304.json\nTrace compression enabled: 1\nINFO:2021-09-27 01:10:15 1651304:1651304 ActivityProfiler.cpp:536] Tracing starting in 14s\nINFO:2021-09-27 01:10:15 1651304:1651304 ActivityProfiler.cpp:48] Target net for iterations not specified - picking first encountered that passes net filter\nINFO:2021-09-27 01:10:15 1651304:1651304 ActivityProfiler.cpp:57] Tracking net PyTorch Profiler for 3 iterations\nINFO:2021-09-27 01:10:15 1651304:1651304 ActivityProfiler.cpp:126] Processing 1 CPU buffers\nINFO:2021-09-27 01:10:15 1651304:1651304 ActivityProfiler.cpp:686] Recorded nets:\nINFO:2021-09-27 01:10:15 1651304:1651304 ActivityProfiler.cpp:689] PyTorch Profiler: 1 iterations\nok\n\n----------------------------------------------------------------------\nRan 1 test in 0.021s\n\nOK\n```\n\nReviewed By: gdankel\n\nDifferential Revision: D31165259\n\nfbshipit-source-id: 15920aaef7138c666e5eca2a71c3bf33073eadc4", "pr_number": "65736", "files_changed": ["aten/src/ATen/record_function.cpp", "test/test_profiler.py", "torch/_C/_autograd.pyi", "torch/autograd/__init__.py", "torch/autograd/profiler.py", "torch/csrc/autograd/init.cpp", "torch/csrc/autograd/record_function_ops.cpp", "torch/csrc/autograd/record_function_ops.h"], "labels": ["fb-exported", "cla signed", "ciflow/default"]}, "87df043f63": {"title": "[Bootcamp][Pytorch]Add testing for complex parameters in Adagrad optimizer (#66501)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66501\n\nAdd testing for the Adagrad optimizer to ensure that it behaves as if complex numbers are two real numbers in R^2 as per issue 65711 on github\nghstack-source-id: 140414042\n\nTest Plan:\nbuck test mode/dev caffe2/test:optim -- 'test_adagrad_complex'\n\nhttps://pxl.cl/1R27M\n\nReviewed By: albanD\n\nDifferential Revision: D31584240\n\nfbshipit-source-id: 5c9938084566b8ea49cc8ff002789731f62fe87e", "pr_number": "66501", "files_changed": ["test/test_optim.py"], "labels": ["cla signed", "ciflow/default"]}, "6401658b08": {"title": "fix type error in hipify_python.py (#66164)", "body": "Summary:\n- [x] Fixed the Pyre type checking errors in `torch/utils/hipify/hipify_python.py`:\n```\ntorch/utils/hipify/hipify_python.py:196:8 Incompatible variable type [9]: clean_ctx is declared to have type `GeneratedFileCleaner` but is used as type `None`.\ntorch/utils/hipify/hipify_python.py:944:4 Incompatible variable type [9]: clean_ctx is declared to have type `GeneratedFileCleaner` but is used as type `None`.\n```\n\nFixing the issue: https://github.com/MLH-Fellowship/pyre-check/issues/78\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66164\n\nReviewed By: onionymous\n\nDifferential Revision: D31411443\n\nPulled By: 0xedward\n\nfbshipit-source-id: c69f8fb839ad1d5ba5e4a223e1322ae7207e1574", "pr_number": "66164", "files_changed": ["torch/utils/hipify/hipify_python.py"], "labels": ["triaged", "open source", "cla signed", "ciflow/default"]}, "84385c40e4": {"title": "Add output_mask (#66068)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/66068\n\nTest Plan: Imported from OSS\n\nReviewed By: dagitses\n\nDifferential Revision: D31431802\n\nPulled By: albanD\n\nfbshipit-source-id: 322aae5614dacb06fd45e513465b7a5cc11f4dbb", "pr_number": "66068", "files_changed": ["aten/src/ATen/native/GridSampler.cpp", "aten/src/ATen/native/cpu/GridSamplerKernel.cpp", "aten/src/ATen/native/cpu/GridSamplerKernel.h", "aten/src/ATen/native/cuda/GridSampler.cu", "aten/src/ATen/native/native_functions.yaml", "test/backward_compatibility/check_backward_compatibility.py", "tools/autograd/derivatives.yaml"], "labels": ["open source", "cla signed", "ciflow/default"]}, "f8d98b5a6d": {"title": "Compute input gradient only if required (CPU) (#66069)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/66069\n\nTest Plan: Imported from OSS\n\nReviewed By: dagitses\n\nDifferential Revision: D31431803\n\nPulled By: albanD\n\nfbshipit-source-id: d4caba5fa092e4ee7411502021836370082670b2", "pr_number": "66069", "files_changed": ["aten/src/ATen/native/cpu/GridSamplerKernel.cpp"], "labels": ["open source", "cla signed", "ciflow/default"]}, "8a40bb62f9": {"title": "Compute input gradient only if required (CUDA) (#66070)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/66070\n\nTest Plan: Imported from OSS\n\nReviewed By: dagitses\n\nDifferential Revision: D31431805\n\nPulled By: albanD\n\nfbshipit-source-id: 8c3de6632aaee168ec6fd7eb79a5af26973af9c5", "pr_number": "66070", "files_changed": ["aten/src/ATen/native/cuda/GridSampler.cu"], "labels": ["open source", "cla signed", "ciflow/default"]}, "a453ebc8ac": {"title": "Use interactive_embedded_interpreter to dynamicly loading various third-party libraries (#66512)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66512\n\nTLDR, we are able to use the interactive_embedded_interpreter (basically just torch::deploy interpreter with an interactive shell) to dynamicly load various third party libraries. We use the popular libraries numpy, scipy, regex, pandas for illustration purpose.\n\nA couple of changes need to be done for the interactive_embedded_interpreter:\n1, we need link with :embedded_interpreter_all rather than :embedded_interpreter so we can enable DEEPBIND and use our custom loader\n2, we provide a pylibRoot path to construct the InterpreterManager. The path will be added to the embedded interpreter's sys.path. Typically we can pass in the python library root path in a conda environment so torch::deploy interpreter can find all installed packages.\n3, we allow interactive_embedded_interpreter execute a script to ease recording the exploration of various python libraries.\nghstack-source-id: 140453213\n\nTest Plan:\nInstall numpy, scipy, regex, pandas in the conda environment or on the machine directly. Suppose /home/shunting/.local/lib/python3.8/site-packages/ is the root path for the installed libraries.\n\n- buck run mode/opt :interactive_embedded_interpreter -- --pylib_root=/home/shunting/.local/lib/python3.8/site-packages/ --pyscript=~/p7/iei_examples/try_regex.py\ncontent of try_regex.py:\n```\nimport regex\n\nprint(regex)\npat = r'(.+)\\1'\nprint(regex.match(pat, \"abcabc\"))\nprint(regex.match(pat, \"abcba\"))\n\nprint(\"bye\")\n```\n\n- buck run mode/opt :interactive_embedded_interpreter -- --pylib_root=/home/shunting/.local/lib/python3.8/site-packages/ --pyscript=~/p7/iei_examples/try_numpy.py\ncontent of try_numpy.py:\n```\nimport numpy as np\nprint(f\"numpy at {np}\")\na = np.random.rand(2, 3)\nb = np.random.rand(3, 2)\nprint(np.matmul(a, b))\n```\n\n- buck run mode/opt :interactive_embedded_interpreter -- --pylib_root=/home/shunting/.local/lib/python3.8/site-packages/ --pyscript=~/p7/iei_examples/try_scipy.py\ncontent of try_scipy.py:\n```\nimport numpy as np\nfrom scipy import linalg\n\nmat_a = np.array([[1, 0, 0, 0], [1, 1, 0, 0], [1, 2, 1, 0], [1, 3, 3, 1]])\nmat_b = linalg.inv(mat_a)\nprint(mat_b)\n```\n\n- buck run mode/opt :interactive_embedded_interpreter -- --pylib_root=/home/shunting/.local/lib/python3.8/site-packages/ --pyscript=~/p7/iei_examples/try_pandas.py\ncontent of try_pandas.py:\n```\nimport pandas as pd\nprint(f\"pandas at {pd}\")\ndf = pd.DataFrame({\n  \"col1\": [1, 2, 3, 4],\n  \"col2\": [2, 4, 8, 16],\n})\nprint(df)\n```\n\nReviewed By: suo\n\nDifferential Revision: D31587278\n\nfbshipit-source-id: c0b031c1fa71a77cdfeba1d04514f83127f79012", "pr_number": "66512", "files_changed": ["torch/csrc/deploy/deploy.cpp", "torch/csrc/deploy/deploy.h", "torch/csrc/deploy/interactive_embedded_interpreter.cpp"], "labels": ["cla signed", "ciflow/default"]}, "fdd9f49cf5": {"title": "add a note on numerical accuracy (#65947)", "body": "Summary:\nPer title\nFixes https://github.com/pytorch/pytorch/issues/54437\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65947\n\nReviewed By: albanD\n\nDifferential Revision: D31612445\n\nPulled By: ngimel\n\nfbshipit-source-id: 5c155891a088aef3b9813f253d0dc1ee4d51ae1c", "pr_number": "65947", "files_changed": ["docs/source/notes/numerical_accuracy.rst"], "labels": ["cla signed", "ciflow/default"]}, "f48f20e154": {"title": "Make ContainerHash compatible with const& types (#66497)", "body": "Summary:\n- this change should not impact existing use cases, but allows for\n  additional use cases where the container holds const types.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66497\n\nReviewed By: alanwaketan\n\nDifferential Revision: D31582242\n\nPulled By: wconstab\n\nfbshipit-source-id: 3a0e18b4afaf3c7ff93a0e3d09067ed066402b44", "pr_number": "66497", "files_changed": ["torch/csrc/lazy/core/hash.h"], "labels": ["ciflow/default"]}, "b792a77895": {"title": "Skip `interactive_embedded_interpreter.cpp` for clang-tidy (#66569)", "body": "Summary:\nFixes #{issue number}\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66569\n\nReviewed By: suo\n\nDifferential Revision: D31622885\n\nPulled By: malfet\n\nfbshipit-source-id: 61bad5ff3011f992cdd149724c935c098996d6a2", "pr_number": "66569", "files_changed": ["tools/linter/clang_tidy/__main__.py"], "labels": ["ciflow/default"]}, "c04bcde245": {"title": "Make empty* and *_like factory functions respect tensor subclasses (#65677)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/65243\n\ncc albanD\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65677\n\nReviewed By: dagitses\n\nDifferential Revision: D31432032\n\nPulled By: albanD\n\nfbshipit-source-id: 77f464974c7656c1206085aba9300471d7e0ef57", "pr_number": "65677", "files_changed": ["aten/src/ATen/native/native_functions.yaml", "test/test_python_dispatch.py", "tools/autograd/derivatives.yaml"], "labels": ["triaged", "open source", "cla signed", "ciflow/default", "module: __torch_dispatch__"]}, "86cf22cb1c": {"title": "Add OpInfo for torch.bucketize (#65821)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/65821\n\nReviewed By: malfet, mruberry\n\nDifferential Revision: D31386048\n\nPulled By: saketh-are\n\nfbshipit-source-id: fae7ec7b6b57436d87d38d421c5f3f52be4cdadd", "pr_number": "65821", "files_changed": ["aten/src/ATen/native/Bucketization.cpp", "aten/src/ATen/native/cuda/Bucketization.cu", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["cla signed", "ciflow/default", "ciflow/all"]}, "82986a17a6": {"title": "fix lint (#66572)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/66572\n\nTest Plan: Imported from OSS\n\nReviewed By: seemethere\n\nDifferential Revision: D31624043\n\nPulled By: suo\n\nfbshipit-source-id: 9db9cee3140d78c2a2f0c937be84755206fee1dd", "pr_number": "66572", "files_changed": ["aten/src/ATen/native/MathBitsFallback.h"], "labels": ["ciflow/default"]}, "37db650c9c": {"title": "[Static Runtime] Clone test does not use uninitialized memory (#66557)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66557\n\nThe test was previously using `at::empty_strided` to initialize one of its inputs. The contents of the tensor returned by this function are random, uninitialized memory. If we happened to get a NaN, this test would fail since `use_equalnan` was not set.\n\nTest Plan: `buck test caffe2/benchmarks/static_runtime:static_runtime_cpptest`\n\nReviewed By: hlu1\n\nDifferential Revision: D31611961\n\nfbshipit-source-id: 79a9476d0d6ce7a9f1412eefcef19bc2618c54b8", "pr_number": "66557", "files_changed": ["benchmarks/static_runtime/test_static_runtime.cc"], "labels": ["fb-exported", "cla signed", "ciflow/default"]}, "5f45927d15": {"title": "Autograd: Delay warnings until the end of backward execution (#66235)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/50209\n\nThis adds a new warning handler that stores all warnings in a shared\nqueue, which can be \"replayed\" at a later time and, crucially, on\nanother thread. Then, I use this inside the autograd engine to ensure\nthat warnings are processed by the handler registered on the main\nthread.\n\nFor testing, I also add an operator that always warns in the backward\npass and test that the warning is a normal Python warning.\n\ncc ezyang albanD zou3519 gqchen pearu nikitaved soulitzer Lezcano Varal7\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66235\n\nReviewed By: ejguan\n\nDifferential Revision: D31505413\n\nPulled By: albanD\n\nfbshipit-source-id: 1a7f60b038f55c20591c0748b9e86735b3fec2f9", "pr_number": "66235", "files_changed": ["aten/src/ATen/native/TestOps.cpp", "aten/src/ATen/native/native_functions.yaml", "c10/util/Exception.h", "test/test_autograd.py", "tools/autograd/derivatives.yaml", "tools/build_variables.bzl", "torch/csrc/Exceptions.cpp", "torch/csrc/Exceptions.h", "torch/csrc/autograd/FunctionsManual.cpp", "torch/csrc/autograd/FunctionsManual.h", "torch/csrc/autograd/engine.cpp", "torch/csrc/autograd/engine.h", "torch/csrc/autograd/utils/warnings.cpp", "torch/csrc/autograd/utils/warnings.h"], "labels": ["module: autograd", "triaged", "open source", "Merged", "cla signed", "ciflow/default"]}, "24202f7fb4": {"title": "Remove native_functions.yaml dependency from Activation.cu (#64499)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/64499\n\nThis moves the native functions into a separate Activation.cpp file,\nwhich calls into `launch_..._kernel` functions defined in `Activation.cu`.\nThe exception is `rrelu_with_noise` which is compilcated by the\nrandom number generation code, so I've moved it into its own file.\n\nTest Plan: Imported from OSS\n\nReviewed By: jbschlosser, ezyang\n\nDifferential Revision: D30867323\n\nPulled By: dagitses\n\nfbshipit-source-id: a4cd6f1fb1b1fed4cc356bf8b3778991ae2278ba", "pr_number": "64499", "files_changed": ["aten/src/ATen/cuda/ApplyGridUtils.cuh", "aten/src/ATen/cuda/CUDAApplyUtils.cuh", "aten/src/ATen/native/Activation.h", "aten/src/ATen/native/cpu/Activation.cpp", "aten/src/ATen/native/cuda/Activation.cpp", "aten/src/ATen/native/cuda/Activation.cu", "aten/src/ATen/native/cuda/Activation.h", "aten/src/ATen/native/cuda/RreluWithNoise.cu", "caffe2/CMakeLists.txt"], "labels": ["open source", "Merged", "cla signed", "ciflow/default"]}, "4e1c075542": {"title": "log_sigmoid: Use log1p for improved precision (#66441)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/20972\n\nlog_sigmoid calculates something like `log(1 + x)` where x is always a\npositive number less than one. This wastes floating point precision\nbecause the exponent always becomes zero. Instead, using\n`log1p(x)` gives the full mantissa precision around `x=0`.\n\nThis also fixes infinity propagation because the old code does,\n`exp(in - in)` when `in` is negative. Which for infinity, results in a\nNaN instead of 0.\n\ncc albanD mruberry jbschlosser walterddr\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66441\n\nReviewed By: bdhirsh\n\nDifferential Revision: D31619630\n\nPulled By: albanD\n\nfbshipit-source-id: e7867f3459a91e944b92f8ca42b6e0697b13f89b", "pr_number": "66441", "files_changed": ["aten/src/ATen/native/cpu/Activation.cpp", "aten/src/ATen/native/cuda/Activation.cu", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["module: nn", "open source", "Merged", "cla signed", "ciflow/default"]}, "e75de4f307": {"title": "remove a few unused THCTensor/Storage methods (#66555)", "body": "Summary:\nPer title\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66555\n\nReviewed By: mruberry\n\nDifferential Revision: D31620969\n\nPulled By: ngimel\n\nfbshipit-source-id: 1922ef523df473e8673a35c4a155b7b0cf000953", "pr_number": "66555", "files_changed": ["aten/src/THC/THCTensor.cpp", "aten/src/THC/THCTensor.hpp", "aten/src/THC/generic/THCStorage.cpp", "aten/src/THC/generic/THCStorage.h", "aten/src/THC/generic/THCTensor.cpp", "aten/src/THC/generic/THCTensor.h"], "labels": ["Merged", "cla signed", "ciflow/default"]}, "30d9fd9cf3": {"title": "Migrate USE_MAGMA config macro to ATen (#66390)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/66390\n\nTest Plan: Imported from OSS\n\nReviewed By: malfet, bdhirsh\n\nDifferential Revision: D31547712\n\nPulled By: ngimel\n\nfbshipit-source-id: 1b2ebc0d5b5d2199029274eabdd014f343cfbdd3", "pr_number": "66390", "files_changed": ["BUILD.bazel", "aten/src/ATen/CMakeLists.txt", "aten/src/ATen/cuda/CUDAConfig.h.in", "aten/src/ATen/cuda/detail/CUDAHooks.cpp", "aten/src/ATen/native/cuda/BatchLinearAlgebra.cpp", "aten/src/ATen/native/cuda/MiscUtils.h", "aten/src/THC/CMakeLists.txt", "aten/src/THC/THCGeneral.h", "aten/src/THC/THCGeneral.h.in", "aten/src/THC/THCTensorMathMagma.cpp"], "labels": ["module: porting", "open source", "Merged", "cla signed", "ciflow/default", "ciflow/cuda"]}, "160946e3f3": {"title": "Use `torch.empty()` instead of `torch.tensor()` in `torch.nn.Parameter` (#66486)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66486\n\nThe newly-introduced Python dispatcher mode (`__torch_dispatch__`) does not have support for `torch.tensor()` (see #64360) and this causes friction in the user experience if some `nn.Modules` use `torch.tensor()` either implicitly or explicitly.\n\nThis PR replaces calls to `torch.tensor()` in `Parameter`, `UninitializedParameter`, and `UninitializedBuffer` with an equivalent call to `torch.empty()` which serves the same purpose and is syntactically more readable.\nghstack-source-id: 140520931\n\nTest Plan: Since no behavioral change, run the existing unit and integration tests.\n\nReviewed By: pbelevich\n\nDifferential Revision: D31575587\n\nfbshipit-source-id: bd7bdeea54370f3e53dc13bd182b97d0f67146f5", "pr_number": "66486", "files_changed": ["torch/nn/parameter.py"], "labels": ["Merged", "cla signed", "ciflow/default"]}, "e1348973ac": {"title": "Add common_fx2trt.py (#66579)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66579\n\nDidn't commit this file in the PR that open sources fx2trt tests\n\nTest Plan: ci\n\nReviewed By: 842974287\n\nDifferential Revision: D31623354\n\nfbshipit-source-id: 6cedbe0f229da40499b83e6df28e16caca392d9c", "pr_number": "66579", "files_changed": ["torch/testing/_internal/common_fx2trt.py"], "labels": ["fb-exported", "Merged", "cla signed", "ciflow/default"]}, "fe41df3601": {"title": "Deprecate x.T on tensors of dimension other than 0 or 2 (#64180)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/64180\n\n**BC-breaking note:**\n\nThis PR deprecates the `Tensor.T` are not matrices. An upgrade guide is added to the\ndocumentation for `Tensor.T`.\n\nThis PR DOES NOT make this attribute to throw an error when called on a tensor of `dim != 2`,\nbut this will be its behavior in a future PyTorch release.\n\ncc mruberry rgommers pmeier asmeurer leofang AnirudhDagar asi1024 emcastillo kmaehashi heitorschueroff\n\nTest Plan: Imported from OSS\n\nReviewed By: bdhirsh\n\nDifferential Revision: D31610611\n\nPulled By: anjali411\n\nfbshipit-source-id: af8ff7e862790dda9f06921de005b3f6fd0803c3", "pr_number": "64180", "files_changed": ["aten/src/ATen/native/TensorShape.cpp", "torch/_tensor_docs.py"], "labels": ["open source", "module: deprecation", "Merged", "cla signed", "module: python array api", "ciflow/default"]}, "77f98ea5e0": {"title": "assert no duplicate yaml keys in codegen (#66238)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66238\n\nThe codegen should error if it sees two yaml entries with the same key. The default behavior of python's yaml loader is to overwrite duplicate keys with the new value.\n\nThis would have caught a nasty bug that showed up in https://github.com/pytorch/pytorch/pull/66225/files#r723796194.\n\nI tested it on that linked PR, to confirm that it errors correctly (and gives the line number containing the duplicate).\n\nTest Plan: Imported from OSS\n\nReviewed By: dagitses, albanD, sean-ngo\n\nDifferential Revision: D31464585\n\nPulled By: bdhirsh\n\nfbshipit-source-id: 5b35157ffa9a933bf4b344c4b9fe2878698370a3", "pr_number": "66238", "files_changed": ["tools/codegen/utils.py"], "labels": ["Merged", "cla signed", "ciflow/default"]}, "49a1d7bfcb": {"title": "[opinfo] elemwise parcel : isfinite, isinf, isposinf, isneginf, isnan, isreal (#66400)", "body": "Summary:\nAdds OpInfo for `isfinite, isinf, isposinf, isneginf, isnan, isreal`\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66400\n\nReviewed By: bdhirsh\n\nDifferential Revision: D31602998\n\nPulled By: mruberry\n\nfbshipit-source-id: 235cc414f373f014f4822a72deb1a04a58ad4a7c", "pr_number": "66400", "files_changed": ["test/test_jit_fuser_te.py", "test/test_unary_ufuncs.py", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["triaged", "open source", "Merged", "cla signed", "ciflow/default", "ciflow/all"]}, "76f3b07caf": {"title": "quantization docs: remove erroneous rebase artifact (#66577)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66577\n\nThere was a rebase artifact erroneously landed to quantization docs,\nthis PR removes it.\n\nTest Plan:\nCI\n\nImported from OSS\n\nReviewed By: soulitzer\n\nDifferential Revision: D31651350\n\nfbshipit-source-id: bc254cbb20724e49e1a0ec6eb6d89b28491f9f78", "pr_number": "66577", "files_changed": ["docs/source/quantization-support.rst"], "labels": ["Merged", "cla signed", "ciflow/default"]}, "833ede33ed": {"title": "Fix ubsan in concat_split_op.h (#66283)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66283\n\nFixes\n```\nUndefinedBehaviorSanitizer: nullptr-with-nonzero-offset caffe2/caffe2/operators/concat_split_op.h:185:52\n```\n\nTest Plan: Sandcastle\n\nReviewed By: swolchok\n\nDifferential Revision: D31486274\n\nfbshipit-source-id: 20128056f19cf814fdc3e6e144cf9208a4080d6a", "pr_number": "66283", "files_changed": ["caffe2/operators/concat_split_op.h"], "labels": ["fb-exported", "Merged", "cla signed", "ciflow/default"]}, "583217fe37": {"title": "changes for pytorch issue 55577 (#66571)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66571\n\nchanges for pytorch issue 55577\n\nTest Plan:\nRan test:\npython test/test_jit.py TestDict\n\nReviewed By: tugsbayasgalan\n\nDifferential Revision: D31622633\n\nfbshipit-source-id: 171c68a65b1d0bf769b3d95f103daba375e95335", "pr_number": "66571", "files_changed": ["test/jit/test_list_dict.py", "test/jit/test_module_containers.py", "test/jit/test_with.py", "torch/jit/frontend.py"], "labels": ["oncall: jit", "fb-exported", "Merged", "cla signed", "ciflow/default"]}, "6436bd3d5d": {"title": "Clarify topk doc (#65938)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/50331\n<img width=\"855\" alt=\"Screen Shot 2021-10-01 at 11 23 23 AM\" src=\"https://user-images.githubusercontent.com/17888388/136036611-f2bd9c77-61b4-4ab8-85eb-44f50c1e03d7.png\">\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65938\n\nReviewed By: bdhirsh\n\nDifferential Revision: D31314875\n\nPulled By: samdow\n\nfbshipit-source-id: bdd9425fd748710f8a64ed1989e1938dd358780f", "pr_number": "65938", "files_changed": ["torch/_torch_docs.py"], "labels": ["Merged", "cla signed", "ciflow/default"]}, "a58852fd44": {"title": "Fix fx2trt broken unit test (#66696)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66696\n\nD31511082 (https://github.com/pytorch/pytorch/commit/9918fd8305dd770b467d96d91b4533477c5628ed) moved unit test but didn't add proper target in build file, fix it in this diff.\n\nTest Plan: buck test mode/opt caffe2/test/fx2trt/converters/...\n\nReviewed By: 842974287\n\nDifferential Revision: D31667697\n\nfbshipit-source-id: 49e04afa323b27a1408c9bc2b5061b6529ced985", "pr_number": "66696", "files_changed": ["test/fx2trt/converters/acc_op/test_adaptive_avgpool.py", "test/fx2trt/converters/acc_op/test_avgpool.py", "test/fx2trt/converters/acc_op/test_batchnorm.py", "test/fx2trt/converters/acc_op/test_binary_ops.py", "test/fx2trt/converters/acc_op/test_cat.py", "test/fx2trt/converters/acc_op/test_clamp.py", "test/fx2trt/converters/acc_op/test_convolution.py", "test/fx2trt/converters/acc_op/test_dequantize.py", "test/fx2trt/converters/acc_op/test_flatten.py", "test/fx2trt/converters/acc_op/test_gelu.py", "test/fx2trt/converters/acc_op/test_getitem.py", "test/fx2trt/converters/acc_op/test_layer_norm.py", "test/fx2trt/converters/acc_op/test_linear.py", "test/fx2trt/converters/acc_op/test_matmul.py", "test/fx2trt/converters/acc_op/test_max.py", "test/fx2trt/converters/acc_op/test_maximum.py", "test/fx2trt/converters/acc_op/test_maxpool.py", "test/fx2trt/converters/acc_op/test_min.py", "test/fx2trt/converters/acc_op/test_minimum.py", "test/fx2trt/converters/acc_op/test_narrow.py", "test/fx2trt/converters/acc_op/test_permute.py", "test/fx2trt/converters/acc_op/test_quantize_per_tensor.py", "test/fx2trt/converters/acc_op/test_relu.py", "test/fx2trt/converters/acc_op/test_reshape.py", "test/fx2trt/converters/acc_op/test_sigmoid.py", "test/fx2trt/converters/acc_op/test_size.py", "test/fx2trt/converters/acc_op/test_softmax.py", "test/fx2trt/converters/acc_op/test_split.py", "test/fx2trt/converters/acc_op/test_squeeze.py", "test/fx2trt/converters/acc_op/test_sum.py", "test/fx2trt/converters/acc_op/test_tanh.py", "test/fx2trt/converters/acc_op/test_tile.py", "test/fx2trt/converters/acc_op/test_topk.py", "test/fx2trt/converters/acc_op/test_unary_ops.py", "test/fx2trt/converters/acc_op/test_unsqueeze.py", "test/fx2trt/converters/vanilla/test_add.py", "test/fx2trt/converters/vanilla/test_convolution.py"], "labels": ["fb-exported", "Merged", "cla signed", "ciflow/default", "fx"]}, "0b8dc0f04a": {"title": "add BFloat16 operators on CPU: logaddexp, logaddexp2, remainder (#63621)", "body": "Summary:\nFixes #{issue number}\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/63621\n\nReviewed By: H-Huang\n\nDifferential Revision: D31640811\n\nPulled By: mruberry\n\nfbshipit-source-id: 1fd061b65c196398738018eefc52bf459e424b1c", "pr_number": "63621", "files_changed": ["aten/src/ATen/native/cpu/BinaryOpsKernel.cpp", "test/test_binary_ufuncs.py", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["triaged", "open source", "Merged", "cla signed"]}, "bd25f92e81": {"title": "Fix Wextra issues in Half.h (#66643)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66643\n\nFixes:\n```\ncaffe2/c10/util/Half.h:456:14: error: comparison of integers of different signs: 'long' and 'unsigned long' [-Werror,-Wsign-compare]\n    return f > limit::max() ||\n           ~ ^ ~~~~~~~~~~~~\n```\n\nTest Plan: Sandcastle\n\nReviewed By: ngimel\n\nDifferential Revision: D31656816\n\nfbshipit-source-id: 7623d20e166a9e95a949ebd8b23793f24960cf07", "pr_number": "66643", "files_changed": ["c10/util/Half.h"], "labels": ["fb-exported", "Merged", "cla signed", "ciflow/default"]}, "b5b7d6a3a6": {"title": "EmbeddingBackward exclusive_scan thrust->cub (#66566)", "body": "Summary:\nFixes #{issue number}\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66566\n\nReviewed By: H-Huang\n\nDifferential Revision: D31637660\n\nPulled By: ngimel\n\nfbshipit-source-id: 8093432bb9a9b902bb6bab7da221f0bcd7e9fb34", "pr_number": "66566", "files_changed": ["aten/src/ATen/cuda/cub.cuh", "aten/src/ATen/native/cuda/Embedding.cu", "aten/src/ATen/native/cuda/EmbeddingBackwardKernel.cu", "aten/src/ATen/native/cuda/EmbeddingBag.cu", "aten/src/ATen/native/cuda/Indexing.cu", "aten/src/ATen/native/cuda/LegacyThrustHelpers.cu", "aten/src/ATen/native/cuda/Randperm.cu", "aten/src/ATen/native/cuda/Sort.cu", "aten/src/ATen/native/cuda/UniqueCub.cu"], "labels": ["triaged", "open source", "Merged", "cla signed", "ciflow/default"]}, "a25648953c": {"title": "Add `warn_only` kwarg to `use_deterministic_algorithms` (#66233)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/64883\n\nAdds a `warn_only` kwarg to `use_deterministic_algorithms`. When enabled, calling an operation that does not have a deterministic implementation will raise a warning, rather than an error.\n\n`torch.testing._internal.common_device_type.expectedAlertNondeterministic` is also refactored and documented in this PR to make it easier to use and understand.\n\ncc mruberry kurtamohler\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66233\n\nReviewed By: bdhirsh\n\nDifferential Revision: D31616481\n\nPulled By: mruberry\n\nfbshipit-source-id: 059634a82d54407492b1d8df08f059c758d0a420", "pr_number": "66233", "files_changed": ["aten/src/ATen/Context.cpp", "aten/src/ATen/Context.h", "test/test_torch.py", "torch/_C/__init__.pyi.in", "torch/__init__.py", "torch/csrc/Module.cpp", "torch/testing/_internal/common_device_type.py", "torch/testing/_internal/common_utils.py"], "labels": ["triaged", "module: determinism", "open source", "Merged", "cla signed", "ciflow/default"]}, "d1b6121935": {"title": "Revert D31656999: Add meta support to tensor range factories", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD31656999 (https://github.com/pytorch/pytorch/commit/7400f34b8e3692384ab19de0914271e01cb3fd88)\n\nOriginal commit changeset: 06e7f3655b94\n\nfbshipit-source-id: 2f9d8d1acbb01c5105ece73472e5c1f5f90886ee", "pr_number": null, "files_changed": ["aten/src/ATen/native/Histogram.cpp", "aten/src/ATen/native/RangeFactories.cpp", "aten/src/ATen/native/native_functions.yaml", "test/test_tensor_creation_ops.py"], "labels": []}, "06cfdfae0e": {"title": "Promote integral inputs to floating for `torch.logsumexp` (#63393)", "body": "Summary:\nFixed https://github.com/pytorch/pytorch/issues/56132, Integral inputs of `torch.logsumexp` would be promoted to the floating point type.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/63393\n\nReviewed By: ezyang\n\nDifferential Revision: D30512180\n\nPulled By: mruberry\n\nfbshipit-source-id: fbde3605c15b930411d0d1eb3a132b0088187097", "pr_number": "63393", "files_changed": ["aten/src/ATen/native/ReduceOps.cpp", "test/test_reductions.py", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["triaged", "open source", "Merged", "cla signed", "ciflow/default"]}, "1e2b2ee5ff": {"title": "sort_out_cuda: Use custom kernels to fill index tensors (#66668)", "body": "Summary:\nThese stable sorts currently use a combination of `at::arange`, view ops and `tensor.copy_` to fill in the initial values for the indices before calling into `CUB` to do the actual sort. This is somewhat inefficient because it requires 2 to 4 kernel launches, and the copies all use strided kernels instead of the more efficient contiguous kernels. Instead, a fairly straight-forward custom kernel is more efficient in terms of both CUDA and CPU runtime.\n\nIn a simple benchmark I profiled `a.sort(stable=True, dim=1)` for different shapes and single out the kernel invocations for intitializing the index tensors (i.e. the non-`cub` kernels). Note that when the batch dim is `<128` we call `segmented_sort_pairs_by_full_sort` instead of `segmented_sort_pairs`:\n\n| shape        | Master (us) | This PR (us) |\n|--------------|:-----------:|:------------:|\n| (100, 1000)  |    5.000    |     2.300    |\n| (1000, 100)  |    2.070    |     1.090    |\n| (100, 10000) |    87.34    |     26.47    |\n| (1000, 1000) |    28.63    |     20.27    |\n\nOf course for sufficiently large inputs, the overall runtime is dominated by the actual sort. But I have another motive of wanting to remove operator the calls from the middle of this kernel launch code. This change makes it easier to split the kernel code that needs to be compiled with `nvcc` into it's own file that doesn't include `Tensor.h`, similar to what I'm doing in https://github.com/pytorch/pytorch/issues/66620.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66668\n\nReviewed By: H-Huang\n\nDifferential Revision: D31693722\n\nPulled By: ngimel\n\nfbshipit-source-id: 5765926e4dbbc7a20d2940c098ed093b3de2204e", "pr_number": "66668", "files_changed": ["aten/src/ATen/native/cuda/Sort.cu"], "labels": ["triaged", "open source", "Merged", "cla signed", "ciflow/default"]}, "32ac001e4d": {"title": "Suppress deprecated copy in vec256_qint.h (#66646)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/66646\n\nTest Plan: Sandcastle\n\nReviewed By: ngimel\n\nDifferential Revision: D31660387\n\nfbshipit-source-id: a1ea9702a8b33f78a7201a1d9214065c2fb930b1", "pr_number": "66646", "files_changed": ["aten/src/ATen/cpu/vec/vec256/vec256_qint.h"], "labels": ["fb-exported", "Merged", "cla signed", "ciflow/default"]}, "8c5928bd78": {"title": "add frozen_numpy as a builtin library to torch::deploy (#66297)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66297\n\nLink register_numpy.cpp with the embedded interpreter will register numpy as a builtin library.\n\nTest Plan: Add unit test to test basic numpy functionality in torch::deploy like creating random matrices, matric multiplication.\n\nReviewed By: suo\n\nDifferential Revision: D31490434\n\nfbshipit-source-id: b052ce01fc64fb0efee846feb0acc1f107ba13e0", "pr_number": "66297", "files_changed": ["torch/csrc/deploy/interpreter/register_numpy.cpp", "torch/csrc/deploy/test_deploy.cpp"], "labels": ["Merged", "cla signed", "ciflow/default"]}, "8854817f44": {"title": "Implement Python Array API `asarray` function. (#60627)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/60627\n\nIn this PR, the core of `frombuffer` and `fromDLPack` onto _tensor_new.cpp_. `asarray`\nuses such refactored functions for interpreting the object as a tensor. We follow the\nPython Array API standard found:\n\nhttps://data-apis.org/array-api/latest/API_specification/creation_functions.html?highlight=asarray\n\nTest Plan: Imported from OSS\n\nReviewed By: H-Huang\n\nDifferential Revision: D31640510\n\nPulled By: mruberry\n\nfbshipit-source-id: d0869e0d73cb50023d5866b001dac5d34ca30dfd", "pr_number": "60627", "files_changed": ["docs/source/torch.rst", "test/test_buffer_protocol.py", "test/test_tensor_creation_ops.py", "tools/pyi/gen_pyi.py", "torch/_torch_docs.py", "torch/csrc/Module.cpp", "torch/csrc/autograd/python_torch_functions_manual.cpp", "torch/csrc/utils/tensor_new.cpp", "torch/csrc/utils/tensor_new.h", "torch/csrc/utils/tensor_numpy.cpp", "torch/csrc/utils/tensor_numpy.h", "torch/overrides.py"], "labels": ["open source", "Merged", "cla signed", "ciflow/default"]}, "719d43a2a2": {"title": "Revert D31547709: Remove THCGeneral.cpp", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD31547709 (https://github.com/pytorch/pytorch/commit/aa0c31876bbd0f9813ec6f000154a24c91beece9)\n\nOriginal commit changeset: 059c47621863\n\nfbshipit-source-id: e8c3597f2badbc5ecf356b381edea06a07331f24", "pr_number": null, "files_changed": ["BUILD.bazel", "aten/src/ATen/cuda/detail/CUDAHooks.cpp", "aten/src/ATen/native/cuda/Dropout.cu", "aten/src/ATen/native/cuda/TensorFactories.cu", "aten/src/ATen/native/cuda/Unique.cu", "aten/src/ATen/native/cudnn/Conv_v7.cpp", "aten/src/THC/CMakeLists.txt", "aten/src/THC/THCGeneral.cpp", "aten/src/THC/THCGeneral.h", "aten/src/THC/THCStorage.cpp", "torch/csrc/cuda/Stream.cpp", "torch/csrc/generic/StorageSharing.cpp", "torch/csrc/generic/serialization.cpp"], "labels": []}, "3b4cb9ddca": {"title": "Revert D31577488: Migrate THCState to ATen", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD31577488 (https://github.com/pytorch/pytorch/commit/65adf1dfa2905f9a8397a55b30c748e1707c848b)\n\nOriginal commit changeset: 90604f30854f\n\nfbshipit-source-id: 3d7e35b3d6ea94f2c999bcf821b33a9cf1db01ee", "pr_number": null, "files_changed": ["aten/src/ATen/cuda/PeerToPeerAccess.cpp", "aten/src/ATen/cuda/PeerToPeerAccess.h", "aten/src/ATen/cuda/detail/CUDAHooks.cpp", "aten/src/ATen/cuda/detail/CUDAHooks.h", "aten/src/ATen/native/cuda/Copy.cu", "aten/src/ATen/native/cuda/Resize.cpp", "aten/src/THC/CMakeLists.txt", "aten/src/THC/THCGeneral.cpp", "aten/src/THC/THCGeneral.h", "aten/src/THC/THCGeneral.hpp", "aten/src/THC/THCTensorMathMagma.cpp"], "labels": []}, "53aac4b6f3": {"title": "[PyTorch] Allow override for macro `HAS_DEMANGLE` (#66540)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66540\n\nCurrently the macro `HAS_DEMANGLE` is determined by compiler predefined macros. Here I'm adding an option to allow `HAS_DEMANGLE` to be defined in build files.\n\nTest Plan: Rely on CI\n\nReviewed By: poweic\n\nDifferential Revision: D31600007\n\nfbshipit-source-id: 76cf088b0f5ee940e977d3b213f1446ea64be036", "pr_number": "66540", "files_changed": ["c10/macros/Macros.h"], "labels": ["oncall: jit", "fb-exported", "Merged", "cla signed", "ciflow/default"]}, "f4a7273b5c": {"title": "Set test owners for module: ci (#66796)", "body": "Summary:\nAction based on RFC https://github.com/pytorch/pytorch/issues/66232\n\ncc seemethere malfet pytorch/pytorch-dev-infra\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66796\n\nReviewed By: seemethere\n\nDifferential Revision: D31732391\n\nPulled By: janeyx99\n\nfbshipit-source-id: b894eab8a4a8737165d1ba7b536e1232f6c07a8f", "pr_number": "66796", "files_changed": ["test/test_determination.py", "test/test_import_stats.py"], "labels": ["module: ci", "Merged", "cla signed", "ciflow/default"]}, "62e89f692f": {"title": "[doc] typo (#66754)", "body": "Summary:\nThis PR fixes a typo in the `torch/autograd/function.py` doc\n\n-----------------------\n\nAdditionally, the example at https://pytorch.org/docs/master/autograd.html#torch.autograd.Function doesn't quite compile:\n```\n'builtin_function_or_method' object has no attribute 'exp'\n```\neven though `i.exp()` is a valid function if `i` is a tensor.\n\nI changed it to:\n```\nresult = torch.exp(i)\n```\nbut python doesn't like it either:\n```\nTypeError: exp(): argument 'input' (position 1) must be Tensor, not builtin_function_or_method\n```\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66754\n\nReviewed By: albanD\n\nDifferential Revision: D31729400\n\nPulled By: soulitzer\n\nfbshipit-source-id: eef783bcdc8d4693a8b7f1ab581e948abc0f9b94", "pr_number": "66754", "files_changed": ["torch/autograd/function.py"], "labels": ["open source", "Merged", "cla signed", "ciflow/default"]}, "09c4e73c95": {"title": "[PyTorch] Fix unnecessary shared_ptr copies in FutureType (#66704)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66704\n\nMissing moves in the construction path.\nghstack-source-id: 140746391\n\nTest Plan: CI\n\nReviewed By: suo\n\nDifferential Revision: D31694296\n\nfbshipit-source-id: 3bed477c811069248611efdb57ad27c6ca233442", "pr_number": "66704", "files_changed": ["aten/src/ATen/core/jit_type.h"], "labels": ["Merged", "cla signed", "ciflow/default"]}, "eb1eefc399": {"title": "[PyTorch] Fix unnecessary shared_ptr copies in DictType (#66702)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66702\n\nMissing moves in the construction path and forced copies of the key & value type on access.\nghstack-source-id: 140744707\n\nTest Plan: CI\n\nReviewed By: suo\n\nDifferential Revision: D31693818\n\nfbshipit-source-id: 4c5d2359f58148744621abe81429e56e7889f754", "pr_number": "66702", "files_changed": ["aten/src/ATen/core/jit_type.h"], "labels": ["Merged", "cla signed", "ciflow/default"]}, "c373e188d8": {"title": "[PyTorch] Fix extra refcount bumps in unifyTypes (#66718)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66718\n\nSome missing moves and use of cast instead of castRaw (due to a previous automated fixup only being a partial fix).\nghstack-source-id: 140755229\n\nTest Plan: CI\n\nReviewed By: suo\n\nDifferential Revision: D31697115\n\nfbshipit-source-id: 86743f8982951a58638ba244b3a92d3737dde58b", "pr_number": "66718", "files_changed": ["aten/src/ATen/core/type.cpp"], "labels": ["Merged", "cla signed", "ciflow/default"]}, "6bde474066": {"title": "[PyTorch] Fix extra refcount bumps in matchTypeVariables (#66719)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66719\n\nSome cast that could be castRaw. Parameters did not need to force a refcount bump.\nghstack-source-id: 140756356\n\nTest Plan: CI\n\nReviewed By: suo\n\nDifferential Revision: D31697455\n\nfbshipit-source-id: 87a8cba221a7ae53f2a485acafd31622e9328ff0", "pr_number": "66719", "files_changed": ["aten/src/ATen/core/jit_type.h", "aten/src/ATen/core/type.cpp"], "labels": ["Merged", "cla signed", "ciflow/default"]}, "7fad47e522": {"title": "`torch.linalg.lstsq`: forward/backward AD support (#65054)", "body": "Summary:\nAs per title.\n\ncc ezyang albanD zou3519 gqchen pearu nikitaved soulitzer Lezcano Varal7 jianyuh mruberry walterddr IvanYashchuk xwang233\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65054\n\nReviewed By: zou3519\n\nDifferential Revision: D31729468\n\nPulled By: albanD\n\nfbshipit-source-id: ab7df824bc80128e7f64f6444c7a4baa4786c161", "pr_number": "65054", "files_changed": ["tools/autograd/derivatives.yaml", "tools/autograd/gen_variable_type.py", "torch/csrc/autograd/FunctionsManual.cpp", "torch/csrc/autograd/FunctionsManual.h", "torch/testing/_internal/common_device_type.py", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["module: autograd", "triaged", "open source", "module: linear algebra", "complex_autograd", "Merged", "cla signed", "ciflow/default"]}, "d5a25faf7a": {"title": "[PyTorch] Fix unnecessary shared_ptr copies in EnumType (#66714)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66714\n\nForced copy in getValueType and unnecessary use of cast over castRaw.\nghstack-source-id: 140752791\n\nTest Plan: CI\n\nReviewed By: suo\n\nDifferential Revision: D31696164\n\nfbshipit-source-id: fc2316617a61ca32f1fb952fb0af18b8784a606b", "pr_number": "66714", "files_changed": ["aten/src/ATen/core/jit_type.h"], "labels": ["Merged", "cla signed", "ciflow/default"]}, "393299b124": {"title": "[PyTorch] Fix unnecessary shared_ptr copies in RRefType (#66706)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66706\n\nMissing moves in the construction path.\nghstack-source-id: 140746585\n\nTest Plan: CI\n\nReviewed By: suo\n\nDifferential Revision: D31694356\n\nfbshipit-source-id: 8e2bf2dd41f3f65fc06e30ffd5fddd487d01aaa8", "pr_number": "66706", "files_changed": ["aten/src/ATen/core/jit_type.h"], "labels": ["Merged", "cla signed", "ciflow/default"]}, "1fcbd8fa15": {"title": "[PyTorch] Fix extra refcount bumps in tryEvalTypeVariables (#66722)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66722\n\nMissing move, s/cast/castRaw/, and take TypePtr arg by const ref because we only sometimes need to take ownership.\nghstack-source-id: 140757141\n\nTest Plan: CI\n\nReviewed By: suo\n\nDifferential Revision: D31697631\n\nfbshipit-source-id: 04afe13688c6e2aaf79157400c0a44021cb8179d", "pr_number": "66722", "files_changed": ["aten/src/ATen/core/jit_type.h", "aten/src/ATen/core/type.cpp"], "labels": ["Merged", "cla signed", "ciflow/default"]}, "8637556d23": {"title": "Migrate THCState to ATen (#66765)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66765\n\nThis guts `THCState` to simply be an empty struct, as well as:\n- moving `THCState_getPeerToPeerAccess` and its cache into `ATen`.\n- cleaning up dead code in `THCGeneral.cpp`\n- moving `THCudaInit` and `THCMagma_init` into `CUDAHooks::initCUDA`\n\nTest Plan: Imported from OSS\n\nReviewed By: zou3519\n\nDifferential Revision: D31721648\n\nPulled By: ngimel\n\nfbshipit-source-id: 772b24787656a95f9e3fcb287d912b1c3400f32d", "pr_number": "66765", "files_changed": ["aten/src/ATen/cuda/PeerToPeerAccess.cpp", "aten/src/ATen/cuda/PeerToPeerAccess.h", "aten/src/ATen/cuda/detail/CUDAHooks.cpp", "aten/src/ATen/cuda/detail/CUDAHooks.h", "aten/src/ATen/native/cuda/BatchLinearAlgebra.cpp", "aten/src/ATen/native/cuda/Copy.cu", "aten/src/ATen/native/cuda/Resize.cpp", "aten/src/THC/CMakeLists.txt", "aten/src/THC/THCGeneral.cpp", "aten/src/THC/THCGeneral.h", "aten/src/THC/THCGeneral.hpp", "aten/src/THC/THCTensorMathMagma.cpp"], "labels": ["open source", "Merged", "cla signed", "ciflow/default", "ciflow/cuda"]}, "6a7296be9c": {"title": "[PyTorch] Use castRaw in InterfaceType (#66728)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66728\n\nTwo extra refcount bumps.\nghstack-source-id: 140760872\n\nTest Plan: CI\n\nReviewed By: suo\n\nDifferential Revision: D31698577\n\nfbshipit-source-id: 1f50195a99f98f857abc9b03b4254519c316fefe", "pr_number": "66728", "files_changed": ["aten/src/ATen/core/jit_type.h", "aten/src/ATen/core/type.cpp"], "labels": ["Merged", "cla signed", "ciflow/default"]}, "44fd312604": {"title": "[PyTorch] Use intrusive_ptr to save space in KernelFunction (#65618)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65618\n\nThis saves 8 bytes per KernelFunction, which should help in resource-constrained environments.\nghstack-source-id: 140731069\n\nTest Plan: CI\n\nReviewed By: ezyang\n\nDifferential Revision: D25405736\n\nfbshipit-source-id: 757c0f1387da9147e46ac69af2aa9fffd2998e35", "pr_number": "65618", "files_changed": ["aten/src/ATen/core/boxing/KernelFunction.h", "aten/src/ATen/core/boxing/KernelFunction_impl.h", "aten/src/ATen/core/boxing/impl/make_boxed_from_unboxed_functor.h", "c10/util/intrusive_ptr.h"], "labels": ["Merged", "cla signed", "ciflow/default"]}, "d0a63c978b": {"title": "[PyTorch][easy] Don't copy string in TensorType::repr_str unnecessarily (#66699)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66699\n\nstd::string::operator+ will copy the string an extra time even if the argument is `\"\"`. See https://godbolt.org/z/3sM5h1qTo\nghstack-source-id: 140743822\n\nTest Plan: CI\n\nReviewed By: suo\n\nDifferential Revision: D31693522\n\nfbshipit-source-id: 6a8033c90366904b9aff44214b600cfb255a0809", "pr_number": "66699", "files_changed": ["aten/src/ATen/core/jit_type.h"], "labels": ["Merged", "cla signed", "ciflow/default"]}, "c9c447f4be": {"title": "[PyTorch] Fix missing moves in ListType (#66701)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66701\n\nWe own the argument vector.\nghstack-source-id: 140760983\n\nTest Plan: CI\n\nReviewed By: suo\n\nDifferential Revision: D31693645\n\nfbshipit-source-id: 02829bc3c728f6d1d07be08b0d977eee1efee38f", "pr_number": "66701", "files_changed": ["aten/src/ATen/core/jit_type.h"], "labels": ["Merged", "cla signed", "ciflow/default"]}, "a17a4e93ce": {"title": "[PyTorch][easy] Fix missing move in UnionType::createWithContained (#66691)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66691\n\nDoes what it says on the tin.\nghstack-source-id: 140736047\n\nTest Plan: CI\n\nReviewed By: suo\n\nDifferential Revision: D31691627\n\nfbshipit-source-id: 21a5d0248bf3412f5af36260597a5f663ab34361", "pr_number": "66691", "files_changed": ["aten/src/ATen/core/jit_type.h"], "labels": ["cla signed", "ciflow/default"]}, "d05c1ec007": {"title": "Add lazy Node base and associated infra (#66601)", "body": "Summary:\n- Adds Node base class and unit tests\n- Also adds metadata utils to enable source code annotation and scope tracking\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66601\n\nTest Plan: Add new unit tests\n\nReviewed By: desertfire\n\nDifferential Revision: D31634044\n\nfbshipit-source-id: a042d54f06fbc480acfc63c18d43cb6fceb6fea5", "pr_number": "66601", "files_changed": ["test/cpp/lazy/CMakeLists.txt", "test/cpp/lazy/test_ir.cpp", "tools/build_variables.bzl", "torch/csrc/lazy/core/config.cpp", "torch/csrc/lazy/core/config.h", "torch/csrc/lazy/core/hash.cpp", "torch/csrc/lazy/core/hash.h", "torch/csrc/lazy/core/ir.cpp", "torch/csrc/lazy/core/ir.h", "torch/csrc/lazy/core/ir_metadata.cpp", "torch/csrc/lazy/core/ir_metadata.h"], "labels": ["fb-exported", "Merged", "cla signed", "ciflow/default", "ciflow/all"]}, "08a464a9f3": {"title": "[PyTorch] Pass c10::optional<bool> to Stride ctor by value (#66698)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66698\n\nthis type should fit in a register; no need to pass by reference.\nghstack-source-id: 140742830\n\nTest Plan: CI\n\nReviewed By: suo\n\nDifferential Revision: D31693291\n\nfbshipit-source-id: 299fb3d1830a059b59268487c22e030446c3496e", "pr_number": "66698", "files_changed": ["aten/src/ATen/core/jit_type.h"], "labels": ["cla signed", "ciflow/default"]}, "8f09292c5e": {"title": "add `OpInfo` for `torch.nn.functional.pairwise_distance` (#65460)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65460\n\ncc albanD mruberry jbschlosser walterddr\n\nTest Plan: Imported from OSS\n\nReviewed By: gchanan\n\nDifferential Revision: D31111701\n\nPulled By: zou3519\n\nfbshipit-source-id: a4034418cf8d14f584134a16d822181703858f99", "pr_number": "65460", "files_changed": ["torch/testing/_internal/common_methods_invocations.py"], "labels": ["module: nn", "module: tests", "open source", "cla signed", "ciflow/default"]}, "1164118fc2": {"title": "add `OpInfo` for `torch.nn.pixel_shuffle` (#65467)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/65467\n\nTest Plan: Imported from OSS\n\nReviewed By: mrshenli\n\nDifferential Revision: D31111697\n\nPulled By: zou3519\n\nfbshipit-source-id: 618e6b2cc927814f85500374a2838d98c9c45d6e", "pr_number": "65467", "files_changed": ["torch/testing/_internal/common_methods_invocations.py"], "labels": ["module: nn", "module: tests", "open source", "cla signed", "ciflow/default"]}, "9f782f8b35": {"title": "add `OpInfo` for `torch.nn.pixel_unshuffle` (#65468)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/65468\n\nTest Plan: Imported from OSS\n\nReviewed By: mrshenli\n\nDifferential Revision: D31111699\n\nPulled By: zou3519\n\nfbshipit-source-id: a92c2f1f4986a54abab82360e97ea2ce22fb9397", "pr_number": "65468", "files_changed": ["torch/testing/_internal/common_methods_invocations.py"], "labels": ["module: nn", "module: tests", "open source", "cla signed", "ciflow/slow-gradcheck", "ciflow/default", "ciflow/all"]}, "05b6dc9d75": {"title": "Fix BatchMatMul test and shape inference (#66733)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66733\n\nFix the test for BatchMatMul to compare glow/caffe2 outputs and fix its shape inference function since it made simplifying assumptions for broadcasting and failed on some of the shapes in the test. The previous inference was failing for any cases where the first n - 2 output dimensions of A x B was not simply that of whichever one of A or B had higher rank (ex. A: [2, 2, 2, 3, 4], B: [3, 1, 2, 2, 4, 5] we expect output dimensions [3, 2, 2, 2, 3, 5] rather than [3, 1, 2, 2, 3, 5].\n\nTest Plan:\n```\nbuck test glow/fb/test/numerics:test_operator_onnxifinnpi -- -r .*test_batch_matmul_manydims.* --env USE_INF_API=1\n```\n\nReviewed By: khabinov\n\nDifferential Revision: D31701184\n\nfbshipit-source-id: 31d0fb17409a399b90fb8042385e000ed81c3581", "pr_number": "66733", "files_changed": ["caffe2/operators/batch_matmul_op.cc"], "labels": ["fb-exported", "Merged", "cla signed", "ciflow/default"]}, "299a6a65b2": {"title": "[skip ci] Set test owners for autograd tests (#66834)", "body": "Summary:\nAction following https://github.com/pytorch/pytorch/issues/66232\n\ncc ezyang albanD zou3519 gqchen pearu nikitaved soulitzer Lezcano Varal7\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66834\n\nReviewed By: albanD\n\nDifferential Revision: D31761778\n\nPulled By: janeyx99\n\nfbshipit-source-id: 355edfb1b940154e84fbba6f7b096605e75ae459", "pr_number": "66834", "files_changed": ["test/autograd/test_complex.py", "test/test_autograd.py", "test/test_public_bindings.py"], "labels": ["module: autograd", "cla signed", "ciflow/default"]}, "c806bb1022": {"title": "[skip ci] Set test owner for test_complex.py (#66835)", "body": "Summary:\nAction following https://github.com/pytorch/pytorch/issues/66232\n\ncc ezyang anjali411 dylanbespalko mruberry Lezcano nikitaved\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66835\n\nReviewed By: anjali411\n\nDifferential Revision: D31761723\n\nPulled By: janeyx99\n\nfbshipit-source-id: ca672f5a1be9dc27284fade725a8238cbfd877a3", "pr_number": "66835", "files_changed": ["test/test_complex.py"], "labels": ["module: complex", "cla signed", "ciflow/default"]}, "fd608cd313": {"title": "[skip ci] Set test owners for optim tests (#66861)", "body": "Summary:\nAction following https://github.com/pytorch/pytorch/issues/66232\n\ncc vincentqb jbschlosser albanD\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66861\n\nReviewed By: albanD\n\nDifferential Revision: D31761369\n\nPulled By: janeyx99\n\nfbshipit-source-id: 57829e1f1509fc2af321530a4b55c9d33b7fb150", "pr_number": "66861", "files_changed": ["test/test_optim.py"], "labels": ["module: optimizer", "cla signed", "ciflow/default"]}, "17f07c310b": {"title": "Fix type checking errors in torch/ao/quantization/quantize_fx.py (#66804)", "body": "Summary:\n- [x] Fix the Pyre type checking errors in `torch/ao/quantization/quantize_fx.py`\n```\ntorch/quantization/quantize_fx.py:41:8 Incompatible variable type [9]: fuse_custom_config_dict is declared to have type `Dict[str, typing.Any]` but is used as type `None`.\ntorch/quantization/quantize_fx.py:143:16 Incompatible variable type [9]: prepare_custom_config_dict is declared to have type `Dict[str, typing.Any]` but is used as type `None`.\ntorch/quantization/quantize_fx.py:144:16 Incompatible variable type [9]: equalization_qconfig_dict is declared to have type `Dict[str, typing.Any]` but is used as type `None`.\ntorch/quantization/quantize_fx.py:206:8 Incompatible variable type [9]: prepare_custom_config_dict is declared to have type `Dict[str, typing.Any]` but is used as type `None`.\ntorch/quantization/quantize_fx.py:230:12 Incompatible variable type [9]: fuse_custom_config_dict is declared to have type `Dict[str, typing.Any]` but is used as type `None`.\ntorch/quantization/quantize_fx.py:268:8 Incompatible variable type [9]: prepare_custom_config_dict is declared to have type `Dict[str, typing.Any]` but is used as type `None`.\ntorch/quantization/quantize_fx.py:269:8 Incompatible variable type [9]: equalization_qconfig_dict is declared to have type `Dict[str, typing.Any]` but is used as type `None`.\ntorch/quantization/quantize_fx.py:427:8 Incompatible variable type [9]: prepare_custom_config_dict is declared to have type `Dict[str, typing.Any]` but is used as type `None`.\ntorch/quantization/quantize_fx.py:464:8 Incompatible variable type [9]: convert_custom_config_dict is declared to have type `Dict[str, typing.Any]` but is used as type `None`.\ntorch/quantization/quantize_fx.py:486:8 Incompatible variable type [9]: convert_custom_config_dict is declared to have type `Dict[str, typing.Any]` but is used as type `None`.\ntorch/quantization/quantize_fx.py:547:8 Incompatible variable type [9]: convert_custom_config_dict is declared to have type `Dict[str, typing.Any]` but is used as type `None`.\n```\nFixes the issue: [MLH-Fellowship/pyre-check/issues/76](https://github.com/MLH-Fellowship/pyre-check/issues/76)\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66804\n\nReviewed By: onionymous\n\nDifferential Revision: D31738171\n\nPulled By: 0xedward\n\nfbshipit-source-id: 00d4c5749c469aff39a1531365461ced747e52fc", "pr_number": "66804", "files_changed": ["torch/ao/quantization/quantize_fx.py"], "labels": ["triaged", "open source", "cla signed", "ciflow/default"]}, "94afbd158c": {"title": "[skip ci] Set test owner for test_numpy_interop.py (#66851)", "body": "Summary:\nAction following https://github.com/pytorch/pytorch/issues/66232\n\ncc mruberry rgommers\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66851\n\nReviewed By: gchanan\n\nDifferential Revision: D31761703\n\nPulled By: janeyx99\n\nfbshipit-source-id: 4dec507dff0ce25d2780b6020f0d9790ab1cb499", "pr_number": "66851", "files_changed": ["test/test_numpy_interop.py"], "labels": ["module: numpy", "Merged", "cla signed", "ciflow/default"]}, "c9d9244166": {"title": "[skip ci] Set test owner for test_spectral_ops.py (#66843)", "body": "Summary:\nAction following https://github.com/pytorch/pytorch/issues/66232\n\ncc mruberry peterbell10\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66843\n\nReviewed By: gchanan\n\nDifferential Revision: D31761715\n\nPulled By: janeyx99\n\nfbshipit-source-id: 1173a200478b87568768fafcfee117c09c1cffbd", "pr_number": "66843", "files_changed": ["test/test_spectral_ops.py"], "labels": ["module: fft", "Merged", "cla signed", "ciflow/default"]}, "50f5689d60": {"title": "Set test owner for distributions tests (#66842)", "body": "Summary:\nAction following https://github.com/pytorch/pytorch/issues/66232\n\ncc fritzo neerajprad alicanb nikitaved\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66842\n\nReviewed By: neerajprad\n\nDifferential Revision: D31761720\n\nPulled By: janeyx99\n\nfbshipit-source-id: 9d9e88d93e2efb90c971f165b4040880e9d90c56", "pr_number": "66842", "files_changed": ["test/distributions/test_constraints.py", "test/distributions/test_distributions.py", "test/distributions/test_transforms.py", "test/distributions/test_utils.py"], "labels": ["module: distributions", "Merged", "cla signed", "ciflow/default"]}, "7e81a89e13": {"title": "[PyTorch] Fix performance-no-automatic-move clang tidy warnings in matchTypeVariables (#66720)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66720\n\nSee the documentation for the warning. https://clang.llvm.org/extra/clang-tidy/checks/performance-no-automatic-move.html\nghstack-source-id: 140922952\n\nTest Plan: CI\n\nReviewed By: suo\n\nDifferential Revision: D31697506\n\nfbshipit-source-id: 26ce6c47d0f3b0c4e48ecc882f6792f1b5a45bac", "pr_number": "66720", "files_changed": ["aten/src/ATen/core/type.cpp"], "labels": ["Merged", "cla signed", "ciflow/default"]}, "552af8bdef": {"title": "[PyTorch] Fix missing move in OptionalType::createWithContained (#66697)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66697\n\nWe own this vector, so we can move from it.\nghstack-source-id: 140742640\n\nTest Plan: CI\n\nReviewed By: suo\n\nDifferential Revision: D31693230\n\nfbshipit-source-id: 3f33ca6e47e29b0e3d6c8fad59c234c55e1e159f", "pr_number": "66697", "files_changed": ["aten/src/ATen/core/jit_type.h"], "labels": ["cla signed", "ciflow/default"]}, "690c2a7076": {"title": "masked_scatter: fuse mask count check into one kernel (#66871)", "body": "Summary:\nThis saves 1 kernel launch, 7 dispatcher calls, 3 `TensorImpl` allocations and 1 CUDA memory allocation.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66871\n\nReviewed By: gchanan\n\nDifferential Revision: D31763713\n\nPulled By: ngimel\n\nfbshipit-source-id: b0d2f9415b7fd013fb4e7d68ade6e38a58f5b153", "pr_number": "66871", "files_changed": ["aten/src/ATen/native/cuda/IndexKernel.cu"], "labels": ["open source", "cla signed", "ciflow/default"]}, "3488a85a76": {"title": "Sparse CSR CUDA: fix input checks for `addmm` and `mm` (#66485)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66485\n\nThe errors for incorrectly sized inputs should match the dense variants\nof functions.\nMoved addmm_out_sparse_csr_dense_cuda from SparseCsrTensorMath.cu and\nremoved unnecessary device check.\n\ncc nikitaved pearu cpuhrsch IvanYashchuk\n\nTest Plan: Imported from OSS\n\nReviewed By: jbschlosser\n\nDifferential Revision: D31764036\n\nPulled By: cpuhrsch\n\nfbshipit-source-id: 76900fe9e4a49474695a01f34bad41cb3422321c", "pr_number": "66485", "files_changed": ["aten/src/ATen/native/sparse/cuda/SparseBlas.cpp", "aten/src/ATen/native/sparse/cuda/SparseCsrTensorMath.cu", "test/test_sparse_csr.py"], "labels": ["module: sparse", "open source", "cla signed", "ciflow/default", "ciflow/cuda"]}, "57c596eb9e": {"title": "add interactive_embedded_interpreter.cpp to the OSS build (#66352)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66352\n\nAdd cmake rules for interactive_embedded_interpreter.cpp .\n\nThe builtin_registry.cpp has already been handled in https://github.com/pytorch/pytorch/pull/66347 . I'll remove the change in this PR once that one is merged.\n\nTest Plan: Imported from OSS\n\nReviewed By: suo\n\nDifferential Revision: D31521249\n\nPulled By: shunting314\n\nfbshipit-source-id: bb9d340e5a6aad7d76078ca03a82b5ae7494a124", "pr_number": "66352", "files_changed": ["torch/csrc/deploy/CMakeLists.txt", "torch/csrc/deploy/interactive_embedded_interpreter.cpp"], "labels": ["cla signed", "ciflow/default"]}, "e70b5d64f4": {"title": "Change README getting started link to explicit instructions (#66828)", "body": "Summary:\nThis changes the link for installing binaries to the page on pytorch.org that is entirely the download command selector (which isn't visible on a normal aspect ratio screen when the main website page first loads anymore).\n\nThis also includes some other random fixes:\n* Update HUD link\n* Clean ups\n\nFixes #{issue number}\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66828\n\nReviewed By: malfet\n\nDifferential Revision: D31750654\n\nPulled By: driazati\n\nfbshipit-source-id: aef9ceba71418f6f7648eab9a8c8a78d6c60518b", "pr_number": "66828", "files_changed": ["README.md"], "labels": ["cla signed", "ciflow/default"]}, "bd4d5cb14c": {"title": "Sparse CSR: Add torch.empty (#63509)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/63509\n\nThe primary use of `torch.empty` is to reserve memory for tensor and set the type, device, size information. The same is done here for SparseCSR.\n`crow_indices` is initialized as an empty tensor of size `num_rows + 1`. `col_indices` and `values` are initialized as empty tensors of size 0.\n\ncc nikitaved pearu cpuhrsch IvanYashchuk\n\nTest Plan: Imported from OSS\n\nReviewed By: anjali411\n\nDifferential Revision: D31770359\n\nPulled By: cpuhrsch\n\nfbshipit-source-id: c83f2a2e0d7514ba24780add1086e1bccf541dd9", "pr_number": "63509", "files_changed": ["aten/src/ATen/native/native_functions.yaml", "aten/src/ATen/native/sparse/SparseCsrTensor.cpp", "test/test_sparse_csr.py"], "labels": ["module: sparse", "open source", "Merged", "cla signed", "ciflow/default"]}, "b3bb234e16": {"title": "Remove THCGeneral.cpp (#66766)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/66766\n\nTest Plan: Imported from OSS\n\nReviewed By: zou3519\n\nDifferential Revision: D31721647\n\nPulled By: ngimel\n\nfbshipit-source-id: 5033a2800871c8745a1a92e379c9f97c98af212e", "pr_number": "66766", "files_changed": ["BUILD.bazel", "aten/src/ATen/cuda/detail/CUDAHooks.cpp", "aten/src/ATen/native/cuda/Dropout.cu", "aten/src/ATen/native/cuda/TensorFactories.cu", "aten/src/ATen/native/cuda/Unique.cu", "aten/src/ATen/native/cudnn/Conv_v7.cpp", "aten/src/THC/CMakeLists.txt", "aten/src/THC/THCGeneral.cpp", "aten/src/THC/THCGeneral.h", "aten/src/THC/THCStorage.cpp", "torch/csrc/cuda/Stream.cpp", "torch/csrc/generic/StorageSharing.cpp", "torch/csrc/generic/serialization.cpp"], "labels": ["open source", "Merged", "cla signed", "ciflow/default"]}, "c69e33bb11": {"title": "Fix doc string for torch.acosh (#66814)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66814\nShift equation above note as per issue 65905 on github\n\nTest Plan:\nImported from OSS\n\nIn preview docs built from PR\n\nhttps://docs-preview.pytorch.org/66814/generated/torch.acosh.html#torch.acosh equation is now above note\n\n{F671441651}\n\nReviewed By: gchanan\n\nDifferential Revision: D31742677\n\nPulled By: mikaylagawarecki\n\nfbshipit-source-id: 9fa5390ad2a01ca001418c0bd624f2145f861bf4", "pr_number": "66814", "files_changed": ["torch/_torch_docs.py"], "labels": ["Merged", "cla signed", "ciflow/default"]}, "867ccc9987": {"title": "Strided masked reduction: amin. (#66385)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66385\n\ncc nikitaved pearu cpuhrsch IvanYashchuk\n\nTest Plan: Imported from OSS\n\nReviewed By: jbschlosser\n\nDifferential Revision: D31779530\n\nPulled By: cpuhrsch\n\nfbshipit-source-id: de753c2d191f7980a48831b892d3a1e8a7a547cd", "pr_number": "66385", "files_changed": ["torch/_masked/__init__.py", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["module: sparse", "open source", "module: reductions", "cla signed", "ciflow/default"]}, "f95fef7897": {"title": "Add prim::TensorExprDynamicGuard to bc allowlist (#66939)", "body": "Summary:\nFixes #{issue number}\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66939\n\nReviewed By: ejguan\n\nDifferential Revision: D31797160\n\nPulled By: soulitzer\n\nfbshipit-source-id: 630b7a0ab99671192397f927391361622f7e9c2e", "pr_number": "66939", "files_changed": ["test/backward_compatibility/check_backward_compatibility.py"], "labels": ["Merged", "cla signed", "ciflow/default"]}, "8a65047acc": {"title": "[skip ci] Set test owners for everything considered with module: tests (#66865)", "body": "Summary:\nAction following https://github.com/pytorch/pytorch/issues/66232\n\ncc mruberry\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66865\n\nReviewed By: anjali411\n\nDifferential Revision: D31771147\n\nPulled By: janeyx99\n\nfbshipit-source-id: 8bebe5ac2098364ef1ee93b590abb5f4455b0f89", "pr_number": "66865", "files_changed": ["test/test_binary_ufuncs.py", "test/test_indexing.py", "test/test_reductions.py", "test/test_shape_ops.py", "test/test_sort_and_select.py", "test/test_testing.py", "test/test_torch.py", "test/test_unary_ufuncs.py", "test/test_view_ops.py"], "labels": ["module: tests", "cla signed", "ciflow/default"]}, "452b359c3f": {"title": "[skip ci] Set test owners for tensor creation tests (#66864)", "body": "Summary:\nAction following https://github.com/pytorch/pytorch/issues/66232\n\ncc gchanan mruberry\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66864\n\nReviewed By: anjali411\n\nDifferential Revision: D31771139\n\nPulled By: janeyx99\n\nfbshipit-source-id: 74adeae7de355fa6c63de22290fa324911230368", "pr_number": "66864", "files_changed": ["test/test_tensor_creation_ops.py"], "labels": ["module: tensor creation", "cla signed", "ciflow/default"]}, "409364e597": {"title": "[skip ci] Set test owners for test_typing.py (#66869)", "body": "Summary:\nAction following https://github.com/pytorch/pytorch/issues/66232\n\ncc ezyang malfet rgommers xuzhao9 gramster\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66869\n\nReviewed By: anjali411\n\nDifferential Revision: D31766850\n\nPulled By: janeyx99\n\nfbshipit-source-id: e9772f5378be07162d4f4d06925165e396d7d6c6", "pr_number": "66869", "files_changed": ["test/test_type_hints.py", "test/test_type_info.py", "test/test_typing.py"], "labels": ["module: typing", "cla signed", "ciflow/default"]}, "822277f302": {"title": "[skip ci] Set test owners for test_type_promotion.py (#66866)", "body": "Summary:\nAction following https://github.com/pytorch/pytorch/issues/66232\n\ncc nairbv mruberry\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66866\n\nReviewed By: anjali411\n\nDifferential Revision: D31771149\n\nPulled By: janeyx99\n\nfbshipit-source-id: 87c04ed4a75ada06a553a11064d44ac65fc4c6ea", "pr_number": "66866", "files_changed": ["test/test_type_promotion.py"], "labels": ["module: type promotion", "cla signed", "ciflow/default"]}, "a015964cf8": {"title": "Strided masked reduction: prod. (#66386)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66386\n\ncc nikitaved pearu cpuhrsch\n\nTest Plan: Imported from OSS\n\nReviewed By: anjali411\n\nDifferential Revision: D31779598\n\nPulled By: cpuhrsch\n\nfbshipit-source-id: 304a3d6abc794a49de5b044aade6cfd727758495", "pr_number": "66386", "files_changed": ["torch/_masked/__init__.py", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["module: sparse", "open source", "module: reductions", "cla signed", "ciflow/default"]}, "793f366e34": {"title": "[skip ci] Set test owners for sparse tests (#66863)", "body": "Summary:\nAction following https://github.com/pytorch/pytorch/issues/66232\n\ncc nikitaved pearu cpuhrsch IvanYashchuk\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66863\n\nReviewed By: anjali411\n\nDifferential Revision: D31771126\n\nPulled By: janeyx99\n\nfbshipit-source-id: 6cb5ca0557e8555f6a09b3e607ff8888e505486e", "pr_number": "66863", "files_changed": ["test/test_sparse.py", "test/test_sparse_csr.py"], "labels": ["module: sparse", "cla signed", "ciflow/default"]}, "5569d5824c": {"title": "Fix documentation of arguments for torch.nn.functional.Linear (#66884)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66884\n\nAddressing docs fix mentioned in issue 64978 on Github\nghstack-source-id: 141093449\n\nTest Plan: https://pxl.cl/1Rxkz\n\nReviewed By: anjali411\n\nDifferential Revision: D31767303\n\nfbshipit-source-id: f1ca10fed5bb768749bce3ddc240bbce1dfb3f84", "pr_number": "66884", "files_changed": ["torch/nn/functional.py"], "labels": ["cla signed", "ciflow/default"]}, "6e67150f57": {"title": "[skip ci] Set test owner for test_mkldnn.py (#66845)", "body": "Summary:\nAction following https://github.com/pytorch/pytorch/issues/66232\n\ncc gujinghui PenghuiCheng XiaobingSuper jianyuh VitalyFedyunin\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66845\n\nReviewed By: anjali411\n\nDifferential Revision: D31803377\n\nPulled By: janeyx99\n\nfbshipit-source-id: 4fcf77d3e4bf976449a0b1ab4d750619db3493a1", "pr_number": "66845", "files_changed": ["test/test_mkldnn.py"], "labels": ["module: mkldnn", "Merged", "cla signed", "ciflow/default"]}, "450221c534": {"title": "Sparse CSR: Add tensor.resize_ and tensor.copy_ (#63510)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/63510\n\nSparse CSR matrix resizing behavior:\nIf we _increase the number of rows_ the number of specified elements in the matrix remains the same -> the size of col_indices, values doesn't change, the size of crow_indices becomes `rows+1`.\nIf we _decrease the number of rows_ the number of specified elements will be `min(nnz, rows*cols)` -> need to resize `crow_indices` to `rows+1` and set the last element to `min(nnz, rows*cols)`; decrease the size of col_indices and values to `min(nnz, rows*cols)`.\nIf we _increase the number of columns_ the number of specified elements in the matrix remains the same, the number of rows remains the same -> no need to resize anything, just set new sizes.\nWe _cannot decrease the number of columns_ because it would require recomputing `crow_indices`.\n\ncc nikitaved pearu cpuhrsch IvanYashchuk\n\nTest Plan: Imported from OSS\n\nReviewed By: anjali411\n\nDifferential Revision: D31796680\n\nPulled By: cpuhrsch\n\nfbshipit-source-id: 7d8a9701ce06d30a1841f94bba0a057cacea9401", "pr_number": "63510", "files_changed": ["aten/src/ATen/SparseCsrTensorImpl.cpp", "aten/src/ATen/SparseCsrTensorImpl.h", "aten/src/ATen/native/native_functions.yaml", "aten/src/ATen/native/sparse/SparseCsrTensor.cpp", "test/test_sparse_csr.py"], "labels": ["module: sparse", "open source", "cla signed", "ciflow/default"]}, "257239972c": {"title": "Fix attr_to_scope's key in `torch/utils/tensorboard/_pytorch_graph.py` (#65692)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/65652\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65692\n\nReviewed By: Reubend\n\nDifferential Revision: D31678606\n\nPulled By: edward-io\n\nfbshipit-source-id: 7c0bf740ee4f8c21bd01ced3ae70df23c9efadfb", "pr_number": "65692", "files_changed": ["test/expect/TestTensorBoard.test_nested_nn_squential.expect", "test/test_tensorboard.py", "torch/utils/tensorboard/_pytorch_graph.py"], "labels": ["triaged", "open source", "Merged", "cla signed", "ciflow/default"]}, "18bbc4c2b7": {"title": "[Static Runtime] Fix a bug in aten::index (#66940)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66940\n\n`aten::index`'s schema is as follows:\n\n```\n\"aten::index.Tensor(Tensor self, Tensor?[] indices) -> Tensor\n```\n\nThe current implementation assumes `indices`' elements are all tensors by doing `elem.toTensor`, which is incorrectly. This change creates an empty optional value if an element from `indices` is not a tensor.\n\nTest Plan: Fixed `StaticRuntime, IndividualOps_Index` to correctly test `aten::index` with `indices` that contains `None`.\n\nReviewed By: hlu1\n\nDifferential Revision: D31712145\n\nfbshipit-source-id: be1c29674bcd55b67b0dcc2a988bc37fd43745f3", "pr_number": "66940", "files_changed": ["aten/src/ATen/native/IndexingUtils.h", "benchmarks/static_runtime/test_scripts.h", "benchmarks/static_runtime/test_static_runtime.cc"], "labels": ["fb-exported", "cla signed", "ciflow/default"]}, "e046386be8": {"title": "Avoid inlining error reporting in checked_convert (#66721)", "body": "Summary:\n**Summary:** Move the error reporting part to the cpp file to avoid callers inlining it, which inflates the generated code size. See https://github.com/pytorch/pytorch/issues/65830.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66721\n\nTest Plan:\nCompiling the simple program below now generates ~150 lines of assembly, compared to 700+ lines before.\n\n```\n#include <c10/core/Scalar.h>\n\nvoid g(float) {}\n\nvoid f(const c10::Scalar& scalar) {\n    auto x = scalar.to<float>();\n    g(x);\n}\n```\n\n**Reviewers:** Brian Hirsh\n\n**Subscribers:** Brian Hirsh, Edward Yang, Yining Lu\n\n**Tasks:** T103384490\n\n**Tags:** pytorch\n\nFixes https://github.com/pytorch/pytorch/issues/65830\n\nReviewed By: zou3519, bdhirsh\n\nDifferential Revision: D31737607\n\nPulled By: andrewor14\n\nfbshipit-source-id: 3d493c4d8e51d8f8a19d00f59b8ea28176c8a9e3", "pr_number": "66721", "files_changed": ["c10/util/TypeCast.cpp", "c10/util/TypeCast.h"], "labels": ["cla signed", "ciflow/default"]}, "db4165892b": {"title": "[SmartCompose][OnDevice]fix function name bug in mobile export & Script to convert mobile model (#66915)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66915\n\nPull Request resolved: https://github.com/pytorch/pytorch-canary/pull/3\n\nfix function name bug in mobile export\n\nTest Plan: buck run pytext/fb/assistant/smart_compose:mobile_converter -- --model_input=pytext_training/tree/teams/assistant/smart_compose/300555761/model.ts --model_output=pytext_training/tree/teams/assistant/smart_compose/300555761/mobile_model_test.ts\n\nReviewed By: JacobSzwejbka\n\nDifferential Revision: D31782983\n\nfbshipit-source-id: 7288bb65adc7346d218980a535d68a12d8ef2033", "pr_number": "66915", "files_changed": ["torch/utils/bundled_inputs.py"], "labels": ["fb-exported", "cla signed", "ciflow/default"]}, "8beabffac3": {"title": "[PyTorchEdge] Make aten function common to aten and torch_common (#66663)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66663\n\nfb: TensorCompare.cpp is in per-app, a target higher than torch_mobile\n\nPlease read this doc to know about [Per-app ATen/native and Template Selective Build](\nhttps://docs.google.com/document/d/1O5--mOAi_gGh2GkE-REo3qJRRQ_Lks69IfgszcB8ThI/edit)\n\nCreate a filed called \"prim_native_functions.cpp\" in ATen, add it to aten_cpu, and cut-paste native::is_nonzero() to prim_native_functions.cpp.\nBy doing this we move the function to lower layer which is more visible to all targets depending on it.\n\nInstruction count comparison new vs old\nhttps://www.internalfb.com/phabricator/paste/view/P463272302?view=diff\n\nTest Plan:\nfb:\n```\n(base) [pavithran@devvm1803.vll0 /data/users/pavithran/fbsource] buck build //xplat/caffe2:aten_cpu\nBuilding: finished in 0.4 sec (100%) 1/202 jobs, 0/202 updated\n  Total time: 0.4 sec\nMore details at https://www.internalfb.com/intern/buck/build/ea35300b-55be-4b9f-bc74-80cdd869c16a\nBUILD SUCCEEDED\n(base) [pavithran@devvm1803.vll0 /data/users/pavithran/fbsource] buck build //xplat/caffe2:aten_native_cpu\nBuilding: finished in 0.7 sec (100%) 1/1 jobs, 0/1 updated\n  Total time: 0.8 sec\nMore details at https://www.internalfb.com/intern/buck/build/ccd97d43-c59d-4f29-9418-485cd24575e2\nBUILD SUCCEEDED\n```\n\nReviewed By: iseeyuan\n\nDifferential Revision: D31669536\n\nfbshipit-source-id: d35f069f975db6dce0b678c5b5ddd74bd690f599", "pr_number": "66663", "files_changed": ["aten/src/ATen/native/TensorCompare.cpp", "aten/src/ATen/native/prim_native_functions.cpp", "tools/build_variables.bzl"], "labels": ["fb-exported", "cla signed", "ciflow/default"]}, "32e790997b": {"title": "[Rocm]Reduce severity of detected possible memory leak from assertion to warning (#65973)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/62533.\nIn very rare cases, the decorator for detecting memory leak is throwing assertion, even when the test is passing, and the memory is being freed with a tiny delay. The issue is not being reproduced in internal testing, but shows up sometimes in CI environment.\n\nReducing the severity of such detection to warning, so as not to fail the CI tests, as the actual test is not failing, rather only the check inside the decorator is failing.\n\nLimiting the change to ROCM only for now.\n\ncc jeffdaily sunway513 jithunnair-amd ROCmSupport\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65973\n\nReviewed By: anjali411\n\nDifferential Revision: D31776154\n\nPulled By: malfet\n\nfbshipit-source-id: 432199fca17669648463c4177c62adb553cacefd", "pr_number": "65973", "files_changed": ["test/test_cuda.py", "torch/testing/_internal/common_utils.py"], "labels": ["module: rocm", "triaged", "open source", "Merged", "cla signed"]}, "f5c5ab2868": {"title": "[skip ci] Set test owner for cpp-extensions tests (#66837)", "body": "Summary:\nAction following https://github.com/pytorch/pytorch/issues/66232\n\ncc yf225 glaringlee zou3519\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66837\n\nReviewed By: anjali411\n\nDifferential Revision: D31828401\n\nPulled By: janeyx99\n\nfbshipit-source-id: 35ac27f3e1c0eb70ccb38c07c42ba61bd0c848fe", "pr_number": "66837", "files_changed": ["test/test_cpp_extensions_aot.py", "test/test_cpp_extensions_jit.py"], "labels": ["module: cpp-extensions", "cla signed", "ciflow/default"]}, "960e3216a4": {"title": "[skip ci] Set test owner for named tensor tests (#66849)", "body": "Summary:\nAction following https://github.com/pytorch/pytorch/issues/66232\n\ncc zou3519\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66849\n\nReviewed By: zou3519\n\nDifferential Revision: D31828903\n\nPulled By: janeyx99\n\nfbshipit-source-id: 30810bcec750ba8e1d5a342c31a5996bf57acd69", "pr_number": "66849", "files_changed": ["test/test_namedtensor.py"], "labels": ["module: named tensor", "cla signed", "ciflow/default"]}, "13b8599831": {"title": "[skip ci] Set test owner for test_dispatch.py (#66840)", "body": "Summary:\nAction following https://github.com/pytorch/pytorch/issues/66232\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66840\n\nReviewed By: saketh-are\n\nDifferential Revision: D31829224\n\nPulled By: janeyx99\n\nfbshipit-source-id: 66aceacd4f976c36ed48ca5be59616d245ba2a82", "pr_number": "66840", "files_changed": ["test/test_dispatch.py", "test/test_gen_backend_stubs.py"], "labels": ["module: dispatch", "cla signed", "ciflow/default"]}, "23321ba7a3": {"title": "Fix bug [#66780]: wrong input to torch.is_floating_point (#66783)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/66783\n\nTest Plan: Imported from OSS\n\nReviewed By: jbschlosser\n\nDifferential Revision: D31802971\n\nPulled By: cpuhrsch\n\nfbshipit-source-id: 6a7d8b83dad219fd683504f9084b77358800507c", "pr_number": "66783", "files_changed": ["test/test_reductions.py"], "labels": ["open source", "cla signed", "ciflow/default"]}, "40e5d31a52": {"title": "Add OpInfo for torch.bincount (#65796)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/65796\n\nReviewed By: bdhirsh\n\nDifferential Revision: D31386560\n\nPulled By: saketh-are\n\nfbshipit-source-id: acb6ed3f743ddcccd0ff7ce1ab21f77c2078da37", "pr_number": "65796", "files_changed": ["torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "cla signed", "ciflow/default"]}, "94f4e9a995": {"title": "Enable warning tests for nondeterministic backward functions (#66736)", "body": "Summary:\nFollowup from https://github.com/pytorch/pytorch/issues/66233\n\nSince https://github.com/pytorch/pytorch/issues/50209 was fixed, we can enable these warning tests now\n\ncc mruberry kurtamohler\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66736\n\nReviewed By: zou3519\n\nDifferential Revision: D31723385\n\nPulled By: mruberry\n\nfbshipit-source-id: dc1922a6d0c45cc80020db85710e755a89113861", "pr_number": "66736", "files_changed": ["test/test_torch.py", "torch/testing/_internal/common_device_type.py"], "labels": ["module: determinism", "open source", "Merged", "cla signed", "ciflow/default"]}, "78f970568c": {"title": "Add dummy op to use instead of searchsorted (#66964)", "body": "Summary:\nWould help unblock https://github.com/pytorch/pytorch/issues/66818 if this actually works\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66964\n\nReviewed By: mruberry\n\nDifferential Revision: D31817942\n\nPulled By: janeyx99\n\nfbshipit-source-id: 9e9a2bcb0c0479ec7000ab8760a2e64bf0e85e95", "pr_number": "66964", "files_changed": ["aten/src/ATen/native/cuda/Bucketization.cu", "aten/src/ATen/native/native_functions.yaml", "caffe2/CMakeLists.txt", "torch/utils/cpp_extension.py"], "labels": ["Merged", "cla signed", "ciflow/default", "ciflow/all"]}, "00a871c5c9": {"title": "[skip ci] Set test owner for multiprocessing tests (#66848)", "body": "Summary:\nAction following https://github.com/pytorch/pytorch/issues/66232\n\ncc VitalyFedyunin\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66848\n\nReviewed By: VitalyFedyunin\n\nDifferential Revision: D31828908\n\nPulled By: janeyx99\n\nfbshipit-source-id: 45d6901648f5564c1bf07ad8d01d69ef486ae104", "pr_number": "66848", "files_changed": ["test/test_multiprocessing.py", "test/test_multiprocessing_spawn.py"], "labels": ["module: multiprocessing", "Merged", "cla signed", "ciflow/default"]}, "6f1ba16d6d": {"title": "[skip ci] Set test owners for cpp test (#66836)", "body": "Summary:\nAction following https://github.com/pytorch/pytorch/issues/66232\n\ncc yf225 glaringlee\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66836\n\nReviewed By: saketh-are\n\nDifferential Revision: D31828641\n\nPulled By: janeyx99\n\nfbshipit-source-id: 076d41686746fecebc07452df8212eef15a7824c", "pr_number": "66836", "files_changed": ["test/test_cpp_api_parity.py"], "labels": ["module: cpp", "Merged", "cla signed", "ciflow/default"]}, "b07371f19c": {"title": "[skip ci] Set test owners for serialization tests (#66862)", "body": "Summary:\nAction following https://github.com/pytorch/pytorch/issues/66232\n\ncc mruberry\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66862\n\nReviewed By: saketh-are\n\nDifferential Revision: D31828615\n\nPulled By: janeyx99\n\nfbshipit-source-id: 8d28970eead9d6f26e9ea64b823295d9c9e1469d", "pr_number": "66862", "files_changed": ["test/test_serialization.py"], "labels": ["module: serialization", "Merged", "cla signed", "ciflow/default"]}, "062ae8df0e": {"title": "Automated submodule update: tensorpipe (#65353)", "body": "Summary:\nThis is an automated pull request to update the first-party submodule for [pytorch/tensorpipe](https://github.com/pytorch/tensorpipe).\n\nNew submodule commit: https://github.com/pytorch/tensorpipe/commit/183172ba8c323a1a325c61d67445874141c88c12\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65353\n\nTest Plan: Ensure that CI jobs succeed on GitHub before landing.\n\nReviewed By: lw\n\nDifferential Revision: D31059779\n\nfbshipit-source-id: 7bddff5139f8168750e22e1cc8c0d49931db542e", "pr_number": "65353", "files_changed": ["third_party/tensorpipe"], "labels": ["triaged", "open source", "Merged", "cla signed", "ciflow/default"]}, "892ac08a02": {"title": "Do not generate not_implemented error for forward AD when input with tangent passed to non-differentiable function (#66926)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/61926\n\n1. update the `if` to just use requires_derivative since that should reflect when function is not differentiable\n2. if `requires_derivative=True` but no outputs have forward derivatives, we should error as usual\n3. ~In the future we may also want to handle the case~ when `len(fw_derivatives) > 0 and len(fw_derivatives) < num_diff_outputs` we should add assert in codegen that this does not happen.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66926\n\nReviewed By: anjali411\n\nDifferential Revision: D31810736\n\nPulled By: soulitzer\n\nfbshipit-source-id: 11a14477cc7554f576cff2ed1711a448a8c6a66a", "pr_number": "66926", "files_changed": ["test/test_autograd.py", "tools/autograd/gen_variable_type.py"], "labels": ["Merged", "cla signed", "ciflow/default"]}, "462f333c01": {"title": "[numpy] add torch.argwhere (#64257)", "body": "Summary:\nAdds `torch.argwhere` as an alias to `torch.nonzero`\n\nCurrently, `torch.nonzero` is actually provides equivalent functionality to `np.argwhere`.\n\nFrom NumPy docs,\n> np.argwhere(a) is almost the same as np.transpose(np.nonzero(a)), but produces a result of the correct shape for a 0D array.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/64257\n\nReviewed By: dagitses\n\nDifferential Revision: D31474901\n\nPulled By: saketh-are\n\nfbshipit-source-id: 335327a4986fa327da74e1fb8624cc1e56959c70", "pr_number": "64257", "files_changed": ["aten/src/ATen/core/aten_interned_strings.h", "aten/src/ATen/core/interned_strings.h", "aten/src/ATen/native/TensorAdvancedIndexing.cpp", "aten/src/ATen/native/native_functions.yaml", "docs/source/tensors.rst", "docs/source/torch.rst", "torch/_tensor_docs.py", "torch/_torch_docs.py", "torch/overrides.py", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["oncall: jit", "open source", "Merged", "cla signed", "ciflow/default"]}, "fcfa06586d": {"title": "Wextra fix for NamedTensor.cpp (#66897)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66897\n\nFixes:\n```\nstderr: caffe2/aten/src/ATen/native/NamedTensor.cpp:226:19: error: comparison of integers of different signs: 'const unsigned long' and 'int64_t' (aka 'long') [-Werror,-Wsign-compare]\n    if (order_idx >= ellipsis_idx) {\n        ~~~~~~~~~ ^  ~~~~~~~~~~~~\nstderr: caffe2/aten/src/ATen/native/NamedTensor.cpp:226:19: error: comparison of integers of different signs: 'const unsigned long' and 'int64_t' (aka 'long') [-Werror,-Wsign-compare]\n    if (order_idx >= ellipsis_idx) {\n        ~~~~~~~~~ ^  ~~~~~~~~~~~~\n```\n\nTest Plan: Sandcastle\n\nReviewed By: malfet\n\nDifferential Revision: D31774623\n\nfbshipit-source-id: b6e5b76695e512084ac5c9cb4215de7e9b763cf8", "pr_number": "66897", "files_changed": ["aten/src/ATen/native/NamedTensor.cpp"], "labels": ["fb-exported", "cla signed", "ciflow/default"]}, "f29e5220a6": {"title": "Revert D31474901: [pytorch][PR] [numpy] add torch.argwhere", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD31474901\n\nOriginal commit changeset: 335327a4986f\n\nfbshipit-source-id: 534093e459762ff7a888c58d76e49e362015f2ba", "pr_number": null, "files_changed": ["aten/src/ATen/core/aten_interned_strings.h", "aten/src/ATen/core/interned_strings.h", "aten/src/ATen/native/TensorAdvancedIndexing.cpp", "aten/src/ATen/native/native_functions.yaml", "docs/source/tensors.rst", "docs/source/torch.rst", "torch/_tensor_docs.py", "torch/_torch_docs.py", "torch/overrides.py", "torch/testing/_internal/common_methods_invocations.py"], "labels": []}, "b696d64ef4": {"title": "Binaries without AVX512 kernels shouldn't report CPU Capability as AVX512 on machines with AVX512 support (#66703)", "body": "Summary:\n### BUG\nIf a PyTorch binary is built with a compiler that doesn't support all the AVX512 intrinsics in the codebase, then it won't have ATen AVX512 kernels, but at runtime, CPU capability would still be incorrectly returned as AVX512 on a machine that supports AVX512. It seems that PyTorch Linux releases are done on CentOS with `gcc 7.3`, so this bug would manifest in the 1.10 release, unless a fix such as this one is added. gcc versions below 9.0 don't support all the AVX512 intrinsics in the codebase, such as `_mm512_set_epi16`.\n\n### FIX\nCPU Capability would be returned as AVX512 at runtime only if the binary was built with a compiler that supports all the AVX512 intrinsics in the codebase, and if the hardware the binary is being run on supports all the required AVX512 instruction sets.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66703\n\nReviewed By: gchanan\n\nDifferential Revision: D31732625\n\nPulled By: malfet\n\nfbshipit-source-id: e52d06b87fbe2af9b303a2e9c264189c8512d5ec", "pr_number": "66703", "files_changed": ["aten/src/ATen/native/DispatchStub.cpp"], "labels": ["triaged", "open source", "Merged", "cla signed", "ciflow/default"]}, "2578de4851": {"title": "[skip ci] Set test owner for test_cuda* tests (#66838)", "body": "Summary:\nAction following https://github.com/pytorch/pytorch/issues/66232\n\ncc ngimel\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66838\n\nReviewed By: saketh-are\n\nDifferential Revision: D31841411\n\nPulled By: janeyx99\n\nfbshipit-source-id: 5cdffdef4a92f9adcef1143ae4598b052c5acc6b", "pr_number": "66838", "files_changed": ["test/test_cuda.py", "test/test_cuda_primary_ctx.py"], "labels": ["module: cuda", "Merged", "cla signed", "ciflow/default"]}, "20f08d23a0": {"title": "Revert D31838513: Strided masked reduction: mean.", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD31838513\n\nOriginal commit changeset: 54b99ccf9821\n\nfbshipit-source-id: 5480e8482c8770b41579ee085e158572b659c1f5", "pr_number": null, "files_changed": ["torch/_masked/__init__.py", "torch/testing/_internal/common_methods_invocations.py"], "labels": []}, "35965869cf": {"title": "Enroll bowangbj@ to PyTorch distributed package (#67062)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/67062\n\nFor cc and potential reviews\n\nTest Plan: Imported from OSS\n\nReviewed By: mrshenli\n\nDifferential Revision: D31849050\n\nfbshipit-source-id: d3899c2ca857b8f22bdc88b4e83cdd20bbf0b1d6", "pr_number": "67062", "files_changed": ["CODEOWNERS"], "labels": ["cla signed", "ciflow/default"]}, "28fac23409": {"title": "Fixes CUDA vs CPU consistency for index_put_ when accumulating (#66790)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/39227\nFixes https://github.com/pytorch/pytorch/issues/66495 (duplicate of 39227)\n\nDescription:\n- Expands values for CUDA implementation\n- Improved shapes checking for CUDA\n- Improved error message for CUDA\n- Added tests\n\ncc zou3519\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66790\n\nReviewed By: mruberry\n\nDifferential Revision: D31843566\n\nPulled By: ngimel\n\nfbshipit-source-id: c9e5d12a33e1067619c210174ba6e3cd66d5718b", "pr_number": "66790", "files_changed": ["aten/src/ATen/native/cuda/Indexing.cu", "test/test_indexing.py"], "labels": ["triaged", "open source", "cla signed", "ciflow/default"]}, "4fe8055b9f": {"title": "made functorch not decompose by default (#66945)", "body": "Summary:\nBasically reverting this: https://github.com/pytorch/pytorch/pull/63616\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66945\n\nReviewed By: zou3519\n\nDifferential Revision: D31802176\n\nPulled By: Chillee\n\nfbshipit-source-id: b1cabd7af66aef26411801516c87336eaea4fccb", "pr_number": "66945", "files_changed": ["c10/core/DispatchKeySet.cpp"], "labels": ["cla signed", "ciflow/default"]}, "01ced45217": {"title": "[iOS] Bump up iOS CocoaPods version to 1.10.0 (#67058)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/67058\n\nTest Plan: Imported from OSS\n\nReviewed By: xta0\n\nDifferential Revision: D31846445\n\nPulled By: hanton\n\nfbshipit-source-id: 7510a6c15fdeecc996fcce5c48db32e148ba7def", "pr_number": "67058", "files_changed": ["ios/LibTorch-Lite.podspec", "ios/LibTorch.podspec"], "labels": ["cla signed", "ciflow/default"]}, "d1a5612a3e": {"title": "remove accscalar from i0 and i0e (#67048)", "body": "Summary:\nRemoves some of the half math ops to make https://github.com/pytorch/pytorch/issues/64023 possible.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/67048\n\nReviewed By: mruberry\n\nDifferential Revision: D31847249\n\nPulled By: ngimel\n\nfbshipit-source-id: 8385aacd846bb990e368ff336eb346d847af70b9", "pr_number": "67048", "files_changed": ["aten/src/ATen/native/cuda/Math.cuh", "aten/src/ATen/native/cuda/UnarySpecialOpsKernel.cu"], "labels": ["cla signed", "ciflow/default"]}, "e8742f15cf": {"title": "[quant][graphmode][fx] Add observation_type.py (#67063)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/67063\n\nAdding ObservationType Enum for `backend_config_dict`\n\nTest Plan: Imported from OSS\n\nReviewed By: vkuzo\n\nDifferential Revision: D31849078\n\nfbshipit-source-id: e9e7225d564b51fa9454f7f087dd134152c069a0", "pr_number": "67063", "files_changed": ["torch/ao/quantization/fx/backend_config_dict/observation_type.py"], "labels": ["cla signed", "ciflow/default", "fx"]}, "7e5aa0d35a": {"title": "fixed unique arguments documentation (#66132)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66132\n\nDifferential Revisi\n<img width=\"875\" alt=\"Screen Shot 2021-10-05 at 12 10 39 PM\" src=\"https://user-images.githubusercontent.com/17888388/136276286-3df20681-7b7a-4a91-97d6-4f1ac3722121.png\">\non: [D31397746](https://our.intern.facebook.com/intern/diff/D31397746/)\n\nTest Plan: Imported from OSS\n\nReviewed By: gchanan\n\nDifferential Revision: D31734476\n\nPulled By: samdow\n\nfbshipit-source-id: 8999443c7f9b24394d7543652b8350261c1f8b3a", "pr_number": "66132", "files_changed": ["torch/functional.py"], "labels": ["cla signed", "ciflow/default"]}, "7b0408684b": {"title": "Fix linter (#67122)", "body": "Summary:\nFixes regression introduced by https://github.com/pytorch/pytorch/commit/7e5aa0d35a540807d524966e118d1ead4d3b2eae\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/67122\n\nReviewed By: seemethere\n\nDifferential Revision: D31872569\n\nPulled By: malfet\n\nfbshipit-source-id: ada0137db9a46cbec573489c9c37a94f3a7576ae", "pr_number": "67122", "files_changed": ["torch/functional.py"], "labels": ["cla signed", "ciflow/default"]}, "af1a2df825": {"title": "enable better depthwise conv perf on cudnn 8.2+ (#58749)", "body": "Summary:\nThere are multiple improvement of depthwise convolution speed in cudnn between 7.6 and 8.2, since https://github.com/pytorch/pytorch/pull/22302.\nThis PR aim to harvest all the new improvement by enable more cudnn kernel. The workload checking logic can also be simplified now.\nTo keep the change simple, I kept things before cudnn 8.2 unchanged.\n\nSimilar to https://github.com/pytorch/pytorch/pull/22302, I used a script [here](https://gist.github.com/FDecaYed/e8ba98a95cd33697df2ace86fdb44897) to benchmark. Both run are using cudnn 8.2\nOne enhancement I did to the script is switch to event based timing. With warmup kernels to fill the launch queue ahead, this should give us accurate kernel timing even in CPU launch bound cases.\n\nHere is A100 and V100 result sorted by speedup.\n[Book1.xlsx](https://github.com/pytorch/pytorch/files/6530371/Book1.xlsx)\n\nResult highlights:\nNewly turned on 5x5 cudnn kernel show up to 6x speedup.\nClose to half of test sizes show >10% speedup.\nFixed some corner cases that previously caused 15-20x slowdown.\nOnly slowdown a handful of cases(~10 out of >1000)\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/58749\n\nReviewed By: bdhirsh\n\nDifferential Revision: D31613199\n\nPulled By: ngimel\n\nfbshipit-source-id: 883b58facad67ccd51dc9ab539368b4738d40398", "pr_number": "58749", "files_changed": ["aten/src/ATen/native/Convolution.cpp"], "labels": ["triaged", "open source", "cla signed", "ciflow/default"]}, "83f70db95c": {"title": "Fix common device computation for comparison ops. (#66245)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66245\n\nFixes #66053\n\nThis PR splits `declare_static_dtype_and_device` into two new methods for\n`TensorIteratorBase`: `declare_static_dtype` and `declare_static_device`.\n\nTest Plan: Imported from OSS\n\nReviewed By: ejguan\n\nDifferential Revision: D31503849\n\nPulled By: ngimel\n\nfbshipit-source-id: 4b131b691d29ceb5f3709f5d6503997ea0875c54", "pr_number": "66245", "files_changed": ["aten/src/ATen/TensorIterator.cpp", "aten/src/ATen/TensorIterator.h", "aten/src/ATen/native/BinaryOps.cpp", "test/test_binary_ufuncs.py"], "labels": ["open source", "cla signed", "module: structured kernels", "ciflow/default"]}}