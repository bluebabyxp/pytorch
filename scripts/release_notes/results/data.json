{"88232b4cee": {"title": "Fix ENABLE_RECORD_KERNEL_FUNCTION_DTYPE build (#65370)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65370\n\nForgot a wrapping 'namespace at' here!  And no contbuilds to test it.\nghstack-source-id: 138565579\n\nTest Plan:\n```\nbuck build --show-output -c pt.disable_per_op_profiling=0 -c pt.enable_record_kernel_dtype=1 -c pt.has_backtraces=1 fbsource//xplat/caffe2/fb/model_tracer:model_tracer\n```\n\nReviewed By: JacobSzwejbka\n\nDifferential Revision: D31065923\n\nfbshipit-source-id: ed4563fbd8f3c29f6b10ac8999c9010bd4359c97", "pr_number": "65370", "files_changed": ["aten/src/ATen/Dispatch.h", "tools/build_variables.bzl"], "labels": ["Merged", "cla signed", "ciflow/default"]}, "6d7bc34b67": {"title": "Make new_empty/new_ones/new_zeros/new_full respect subclass (#65169)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65169\n\nPreviously these composite functions created a new tensor\nusing at::empty (or some other factory function) using TensorOptions\nwhich doesn't preserve Python subclass.  Making new_empty a\nnon-composite op and then routing everyone through it makes it\nrespect subclass.  We could also make all of these non-composite\nbut this reduces the number of derivatives.yaml entries I have to\nmake and allows you to trace the fill calls.\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nReviewed By: albanD\n\nDifferential Revision: D31003713\n\nPulled By: ezyang\n\nfbshipit-source-id: 19f906f1404a6b724769c49f48d123f407a561ff", "pr_number": "65169", "files_changed": ["aten/src/ATen/native/TensorFactories.cpp", "aten/src/ATen/native/native_functions.yaml", "test/test_python_dispatch.py", "tools/autograd/derivatives.yaml"], "labels": ["Merged", "cla signed", "ciflow/default"]}, "00b732e98b": {"title": "Remove orphan from cuDNN persistent note (#65160)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/60009.\n\nAs the document is properly [included](https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/rnn.py#L799), and [`:orphan:` doesn't need to be used in included documents](https://github.com/sphinx-doc/sphinx/issues/6787#issuecomment-549256840), and no warning is emitted in my local build when removing it, I think it can be removed.\n\nThe artifact reported in https://github.com/pytorch/pytorch/issues/60009 can be seen in 3 pages: [torch.nn.RNN](https://pytorch.org/docs/stable/generated/torch.nn.RNN.html#torch.nn.RNN), [torch.nn.LSTM](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html#torch.nn.LSTM), and [torch.nn.GRU](https://pytorch.org/docs/stable/generated/torch.nn.GRU.html#torch.nn.GRU).\n\ncc ezyang suo\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65160\n\nReviewed By: bdhirsh\n\nDifferential Revision: D31020280\n\nPulled By: ezyang\n\nfbshipit-source-id: 6c3541e5a856a91cf1ce1d2db4d04f5d13118ee4", "pr_number": "65160", "files_changed": ["docs/source/cudnn_persistent_rnn.rst"], "labels": ["open source", "Merged", "cla signed"]}, "28bfdbb066": {"title": "OpInfo for `nn.functional.batch_norm` (#63218)", "body": "Summary:\nAddresses https://github.com/facebookresearch/functorch/issues/78 and https://github.com/pytorch/pytorch/issues/54261.\n\n* There exists `torch.batch_norm` but it takes an extra arg: `cudnn_enabled` (not there in functional variant). This is passed from the functional variant to `torch.batch_norm` here: https://github.com/pytorch/pytorch/blob/master/torch/nn/functional.py#L2282. `test_variant_consistency_jit` fails with an error: (when passed an alias)\n    ```python\n    File \"/home/krshrimali/Documents/Projects/Quansight/pytorch/test/test_ops.py\", line 457, in _test_consistency_helper\n    variant_forward = variant(cloned,\n    TypeError: batch_norm() missing 1 required positional arguments: \"cudnn_enabled\"\n    ```\n    * I'm not sure of a solution to this, as AFIK - there is no way to pass a lambda wrapper for an alias. Hence, I've skipped adding this as an alias there.\n    * On second thought, is this even an alias?\n\ncc: mruberry zou3519 kshitij12345\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/63218\n\nReviewed By: bdhirsh\n\nDifferential Revision: D31019785\n\nPulled By: zou3519\n\nfbshipit-source-id: 2a834d05835da975289efc544a7ad7e98c99438f", "pr_number": "63218", "files_changed": ["torch/testing/_internal/common_device_type.py", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["triaged", "open source", "Merged", "cla signed", "module: testing", "ci/windows", "ci/master", "ciflow/slow-gradcheck", "ciflow/scheduled"]}, "8bab468943": {"title": "Reduce test size for max_pool (#65336)", "body": "Summary:\nFixe OOM in slow gradcheck tests.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65336\n\nReviewed By: malfet\n\nDifferential Revision: D31059007\n\nPulled By: albanD\n\nfbshipit-source-id: 2dd6967d88663558e37f8c0836ad33333c92dfb5", "pr_number": "65336", "files_changed": ["torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "ciflow/slow-gradcheck"]}, "1fec9cd76b": {"title": "[Fixed] Enable Half, BFloat16, and Complex dtypes for coo-coo sparse matmul [CUDA] (#59980)", "body": "Summary:\nThis PR enables Half, BFloat16, ComplexFloat, and ComplexDouble support for matrix-matrix multiplication of COO sparse matrices.\nThe change is applied only to CUDA 11+ builds.\n\n`cusparseSpGEMM` also supports `CUDA_C_16F` (complex float16) and `CUDA_C_16BF` (complex bfloat16). PyTorch also supports the complex float16 dtype (`ScalarType::ComplexHalf`), but there is no convenient dispatch, so this dtype is omitted in this PR.\n\ncc nikitaved pearu cpuhrsch IvanYashchuk ezyang anjali411 dylanbespalko mruberry Lezcano\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/59980\n\nReviewed By: ngimel\n\nDifferential Revision: D30994115\n\nPulled By: cpuhrsch\n\nfbshipit-source-id: 4f55b99e8e25079d6273b4edf95ad6fa85aeaf24", "pr_number": "59980", "files_changed": ["aten/src/ATen/cuda/CUDADataType.h", "aten/src/ATen/native/sparse/cuda/SparseMatMul.cu", "test/test_sparse.py", "torch/testing/_internal/common_cuda.py", "torch/utils/hipify/cuda_to_hip_mappings.py"], "labels": ["module: sparse", "triaged", "module: complex", "open source", "Merged", "cla signed", "ciflow/default", "with-ssh", "ciflow/win"]}, "3f5f721ab3": {"title": "Pass through allow-list from prepare_qat into propagate_qconfig_ to allow custom mapping and custom QAT module (#65119)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65119\n\nPytorch Quantization: allow prepare_qat to include custom module by passing allow_list into the prepare_qat.\n\nWhen we are implementing custom module and custom mapping for Quantization Aware Training (QAT), we need to add the custom module to the mappings and to the allow_list during prepare_qat. The allow_list needs to be surfaced to the  propagate_qconfig_.\n\nTest Plan: relying on general unit test\n\nReviewed By: supriyar\n\nDifferential Revision: D30982060\n\nfbshipit-source-id: 1114115b6a3b853238d33d72b5cbaafc60f463e0", "pr_number": "65119", "files_changed": ["torch/ao/quantization/quantize.py"], "labels": ["fb-exported", "Merged", "cla signed"]}, "f90d9b48db": {"title": "test_neg_view: preseve sign of sample input (#63010)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/63010\n\nThis changes `test_neg_view` to call the operator with the same numeric values as the original sample input.\n\nTest Plan: Imported from OSS\n\nReviewed By: pbelevich\n\nDifferential Revision: D31082824\n\nPulled By: anjali411\n\nfbshipit-source-id: 7d50f99dc0d1343247e366cbe9b0ca081bd0a9b1", "pr_number": "63010", "files_changed": ["test/test_ops.py", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["open source", "Merged", "cla signed", "ciflow/default"]}, "9324d682fd": {"title": "Fix autograd engine checks and update InputMetadata (#65235)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65235\n\n1. Updated the legacy type checks in `torch/csrc/autograd/engine.cpp` to individually validate the dtype, device, and layout equality for grad and tensor.\n2. Removed device field from `InputMetadata` since it's already stored via storing options. Also, added `dtype()` and `layout()` methods to `InputMetadata`. To make this change, some calls had to be updated due to the change in constructor.\n3. To fix https://github.com/pytorch/pytorch/issues/65016:\n     a. Added a `is_tensor_subclass` field in `InputMetadata` to skip device checks for grad and tensor when the tensor has\n         python key set on it (tensor subclass).\n\nTest Plan: Imported from OSS\n\nReviewed By: pbelevich\n\nDifferential Revision: D31082693\n\nPulled By: anjali411\n\nfbshipit-source-id: cb551cd438c6ca40b0f18a4d0009e0861cf0fd4e", "pr_number": "65235", "files_changed": ["test/test_python_dispatch.py", "torch/csrc/autograd/engine.cpp", "torch/csrc/autograd/function.h", "torch/csrc/autograd/input_metadata.h", "torch/csrc/autograd/variable.cpp"], "labels": ["Merged", "cla signed"]}, "bcc6e3ab5e": {"title": "add python API to print all operators that have kernels registered to a particular DispatchKey (#63575)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/63575\n\nTest Plan: Imported from OSS\n\nReviewed By: ezyang, Chillee\n\nDifferential Revision: D30426919\n\nPulled By: bdhirsh\n\nfbshipit-source-id: b0e487e48dfe02f7b9d678403f0a2b5bfe146f4e", "pr_number": "63575", "files_changed": ["aten/src/ATen/core/dispatch/Dispatcher.cpp", "aten/src/ATen/core/dispatch/Dispatcher.h", "aten/src/ATen/core/dispatch/OperatorEntry.h", "aten/src/ATen/core/op_registration/op_registration_test.cpp", "c10/core/DispatchKey.cpp", "c10/core/DispatchKey.h", "test/test_dispatch.py", "torch/csrc/utils/python_dispatch.cpp", "torch/testing/_internal/common_utils.py"], "labels": ["Merged", "cla signed"]}, "7c9a278895": {"title": "fix trailing newlines (#65474)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/65474\n\nTest Plan: Imported from OSS\n\nReviewed By: ezyang\n\nDifferential Revision: D31114952\n\nPulled By: suo\n\nfbshipit-source-id: 3b8cde2098635450c3e22571a401f78e4e54e9e0", "pr_number": "65474", "files_changed": ["torch/csrc/Exceptions.cpp"], "labels": ["Merged", "cla signed", "ciflow/default"]}, "152f0236c3": {"title": "Revert D31082693: Fix autograd engine checks and update InputMetadata", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD31082693 (https://github.com/pytorch/pytorch/commit/9324d682fdb87102d049d0579b433c759e7f998a)\n\nOriginal commit changeset: cb551cd438c6\n\nfbshipit-source-id: fc60f86b80fc70058984df6bccbf240d27f5843e", "pr_number": null, "files_changed": ["test/test_python_dispatch.py", "torch/csrc/autograd/engine.cpp", "torch/csrc/autograd/function.h", "torch/csrc/autograd/input_metadata.h", "torch/csrc/autograd/variable.cpp"], "labels": []}, "b3ec88f41f": {"title": "ugh (#65477)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/65477\n\nTest Plan: Imported from OSS\n\nReviewed By: zhouzhuojie\n\nDifferential Revision: D31115936\n\nPulled By: suo\n\nfbshipit-source-id: fb16911a683713fdc2393bfe7150fc29c7d6814f", "pr_number": "65477", "files_changed": ["torch/csrc/Exceptions.cpp"], "labels": ["Merged", "cla signed", "ciflow/default"]}, "db4b68b3ac": {"title": "Back out \"Eagerly populate python_error::what() when TORCH_SHOW_CPP_STACKTRACES=1\"", "body": "Summary: Original commit changeset: 9cfda47cafb3\n\nTest Plan: unland\n\nReviewed By: ezyang\n\nDifferential Revision: D31116643\n\nfbshipit-source-id: 631eea446ed48c63ca39281d24163a2eadbe8d12", "pr_number": null, "files_changed": ["torch/csrc/Exceptions.cpp", "torch/csrc/Exceptions.h"], "labels": []}, "158393e1a1": {"title": "Fix autograd engine checks and update InputMetadata (#65235)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65235\n\n1. Updated the legacy type checks in `torch/csrc/autograd/engine.cpp` to individually validate the dtype, device, and layout equality for grad and tensor.\n2. Removed device field from `InputMetadata` since it's already stored via storing options. Also, added `dtype()` and `layout()` methods to `InputMetadata`. To make this change, some calls had to be updated due to the change in constructor.\n3. To fix https://github.com/pytorch/pytorch/issues/65016:\n     a. Added a `is_tensor_subclass` field in `InputMetadata` to skip device checks for grad and tensor when the tensor has\n         python key set on it (tensor subclass).\n\nTest Plan: Imported from OSS\n\nReviewed By: jbschlosser\n\nDifferential Revision: D31117318\n\nPulled By: anjali411\n\nfbshipit-source-id: 825401df98695c48bf9b320be54585f6aff500bd", "pr_number": "65235", "files_changed": ["test/test_python_dispatch.py", "torch/csrc/autograd/engine.cpp", "torch/csrc/autograd/function.h", "torch/csrc/autograd/input_metadata.h", "torch/csrc/autograd/variable.cpp"], "labels": ["Merged", "cla signed"]}, "70a545b21e": {"title": "Add Tensor._make_wrapper_subclass (#65340)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65340\n\nI thought about a few possible ways of doing this.  The main hazard is\nthat if I create a CPU tensor that doesn't have any real storage, the\nmoment I actually try to access the data on the tensor I will segfault.\nSo I don't want to use _make_subclass on a \"cpu meta tensor\" because\nthe CPU meta tensor (with no subclass) is radioactive: printing it\nwill immediately cause a segfault.  So instead, I have to create\nthe CPU meta tensor AND subclass all in one go, and that means I need\nanother function for it.  One downside to doing it this way is\nI need another overload for explicit strides, and in general it is\ndifficult to get the view relationships to all work out properly;\ntracked at https://github.com/pytorch/pytorch/issues/65339\n\nFixes https://github.com/pytorch/pytorch/issues/62972\nFixes https://github.com/pytorch/pytorch/issues/62730\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nReviewed By: albanD\n\nDifferential Revision: D31057231\n\nPulled By: ezyang\n\nfbshipit-source-id: 73522769e093ae8a1bf0c7f7e594659bfb827b28", "pr_number": "65340", "files_changed": ["test/test_python_dispatch.py", "torch/csrc/DynamicTypes.cpp", "torch/csrc/autograd/python_variable.cpp", "torch/overrides.py"], "labels": ["Merged", "cla signed", "ciflow/default"]}, "0fe86ac6c6": {"title": "Fix torch.any documentation (#65310)", "body": "Summary:\nCurrently, the description of torch.any would be parsed like\n\n```\nparam input\nthe input tensor.\n```\n\nHowever, it should be\n\n```\nTests if any element in input evaluates to True.\n```\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65310\n\nReviewed By: ezyang\n\nDifferential Revision: D31102918\n\nPulled By: soulitzer\n\nfbshipit-source-id: 678ade20ba16ad2643639fbd2420c8b36fcd8bd7", "pr_number": "65310", "files_changed": ["torch/_torch_docs.py"], "labels": ["open source", "Merged", "cla signed"]}, "cbc3db8274": {"title": "Create test for builtin tensorrt module in torch deploy (#63819)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/63819\n\nghstack-source-id: 138521664\n\nTest Plan:\nbuck test mode/dev-nosan caffe2/torch/csrc/deploy:test_deploy_gpu\n\nbuck test mode/opt-split-dwarf caffe2/torch/csrc/deploy:test_deploy_gpu\n\nReviewed By: wconstab\n\nDifferential Revision: D30499301\n\nfbshipit-source-id: 0bc165b4ed5be28ebb0becc65f292cf26368692f", "pr_number": "63819", "files_changed": ["torch/csrc/deploy/example/generate_examples.py", "torch/csrc/deploy/example/tensorrt_example.py", "torch/csrc/deploy/test_deploy_gpu.cpp"], "labels": ["Merged", "cla signed"]}, "2898ef7549": {"title": "Minor ScanKernels.cu cleanup (#65350)", "body": "Summary:\n- Replace THCNumerics with `at::_isnan`\n- Replace `contiguous` with `expect_contiguous`\n- Don't use `contiguous` on output tensors. Instead skip the copy and\n  just create a new empty tensor.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65350\n\nReviewed By: ezyang\n\nDifferential Revision: D31103501\n\nPulled By: ngimel\n\nfbshipit-source-id: 9030869e28d6c570fad074fd0502076de8e2ab09", "pr_number": "65350", "files_changed": ["aten/src/ATen/native/cuda/ScanKernels.cu"], "labels": ["triaged", "open source", "Merged", "cla signed", "ciflow/default"]}, "32f0387ee8": {"title": "Bug in CosineAnnealingWarmRestarts in optim/lr_scheduler.py (#64758)", "body": "Summary:\n## {emoji:1f41b} Bug\n'CosineAnnealingWarmRestarts'  object has no attribute 'T_cur'.\nIn the Constructor of the CosineAnnealingWarmRestarts, we're calling the constructor of the Parent class (_LRScheduler) which inturn calls the step method of the CosineAnnealingWarmRestarts.\nThe called method tries to update the object's attribute  'T_cur' which is not defined yet. So it raises the error.\nThis only holds, when we give the value for last_epoch argument as 0 or greater than 0 to the 'CosineAnnealingWarmRestarts', while initializing the object.\n\n![Bug_in_CosineAnnealingWarmRestarts](https://user-images.githubusercontent.com/77477328/132552212-70abc8b5-0357-4c35-90a9-832648bac607.png)\n## To Reproduce\n\nSteps to reproduce the behavior:\n\n1. Give the value for the last_epoch argument as zero OR\n1. Give the value for the last_epoch argument as a Positive integer.\n\n## Expected behavior\n\nI only expected the 'CosineAnnealingWarmRestarts' object to be initialized.\n\n## Environment\n\nPyTorch version: 1.9.0+cpu\nIs debug build: False\nCUDA used to build PyTorch: None\nROCM used to build PyTorch: N/A\nOS: Ubuntu 20.04.2 LTS (x86_64)\nGCC version: (Ubuntu 9.3.0-17ubuntu1~20.04) 9.3.0\nClang version: Could not collect\nCMake version: version 3.21.2\nLibc version: glibc-2.31\nPython version: 3.8.10  [GCC 9.4.0] (64-bit runtime)\nPython platform: Linux-5.8.0-59-generic-x86_64-with-glibc2.29\nIs CUDA available: False\nCUDA runtime version: No CUDA\n\n## Additional context\nWe can able to solve this bug by moving the line 'self.T_cur = self.last_epoch' above the 'super(CosineAnnealingWarmRestarts,self).__init__()' line. Since we've initialized the \"self.T_cur\" to the object.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/64758\n\nReviewed By: ezyang\n\nDifferential Revision: D31113694\n\nPulled By: jbschlosser\n\nfbshipit-source-id: 98c0e292291775895dc3566fda011f2d6696f721", "pr_number": "64758", "files_changed": ["torch/optim/lr_scheduler.py"], "labels": ["triaged", "open source", "Merged", "cla signed"]}, "7e7be526c9": {"title": "Add TORCH_SHOW_CPP_STACKTRACES to Contributing.md (#64052)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/64052\n\nReviewed By: ezyang\n\nDifferential Revision: D31107779\n\nPulled By: Chillee\n\nfbshipit-source-id: 2ad8ad40cd48e54fe711863c3c74df884a2e2de7", "pr_number": "64052", "files_changed": ["CONTRIBUTING.md"], "labels": ["Merged", "cla signed"]}, "97b535dabd": {"title": "[PyTorch] add fastToString for infer_schema (#64823)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/64823\n\nWe seem to spend noticable time in vfprintf for this, and the number of arguments is almost always small enough to do this in just a few instructions.\nghstack-source-id: 138623354\n\nTest Plan: Profile schema parsing, saw less time in vfprintf\n\nReviewed By: ezyang, dhruvbird\n\nDifferential Revision: D30860716\n\nfbshipit-source-id: 09ef085cd6f93dc1eaa78790dde918ac60e67450", "pr_number": "64823", "files_changed": ["aten/src/ATen/core/op_registration/infer_schema.cpp"], "labels": ["Merged", "cla signed"]}, "c731be8066": {"title": "[BE] Use `DispatchKeySet` in `check_base_legacy_new` (#65535)", "body": "Summary:\nRefactor:\n```\nTORCH_CHECK ( key == a ||\n              key == b ||\n              key == c,\n              \"expected key to be in \", a, \" or \", b , \" or \", c,\n              \" but got \", key);\n```\ninto\n```\nTORCH_CHECK( key_set.has(key),\n            \"expected key to be in \", key_set,\n            \" but got \", key );\n```\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65535\n\nReviewed By: wconstab\n\nDifferential Revision: D31144239\n\nPulled By: malfet\n\nfbshipit-source-id: 68a053041a38f043e688e491889dd7ee258f3db3", "pr_number": "65535", "files_changed": ["torch/csrc/utils/tensor_new.cpp"], "labels": ["Merged", "cla signed", "ciflow/default"]}, "7e772e7685": {"title": "Update link to tutorial on defining NN modules (#65534)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/65527. Please, see my comment in the issue: https://github.com/pytorch/pytorch/issues/65527#issuecomment-925863193. The file was renamed in https://github.com/pytorch/tutorials/commit/ce58d5904c04c4be10561447e41a153f573a3f93#diff-e5ef486bd89eb38de15752211d9437953681b8caa8f44d7c86bb820d13151df2, but the link in this repository was not updated.\n\nIt doesn't change the fact that the old link is still working, but I guess this has to be fixed in [pytorch/tutorials](https://github.com/pytorch/tutorials) instead of here.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65534\n\nReviewed By: soulitzer\n\nDifferential Revision: D31144269\n\nPulled By: H-Huang\n\nfbshipit-source-id: f70744a21113b7dc84510e2992d87f0fed793985", "pr_number": "65534", "files_changed": ["docs/source/notes/modules.rst"], "labels": ["open source", "Merged", "cla signed"]}, "1f0f246fe2": {"title": "Automated submodule update: FBGEMM (#65360)", "body": "Summary:\nThis is an automated pull request to update the first-party submodule for [pytorch/FBGEMM](https://github.com/pytorch/FBGEMM).\n\nNew submodule commit: https://github.com/pytorch/FBGEMM/commit/0108d4f5527ee262bc92a4949b6ec6239ded9d4d\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65360\n\nTest Plan: Ensure that CI jobs succeed on GitHub before landing.\n\nReviewed By: jspark1105\n\nDifferential Revision: D31061552\n\nfbshipit-source-id: 8bce5157a281e38cad5d5d0e9dcd123beda39735", "pr_number": "65360", "files_changed": ["third_party/fbgemm"], "labels": ["triaged", "open source", "Merged", "cla signed"]}, "36485d36b6": {"title": "Docathon: Add docs for nn.functional.*d_max_pool (#63264)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/63264\n\nAdding docs to max_pool to resolve docathon issue #60904\n\nTest Plan: Imported from OSS\n\nReviewed By: malfet\n\nDifferential Revision: D31071491\n\nPulled By: Gamrix\n\nfbshipit-source-id: f4f6ec319c62ff1dfaeed8bb6bb0464b9514a7e9", "pr_number": "63264", "files_changed": ["torch/nn/functional.py"], "labels": ["Merged", "cla signed", "hackathon"]}, "c73f0e457e": {"title": "Tensor and device is_hpu methods (#65408)", "body": "Summary:\nAdd is_hpu() methods for Aten tensor and device\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65408\n\nReviewed By: malfet\n\nDifferential Revision: D31144227\n\nPulled By: wconstab\n\nfbshipit-source-id: 115f4df4b8d54e6913dd51af7b6d4cacf6dd43c5", "pr_number": "65408", "files_changed": ["aten/src/ATen/core/TensorBase.h", "c10/core/Device.h", "c10/core/TensorImpl.h"], "labels": ["triaged", "open source", "Merged", "cla signed"]}, "963ae25e41": {"title": "Migrate THCAtomics to ATen (#65470)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/65470\n\nTest Plan: Imported from OSS\n\nReviewed By: H-Huang\n\nDifferential Revision: D31148184\n\nPulled By: ngimel\n\nfbshipit-source-id: aaac3dfb5f2c6f88e9bd922b3a56d0a16a861e17", "pr_number": "65470", "files_changed": ["aten/src/ATen/cuda/Atomic.cuh", "aten/src/ATen/cuda/CUDAApplyUtils.cuh", "aten/src/ATen/native/cuda/AdaptiveAveragePooling.cu", "aten/src/ATen/native/cuda/AdaptiveAveragePooling3d.cu", "aten/src/ATen/native/cuda/AdaptiveMaxPooling2d.cu", "aten/src/ATen/native/cuda/AdaptiveMaxPooling3d.cu", "aten/src/ATen/native/cuda/AveragePool3d.cu", "aten/src/ATen/native/cuda/DilatedMaxPool3d.cu", "aten/src/ATen/native/cuda/EmbeddingBackwardKernel.cu", "aten/src/ATen/native/cuda/EmbeddingBackwardKernel.cuh", "aten/src/ATen/native/cuda/EmbeddingBag.cu", "aten/src/ATen/native/cuda/FractionalMaxPool2d.cu", "aten/src/ATen/native/cuda/FractionalMaxPool3d.cu", "aten/src/ATen/native/cuda/FunctionOfAMatrixUtilsKernel.cu", "aten/src/ATen/native/cuda/Indexing.cu", "aten/src/ATen/native/cuda/KernelUtils.cuh", "aten/src/ATen/native/cuda/LossCTC.cu", "aten/src/ATen/native/cuda/NLLLoss2d.cu", "aten/src/ATen/native/cuda/RNN.cu", "aten/src/ATen/native/cuda/ReflectionPad.cu", "aten/src/ATen/native/cuda/ReplicationPadding.cu", "aten/src/ATen/native/cuda/ScatterGatherKernel.cu", "aten/src/ATen/native/cuda/SortingRadixSelect.cuh", "aten/src/ATen/native/cuda/SummaryOps.cu", "aten/src/ATen/native/cuda/UpSample.cuh", "aten/src/ATen/native/cuda/UpSampleLinear1d.cu", "aten/src/ATen/native/cuda/UpSampleTrilinear3d.cu", "aten/src/ATen/native/cuda/im2col.cuh", "aten/src/ATen/test/cuda_atomic_ops_test.cu", "aten/src/THC/THCAtomics.cuh"], "labels": ["module: porting", "open source", "Merged", "cla signed", "ciflow/default"]}, "8c7caedbb8": {"title": "avoid re-allocation of view_shape for every tensor in `torch.meshgrid` (#62908)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/62908\n\nReviewed By: mruberry\n\nDifferential Revision: D31064165\n\nPulled By: dagitses\n\nfbshipit-source-id: 3ddc3088e70fc8ef6dcf56ceb67fd20991169af1", "pr_number": "62908", "files_changed": ["aten/src/ATen/native/TensorShape.cpp"], "labels": ["Merged", "cla signed"]}, "d85e12a5bf": {"title": "add OpInfo for `torch.argsort` (#65454)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65454\n\nAddresses facebookresearch/functorch#103.\n\nTest Plan: Imported from OSS\n\nReviewed By: ezyang\n\nDifferential Revision: D31111700\n\nPulled By: zou3519\n\nfbshipit-source-id: ec4babd2fcdcea856ba0ee8db0fd8f42b87269f3", "pr_number": "65454", "files_changed": ["torch/testing/_internal/common_methods_invocations.py"], "labels": ["open source", "Merged", "cla signed", "module: testing", "ciflow/default", "ciflow/all"]}, "fd24e1b61f": {"title": "add `OpInfo` for `torch.repeat_interleave` (#65455)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65455\n\nAddresses facebookresearch/functorch#103.\n\nTest Plan: Imported from OSS\n\nReviewed By: ezyang\n\nDifferential Revision: D31111696\n\nPulled By: zou3519\n\nfbshipit-source-id: 4fa73708fa915cb21adbba9cb8fd2b8f75bcd3e0", "pr_number": "65455", "files_changed": ["torch/testing/_internal/common_methods_invocations.py"], "labels": ["open source", "Merged", "cla signed", "module: testing", "ciflow/default", "ciflow/all"]}, "e742839f0e": {"title": "Fix autograd engine test in python_dispatch (#65567)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/65567\n\nTest Plan: Imported from OSS\n\nReviewed By: soulitzer\n\nDifferential Revision: D31158090\n\nPulled By: albanD\n\nfbshipit-source-id: 651b78016ad978c7419343554ce7ceffd54aef1b", "pr_number": "65567", "files_changed": ["test/test_python_dispatch.py"], "labels": ["Merged", "cla signed"]}, "b858993c97": {"title": "Fix engine check for case where grad is a subclass (#65568)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/65568\n\nTest Plan: Imported from OSS\n\nReviewed By: soulitzer\n\nDifferential Revision: D31158089\n\nPulled By: albanD\n\nfbshipit-source-id: 2a77df9b6340107de02a043b57a36cb7ae68df34", "pr_number": "65568", "files_changed": ["test/test_python_dispatch.py", "torch/csrc/autograd/engine.cpp"], "labels": ["Merged", "cla signed"]}, "f3587f6bfa": {"title": "Remove THC ScalarConvert (#65471)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/65471\n\nTest Plan: Imported from OSS\n\nReviewed By: H-Huang\n\nDifferential Revision: D31148182\n\nPulled By: ngimel\n\nfbshipit-source-id: bbf74e36a3d91a7be3e47199981440c68a2f645f", "pr_number": "65471", "files_changed": ["aten/src/ATen/native/cuda/AdaptiveAveragePooling.cu", "aten/src/ATen/native/cuda/AveragePool2d.cu", "aten/src/ATen/native/cuda/AveragePool3d.cu", "aten/src/ATen/native/cuda/DilatedMaxPool2d.cu", "aten/src/ATen/native/cuda/EmbeddingBag.cu", "aten/src/ATen/native/cuda/NaiveDilatedConvolution.cu", "aten/src/ATen/native/cuda/RNN.cu", "aten/src/ATen/native/cuda/SortUtils.cuh", "aten/src/ATen/native/cuda/TensorTopK.cu", "aten/src/ATen/native/cuda/WeightNorm.cu", "aten/src/ATen/native/cuda/im2col.cuh", "aten/src/THC/THCNumerics.cuh"], "labels": ["module: porting", "open source", "Merged", "cla signed", "ciflow/default"]}, "760aefd34d": {"title": "Fix nullptr addition (#65548)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65548\n\nFixes\ncaffe2/test:jit - test_unsupported_nn_functional_pad_circular_cpu_float32 (test_jit_fuser_te.TestNNCOpInfoCPU)\n\nTest Plan: Sandcastle\n\nReviewed By: ngimel\n\nDifferential Revision: D31148405\n\nfbshipit-source-id: 4c8c693a45229ab4e59b0b0ec5326d3ac114dbaf", "pr_number": "65548", "files_changed": ["c10/core/TensorImpl.h"], "labels": ["fb-exported", "cla signed"]}, "eca4f14b6c": {"title": "[PyTorch] Add C10_ prefix to MPARK_* macros in variant.h (#65589)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65589\n\nWithout this prefix, the include guards interfere with attempts to indirectly include both c10::variant and the original mpark variant in the same translation unit.\nghstack-source-id: 138901838\n\nTest Plan: Temporarily `#include <c10/util/variant.h>` in ivalue.h and buck build //data_preproc/preproc:preproc_adapter_utils mode/no-gpu -- this delayed D31101962 (https://github.com/pytorch/pytorch/commit/01720d6a2352fc1c16f41753c3929f9c12dac528) from fixing S244170\n\nReviewed By: bhosmer\n\nDifferential Revision: D31159414\n\nfbshipit-source-id: 234c5ed37ca853702bcdf3263e4f185b95ac1d08", "pr_number": "65589", "files_changed": ["c10/util/variant.h"], "labels": ["cla signed", "ciflow/default"]}, "640a615150": {"title": "[easy] [PyTorch Edge] Remove double pragma once directive in the generated code (#65620)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65620\n\nThis was bothering me for a while.\n\nghstack-source-id: 138914860\n\nTest Plan: Sandcastle\n\nReviewed By: beback4u\n\nDifferential Revision: D31162648\n\nfbshipit-source-id: 72c47ea34d40c772bb53da721fcb36365b5dbaf3", "pr_number": "65620", "files_changed": ["tools/lite_interpreter/gen_selected_mobile_ops_header.py"], "labels": ["cla signed", "ciflow/default"]}, "15724bcc03": {"title": "[TensorExpr] Re-enable a float16 test. (#65632)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/65632\n\nTest Plan: Imported from OSS\n\nReviewed By: huiguoo\n\nDifferential Revision: D31181798\n\nPulled By: ZolotukhinM\n\nfbshipit-source-id: 1a57d0a878d44f8b73f3c24eef7ba707ce18fb70", "pr_number": "65632", "files_changed": ["test/test_tensorexpr.py"], "labels": ["cla signed"]}, "10d0dbc6d9": {"title": "Avoid storage access for HPU tensors (#65409)", "body": "Summary:\nAdd is_hpu() methods for Aten tensor and device\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65409\n\nReviewed By: wconstab, H-Huang\n\nDifferential Revision: D31134422\n\nPulled By: malfet\n\nfbshipit-source-id: 181ebb67dce8e05a0723ef3c82f23e39228841ee", "pr_number": "65409", "files_changed": ["torch/_tensor.py"], "labels": ["triaged", "open source", "cla signed"]}, "cd2656a2e5": {"title": "[package] add some docs describing how to debug dependencies (#65704)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65704\n\nAs title.\n\nTest Plan: Imported from OSS\n\nReviewed By: tugsbayasgalan\n\nDifferential Revision: D31209866\n\nPulled By: suo\n\nfbshipit-source-id: 4c8ec1d5418ea75b71c4b9a498b86f0ef5383544", "pr_number": "65704", "files_changed": ["docs/source/package.rst"], "labels": ["cla signed", "ciflow/default"]}, "ea546e20fd": {"title": "[Reland] nn.functional.linear OpInfo (#65498)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/65498\n\nTest Plan: Imported from OSS\n\nReviewed By: gchanan\n\nDifferential Revision: D31171149\n\nPulled By: zou3519\n\nfbshipit-source-id: badb06af08a772397b0280189385723c0175200b", "pr_number": "65498", "files_changed": ["torch/testing/_internal/common_methods_invocations.py"], "labels": ["cla signed", "ci/windows", "ci/master", "ciflow/slow-gradcheck", "ciflow/default", "ciflow/all"]}, "fea32be964": {"title": "Add HPU type for check_base_legacy_new (#65410)", "body": "Summary:\nFixes #{issue number}\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65410\n\nReviewed By: H-Huang\n\nDifferential Revision: D31143754\n\nPulled By: malfet\n\nfbshipit-source-id: 32abfbae4f7c09924c7dfa16758d64a2215ec636", "pr_number": "65410", "files_changed": ["torch/csrc/utils/tensor_new.cpp"], "labels": ["triaged", "open source", "cla signed"]}, "f5b4e369f6": {"title": "Sparse SoftMax: Remove unused variables (#65539)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65539\n\nThis function doesn't directly use thrust so these are simply unused variables.\n\nTest Plan: Imported from OSS\n\nReviewed By: gchanan\n\nDifferential Revision: D31193191\n\nPulled By: malfet\n\nfbshipit-source-id: 231b6a197c9f1bd5a61e46cb858e8eedc85b2818", "pr_number": "65539", "files_changed": ["aten/src/ATen/native/sparse/cuda/SoftMax.cu"], "labels": ["open source", "cla signed", "ciflow/default"]}, "7c62b6e973": {"title": "add deepcopy support to subclasses (#65584)", "body": "Summary:\nHappy to get any feedback on how to make this code cleaner!\n\nThis:\n- Fix Tensor attribute deepcopy BC-breaking?\n- Add a test for Tensor attribute deepcopy\n- Fix subclass deepcopy\n- Moves the subclass serialization tests into their own class not to interfere with other serialization test logic\n- Add a test for subclass deepcopy\n\ncc ezyang gchanan\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65584\n\nReviewed By: gchanan\n\nDifferential Revision: D31206590\n\nPulled By: albanD\n\nfbshipit-source-id: 74a8f0767f4933b9c941fbea880a8fd1b893ea2f", "pr_number": "65584", "files_changed": ["test/test_serialization.py", "test/test_torch.py", "torch/_tensor.py"], "labels": ["module: bc-breaking", "cla signed"]}, "6a6ee92e36": {"title": "[quant] Add op benchmark for CPU FakeQuantizePerChannel with float zero_points (#65241)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/65241\n\nTest Plan: Imported from OSS\n\nReviewed By: jingsh\n\nDifferential Revision: D31150087\n\nPulled By: b-koopman\n\nfbshipit-source-id: a00d4995841eee81305d0007c908473cc3d5a727", "pr_number": "65241", "files_changed": ["benchmarks/operator_benchmark/pt/quantization_test.py"], "labels": ["cla signed"]}, "6a99053515": {"title": "Added sparse-tensor copy logic to dispatcher (#65304)", "body": "Summary:\n- Only ported copy for sparse tensor to dispatcher. Everything else is the same\n- Duplicated code for named tensor handling in sparse tensor copy\n\t- Might change it later to handle named tensors using dispatcher\n\nIssue https://github.com/pytorch/pytorch/issues/61122\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65304\n\nReviewed By: gchanan\n\nDifferential Revision: D31176720\n\nPulled By: ezyang\n\nfbshipit-source-id: 56757a3b0fb56c3d05c16dd935428a0cd91ea766", "pr_number": "65304", "files_changed": ["aten/src/ATen/native/Copy.cpp", "aten/src/ATen/native/native_functions.yaml", "aten/src/ATen/native/sparse/SparseTensor.cpp"], "labels": ["open source", "cla signed"]}, "3324bae5f1": {"title": "Remove THCTensor.cu and THCTensorCopy.cu copy (#65491)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65491\n\nThe only user of any of this code is THCStorage_copy, so I've\nmigrated that to call `Tensor.copy_` directly.\n\nTest Plan: Imported from OSS\n\nReviewed By: H-Huang\n\nDifferential Revision: D31148183\n\nPulled By: ngimel\n\nfbshipit-source-id: 92bab71306c84bc481c47a0615ebb811af2c2875", "pr_number": "65491", "files_changed": ["aten/src/THC/CMakeLists.txt", "aten/src/THC/THC.h", "aten/src/THC/THCStorageCopy.cpp", "aten/src/THC/THCStorageCopy.cu", "aten/src/THC/THCStorageCopy.h", "aten/src/THC/THCTensor.cpp", "aten/src/THC/THCTensor.cu", "aten/src/THC/THCTensorCopy.cu", "aten/src/THC/THCTensorCopy.h", "aten/src/THC/THCTensorCopy.hpp", "aten/src/THC/generic/THCStorageCopy.cpp", "aten/src/THC/generic/THCStorageCopy.cu", "aten/src/THC/generic/THCTensor.cpp", "aten/src/THC/generic/THCTensor.cu", "aten/src/THC/generic/THCTensorCopy.cu", "aten/src/THC/generic/THCTensorCopy.h"], "labels": ["module: porting", "open source", "cla signed", "ciflow/default"]}, "26e31f76b0": {"title": "`*_solve` methods: implements forward AD (#65546)", "body": "Summary:\nThis PR adds forward AD for `*_solve` methods.\nAdditionally, `cholesky_solve` gets OpInfo + a bug fix when wrong leading dimensions could be passed to LAPACK,\nand `lu_solve` gets forward AD with 2x`lu_solve` instead of 1x`lu_solve` + 2x`triangular_solve`.\n\ncc ezyang albanD zou3519 gqchen pearu nikitaved soulitzer Lezcano Varal7 jianyuh mruberry walterddr IvanYashchuk xwang233\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65546\n\nReviewed By: gchanan\n\nDifferential Revision: D31206837\n\nPulled By: albanD\n\nfbshipit-source-id: 040beda97442e7a88a9df9abc7bb18313ce55bc3", "pr_number": "65546", "files_changed": ["aten/src/ATen/native/BatchLinearAlgebra.cpp", "tools/autograd/derivatives.yaml", "torch/csrc/autograd/FunctionsManual.cpp", "torch/csrc/autograd/FunctionsManual.h", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["module: autograd", "open source", "module: linear algebra", "cla signed", "ciflow/slow-gradcheck", "ciflow/default"]}, "e155e7520f": {"title": "MaxUnpooling: parallel_for not always backed by OMP (#65655)", "body": "Summary:\nUse `c10::optional` + thread_fence  instead of `#pragma omp critical` inside max_unpooling kernels\n\nUsing any OpenMP pragma in `at::parallel_for` body is wrong, as it can\nbe implemented using native treading algorithms such as ptrheads\n\n`c10::optional` sounds like a much better approach to pair of\n`has_error` and `error_index` variables. Use `std::atomic_thread_fence` to ensure error_index value is synchronized.\n\nIt also fixes ICE reported in https://github.com/pytorch/pytorch/issues/65578\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65655\n\nReviewed By: ngimel\n\nDifferential Revision: D31206501\n\nPulled By: malfet\n\nfbshipit-source-id: 93df34530e721777b69509cd6c68f5d713fb2b2a", "pr_number": "65655", "files_changed": ["aten/src/ATen/native/cpu/MaxUnpoolKernel.cpp"], "labels": ["cla signed", "ciflow/default"]}, "87cd658c27": {"title": "Add override to virtual destructor in derived class (#65476)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65476\n\nAs suggested by `-Winconsistent-missing-destructor-override`.\n\nTest Plan: CI\n\nReviewed By: pritamdamania87\n\nDifferential Revision: D31115128\n\nfbshipit-source-id: a4e2441c13704c0c46e3e86f7886fca76c40ca39", "pr_number": "65476", "files_changed": ["c10/core/thread_pool.h"], "labels": ["fb-exported", "cla signed"]}, "c2252b3aa6": {"title": "Port `max` kernel to structured kernels. (#61449)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/61449\n\nTracking issue: #55070\n\nTest Plan: Imported from OSS\n\nReviewed By: saketh-are\n\nDifferential Revision: D29741714\n\nPulled By: bdhirsh\n\nfbshipit-source-id: 6c8c17d20f578ab0af8a969d103a19ccd8d51842", "pr_number": "61449", "files_changed": ["aten/src/ATen/native/ReduceOpsUtils.h", "aten/src/ATen/native/TensorCompare.cpp", "aten/src/ATen/native/TensorCompare.h", "aten/src/ATen/native/cpu/TensorCompareKernel.cpp", "aten/src/ATen/native/cuda/ReduceMinMaxKernel.cu", "aten/src/ATen/native/native_functions.yaml"], "labels": ["open source", "cla signed"]}, "c829cb6840": {"title": "Port `min` kernel to structured kernels. (#61450)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/61450\n\nTracking issue: #55070\n\nTest Plan: Imported from OSS\n\nReviewed By: saketh-are\n\nDifferential Revision: D29741713\n\nPulled By: bdhirsh\n\nfbshipit-source-id: 2c107752a90fd39cfb55e08aaf3541bd484a5fc3", "pr_number": "61450", "files_changed": ["aten/src/ATen/native/ReduceOps.cpp", "aten/src/ATen/native/ReduceOpsUtils.h", "aten/src/ATen/native/TensorCompare.cpp", "aten/src/ATen/native/TensorCompare.h", "aten/src/ATen/native/cpu/TensorCompareKernel.cpp", "aten/src/ATen/native/cuda/ReduceMinMaxKernel.cu", "aten/src/ATen/native/native_functions.yaml", "test/test_autograd.py", "test/test_sparse.py"], "labels": ["open source", "cla signed"]}, "a90912ecc5": {"title": "[sparsity] Remove the pack_param from the sparsifier state_dict (#65292)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65292\n\nThat was the original design, that we decided to simplify by removing the packing in the sparsifier.\nThe state of the sparsifier is saved directly, and the old behavior accidentally bled through to the current version.\nThis change removes the `_pack_params` method, and changes the state_dict to include the state directly.\nWe don't have to change the load_state_dict, as it will work with either the old or the new format.\n\nThe main reason for this PR is the simplification. The original design didn't achieve anything useful by packing the sparsification parameters.\n\nTest Plan: Imported from OSS\n\nReviewed By: gchanan\n\nDifferential Revision: D31186826\n\nPulled By: z-a-f\n\nfbshipit-source-id: 4ad72a7e669f048d2f2d269269ee11b63fa169db", "pr_number": "65292", "files_changed": ["test/ao/sparsity/test_parametrization.py", "test/ao/sparsity/test_sparsifier.py", "torch/ao/sparsity/sparsifier/base_sparsifier.py", "torch/ao/sparsity/sparsifier/utils.py"], "labels": ["cla signed"]}, "92ee5cc2e2": {"title": "[sparsity] Fix for accumulation bug in WeightNormSparsifier (#65293)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65293\n\nThis fixes a bug in the WeightNormSparsifier, where the mask is being multiplied by the newly computed mask.\nBecause the mask elements are binary 0/1, this accumulates the mask over every iteration, eventually collapsing the mask to zero.\nThis bug accidentally bled through from old versions.\n\nTest Plan: Imported from OSS\n\nReviewed By: gchanan\n\nDifferential Revision: D31186829\n\nPulled By: z-a-f\n\nfbshipit-source-id: 3f5b2c833148ab0bd8084e7410ce398f1252e65e", "pr_number": "65293", "files_changed": ["test/ao/sparsity/test_sparsifier.py", "torch/ao/sparsity/sparsifier/weight_norm_sparsifier.py"], "labels": ["cla signed"]}, "609384c056": {"title": "[sparsity][doc] Docstring for WeightNormSparsifier (#65294)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65294\n\nThis adds the docstring documentation to the WeightNormSparsifier and adds the typehints for the constructor args.\nNote, this does not require testing as only the doc is changed.\n\nTest Plan: Imported from OSS\n\nReviewed By: gchanan\n\nDifferential Revision: D31186827\n\nPulled By: z-a-f\n\nfbshipit-source-id: c5010c9bba25b074c4cc6c88f251474b758f950d", "pr_number": "65294", "files_changed": ["torch/ao/sparsity/sparsifier/weight_norm_sparsifier.py"], "labels": ["cla signed"]}, "0d7036fdaf": {"title": "don't leak build time path name to runtime for frozen python modules (#65715)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65715\n\nHere is how we freeze a python module:\n- we call python builtin compile method with the source code of the modules and the path. This method returns a python code object\n- we call marshal.dumps to serialize the code object to bytes.\n\nThe code_object.co_filename actually matches the one passed in to the compile method. We can simply replace that with a marker\nto avoid leak build time path to runtime.\n\nThis works on nested code objects as well:\n```\n#!/bin/env python3.8\nimport marshal\n\ncode_str = \"\"\"\nprint(\"hello\")\n\nclass MyCls:\n    def __init__(self):\n        pass\n\"\"\"\nco = compile(code_str, \"<Generated by torch::deploy>\", \"exec\")\ncobytes = marshal.dumps(co)\nimport pdb; pdb.set_trace()\n```\n\nChecking `co`:\n```\n(Pdb) co.co_filename\n'<Generated by torch::deploy>'\n(Pdb) co.co_consts\n('hello', <code object MyCls at 0x7f0e8670bbe0, file \"<Generated by torch::deploy>\", line 4>, 'MyCls', None)\n(Pdb) co.co_consts[1].co_filename\n'<Generated by torch::deploy>'\n```\n\nTest Plan:\nFind the serialized frozenmodule for torch.nn.modules.linear module in the generated bytecode_x.c file. Put the content to /tmp/linear.bytecode\n\nRun the testing script:\n```\nimport marshal\nco_bytes = bytes(eval(\"[{}]\".format(\"\".join(open('/tmp/linear.bytecode').readlines()).replace('\\n', '').replace('\\t', ''))))\nco = marshal.loads(co_bytes)\nprint(co)\n\n```\n\nThe output for the paste without the change:\n```\n<code object <module> at 0x7f39ca7f07c0, file \"/data/users/shunting/fbsource/fbcode/buck-out/opt/gen/caffe2/gen_frozen_torchpython_src__srcs/torch/nn/modules/linear.py\", line 1>\n```\n\nThe output for the paste with the change:\n```\n<code object <module> at 0x7f05a765d710, file \"<Generated by torch::deploy>\", line 1>\n````\n\nNote that the file part is changed as expected.\n\nReviewed By: suo\n\nDifferential Revision: D31214555\n\nfbshipit-source-id: 56958e0a7352f8c30a3377f83209efe7db61f0fb", "pr_number": "65715", "files_changed": ["test/test_deploy.py", "torch/csrc/deploy/interpreter/CMakeLists.txt", "torch/csrc/deploy/interpreter/freeze.py", "torch/utils/_freeze.py"], "labels": ["fb-exported", "cla signed"]}, "8a247fb418": {"title": "LLVM-12 fix for shm_mutex (#65781)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65781\n\nFixes\n```\nstderr: In file included from caffe2/caffe2/contrib/shm_mutex/shm_mutex.cc:1:\ncaffe2/caffe2/contrib/shm_mutex/shm_mutex.h:334:28: error: anonymous non-C-compatible type given name for linkage purposes by alias declaration; add a tag name here [-Werror,-Wnon-c-typedef-for-linkage]\nusing TicketStruct = struct : ShmBaseHeader {\n                           ^\n                            TicketStruct\ncaffe2/caffe2/contrib/shm_mutex/shm_mutex.h:334:31: note: type is not C-compatible due to this base class\nusing TicketStruct = struct : ShmBaseHeader {\n                              ^~~~~~~~~~~~~\ncaffe2/caffe2/contrib/shm_mutex/shm_mutex.h:334:7: note: type is given name 'TicketStruct' for linkage purposes by this alias declaration\nusing TicketStruct = struct : ShmBaseHeader {\n      ^\n1 error generated.\nCannot execute a rule out of process. On RE worker. Thread: Thread[main,5,main]\nCommand failed with exit code 1.\n```\n\nTest Plan: Sandcastle\n\nReviewed By: ngimel\n\nDifferential Revision: D31248938\n\nfbshipit-source-id: 47342fecc72ada9397a1b7bd6fcabfccf988dd3e", "pr_number": "65781", "files_changed": ["caffe2/contrib/shm_mutex/shm_mutex.h"], "labels": ["fb-exported", "cla signed"]}, "f9c2dc860d": {"title": "make layout check optional in torch.testing.assert_close() (#65419)", "body": "Summary:\nIn case the inputs have a different layout, `assert_close(..., check_layout=False)` converts them to strided before comparison. This is helpful if you just want to compare the values of sparse COO / CSR tensor against a strided reference.\n\nThis keeps BC, since the default `check_layout=True` was the old, hard-coded behavior.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65419\n\nReviewed By: H-Huang\n\nDifferential Revision: D31133629\n\nPulled By: mruberry\n\nfbshipit-source-id: ca8918af81fb0e0ba263104836a4c2eeacdfc7e6", "pr_number": "65419", "files_changed": ["test/test_testing.py", "torch/testing/_asserts.py"], "labels": ["open source", "cla signed", "module: testing", "ciflow/default"]}, "0a0564a347": {"title": "Revert D31206837: [pytorch][PR] `*_solve` methods: implements forward AD", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD31206837 (https://github.com/pytorch/pytorch/commit/26e31f76b0a6257d40b8dbcd7fe393acea49f988)\n\nOriginal commit changeset: 040beda97442\n\nfbshipit-source-id: f28091327357af9f54f367eda6606240924b93ac", "pr_number": null, "files_changed": ["aten/src/ATen/native/BatchLinearAlgebra.cpp", "tools/autograd/derivatives.yaml", "torch/csrc/autograd/FunctionsManual.cpp", "torch/csrc/autograd/FunctionsManual.h", "torch/testing/_internal/common_methods_invocations.py"], "labels": []}, "07d5d7b5cc": {"title": "move kernel launch checks from `torch.testing` to `torch.testing._internal.check_kernel_launches` (#60862)", "body": "Summary:\nThe fact that these functions are only used in a single test might be a good enough reason to move them to that module.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/60862\n\nReviewed By: H-Huang\n\nDifferential Revision: D31141354\n\nPulled By: mruberry\n\nfbshipit-source-id: 6ce1f721b88620c5f46222ad1b942bc689f0a3e0", "pr_number": "60862", "files_changed": ["test/test_kernel_launch_checks.py", "torch/testing/__init__.py", "torch/testing/_check_kernel_launches.py", "torch/testing/_internal/check_kernel_launches.py"], "labels": ["open source", "cla signed", "module: testing", "ciflow/default"]}, "f63150fd1d": {"title": "[PyTorch Edge] Reduce the cost of computing isIncludedInAlias() (#65735)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65735\n\nCurrently, `isIncludedInAlias()` calls `getRuntimeDispatchKeySet()` which creates a new `DispatchKeySet` object from an enumerated list of dispatch keys. `isIncludedInAlias()` then checks if a single dispatch key is part of this set. Instead, just pass in the key one wishes to check. This is marginally faster.\n\nghstack-source-id: 139281528\n\nTest Plan:\nSee these 2 AI Bench Runs on the Milan-FFF-11-30 device.\n\n### Before\n[AI Bench](https://www.internalfb.com/intern/aibench/details/237302972704466), [Flamegraph](https://interncache-all.fbcdn.net/manifold/aibench/tree/mobile/pt/profiling_reports/speech_transducer_v25_perf_1632804218329.html)\n\n### After\n[AI Bench](https://www.internalfb.com/intern/aibench/details/606320012968375), [Flamegraph](https://interncache-all.fbcdn.net/manifold/aibench/tree/mobile/pt/profiling_reports/speech_transducer_v25_perf_1632807348803.html)\n\nCheck the the flamegraphs, and focus on any kernel registration code path during library initialization.\n\nReviewed By: swolchok\n\nDifferential Revision: D31228062\n\nfbshipit-source-id: 7a986e3593c30ded7919cd3b564ec579dc97ab5f", "pr_number": "65735", "files_changed": ["aten/src/ATen/test/CMakeLists.txt", "aten/src/ATen/test/dispatch_key_set_test.cpp", "c10/core/DispatchKeySet.cpp", "c10/core/DispatchKeySet.h"], "labels": ["cla signed", "ciflow/default"]}, "1d681c1ab2": {"title": "Migrate THCThrustAllocator to ATen (#65492)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/65492\n\nTest Plan: Imported from OSS\n\nReviewed By: H-Huang\n\nDifferential Revision: D31148180\n\nPulled By: ngimel\n\nfbshipit-source-id: d5e4902036493517ca97c3442713b5e0e79229f9", "pr_number": "65492", "files_changed": ["aten/src/ATen/cuda/ThrustAllocator.h", "aten/src/ATen/native/cuda/EmbeddingBackwardKernel.cu", "aten/src/ATen/native/cuda/LegacyThrustHelpers.cu", "aten/src/ATen/native/cuda/SoftMax.cu", "aten/src/ATen/native/cuda/SpectralOps.cu", "aten/src/ATen/native/cuda/TensorModeKernel.cu", "aten/src/ATen/native/cuda/Unique.cu", "aten/src/ATen/native/cudnn/Conv_v7.cpp", "aten/src/ATen/native/miopen/Conv_miopen.cpp", "aten/src/ATen/native/sparse/cuda/SoftMax.cu", "aten/src/ATen/native/sparse/cuda/SparseCUDATensor.cu", "aten/src/ATen/native/sparse/cuda/SparseCUDATensorMath.cu", "aten/src/ATen/native/sparse/cuda/SparseCsrTensorMath.cu", "aten/src/ATen/native/sparse/cuda/SparseMatMul.cu", "aten/src/THC/CMakeLists.txt", "aten/src/THC/THCGeneral.cpp", "aten/src/THC/THCGeneral.h.in", "aten/src/THC/THCStorage.cu", "aten/src/THC/THCThrustAllocator.cuh", "aten/src/THC/generic/THCStorage.cu"], "labels": ["module: porting", "open source", "cla signed", "ciflow/default"]}, "2670cacfc2": {"title": "LLVM-12 fix for tensor_new.cpp (#65785)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65785\n\nFixes offset to nullptr at fbcode/caffe2/torch/csrc/utils/tensor_new.cpp:206\n\nTest Plan: Sandcastle\n\nReviewed By: ngimel\n\nDifferential Revision: D31250995\n\nfbshipit-source-id: 56c7761787e732180a2537a8aa4346a39e7399a8", "pr_number": "65785", "files_changed": ["torch/csrc/utils/tensor_new.cpp"], "labels": ["fb-exported", "cla signed"]}, "9b40eaaaab": {"title": "Revert D31193205: [pytorch][PR] CMake: Limit python include directories to only python libraries", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD31193205 (https://github.com/pytorch/pytorch/commit/971c57f1d094806eab5340263dd59dd817267994)\n\nOriginal commit changeset: 5c1b554a59d0\n\nfbshipit-source-id: 5719b7df987ded6e7e212749a438db947656df87", "pr_number": null, "files_changed": ["caffe2/CMakeLists.txt", "cmake/Dependencies.cmake", "torch/CMakeLists.txt"], "labels": []}, "6c2f235d36": {"title": "common_utils.py: Add ASAN as a platform for which you can disable tests (#65791)", "body": "Summary:\nCould be useful for the future.\n\nNext steps: document it.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65791\n\nReviewed By: suo\n\nDifferential Revision: D31254115\n\nPulled By: janeyx99\n\nfbshipit-source-id: 715c18b4505f2be6328aa0be25976116d6956b25", "pr_number": "65791", "files_changed": ["torch/testing/_internal/common_utils.py"], "labels": ["cla signed", "ciflow/default"]}, "a84feeeade": {"title": "[PyTorch Edge] Conditionally trim dispatch key set to save heap memory at runtime (#65732)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65732\n\nFor certain on-device uses, runtime memory comes at a premium. On-device deployments won't use all the available dispatch keys, so it makes sense to keep only the on-device specific ones around for such uses to reduce runtime heap memory allocated.\n\nThis change keeps just 10 dispatch keys (the ones that used on-device), guarded under the `C10_MOBILE_TRIM_DISPATCH_KEYS` macro. it tries to keep the other code-paths unaffected and uses `constexpr` for use in the `array` declaration, and simple inline functions to ensure that the compiler is able to optimize these for server builds.\n\nTest Plan:\nBuild and check mobile models end to end.\n\n```\nbuck build -c \"pt.enable_milan_dispatch_keys_trimming\"=1 //xplat/caffe2/fb/lite_predictor:lite_predictor\n```\n\nReviewed By: ezyang\n\nDifferential Revision: D31185407\n\nfbshipit-source-id: e954765606373dea6ee9466a851dca7684167b0b", "pr_number": "65732", "files_changed": ["CMakeLists.txt", "aten/src/ATen/core/dispatch/OperatorEntry.cpp", "aten/src/ATen/core/dispatch/OperatorEntry.h", "c10/core/DispatchKey.h"], "labels": ["cla signed", "ciflow/default"]}, "0dd1b74a5b": {"title": "Migrate THCScanUtils to ATen (#65743)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/65743\n\nTest Plan: Imported from OSS\n\nReviewed By: navahgar\n\nDifferential Revision: D31257938\n\nfbshipit-source-id: 273b22df41bb7f2a0ab605ec1f6322c2937e7472", "pr_number": "65743", "files_changed": ["aten/src/ATen/cuda/AsmUtils.cuh", "aten/src/ATen/cuda/ScanUtils.cuh", "aten/src/ATen/native/cuda/SortingRadixSelect.cuh", "aten/src/ATen/native/cuda/TensorTopK.cu", "aten/src/THC/CMakeLists.txt", "aten/src/THC/THCAsmUtils.cuh", "aten/src/THC/THCScanUtils.cuh"], "labels": ["module: porting", "open source", "cla signed", "ciflow/default"]}, "20374c991b": {"title": "slow_conv2d_forward: avoid calling dispatcher in parallel region (#65724)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65724\n\nSee gh-56794\n\nAvoid dispatch inside of parallel_for by:\n1. Replacing Tensor slicing with TensorAccessor\n2. Copy bias into output only once, outside of the parallel region\n3. Replaces `addmm`_ with a direct call to gemm.\n\nTechnically this also adds a new requirement that the output always be\ncontiguous, but the out argument version isn't exposed or used\nanywhere in the `torch.nn` API. So that should be fine.\n\nTest Plan: Imported from OSS\n\nReviewed By: saketh-are\n\nDifferential Revision: D31257875\n\nPulled By: ngimel\n\nfbshipit-source-id: 84d2b39e7f65334bdfcc2c4719f93ee3c514ca32", "pr_number": "65724", "files_changed": ["aten/src/ATen/native/ConvolutionMM2d.cpp", "aten/src/ATen/native/Unfold2d.h", "aten/src/ATen/native/cpu/Unfold2d.cpp"], "labels": ["open source", "cla signed", "ciflow/default"]}, "ad85b582da": {"title": "Remove THCDeviceTensor (#65744)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65744\n\nThis is just dead code.\n\nTest Plan: Imported from OSS\n\nReviewed By: navahgar\n\nDifferential Revision: D31257940\n\nfbshipit-source-id: 6c02264106c2dcbadd332f24b95bc9351a04fd9e", "pr_number": "65744", "files_changed": ["aten/src/THC/CMakeLists.txt", "aten/src/THC/THCDeviceTensor-inl.cuh", "aten/src/THC/THCDeviceTensor.cuh", "aten/src/THC/THCDeviceTensorUtils-inl.cuh", "aten/src/THC/THCDeviceTensorUtils.cuh"], "labels": ["module: porting", "open source", "cla signed", "ciflow/default"]}, "91611fe1d1": {"title": "Decouple forward AD checks from backward AD in OpInfo tests and gradcheck (#65040)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/64999\n\n- Adds a flag to gradcheck `check_backward_ad` that can be used to disable gradcheck for backward ad\n  - This is a bit bc-breaking in terms of positional args, but I prefer this ordering\n- In OpInfo tests for forward ad:\n  - set `check_backward_ad` False\n- In test_ops treat `supports_autograd` as if it is `supports_backward_ad` (it basically already is)\n  - the only modification needed is to no longer skip forward ad tests if `supports_autograd` is false\n  - test_dtype, test_variant_consistency, etc behave correctly as-is\n  - In a follow-up PR, we can rename it to actually be `supports_backward_ad`\n- Testing\n  - https://github.com/pytorch/pytorch/pull/65060\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65040\n\nReviewed By: albanD\n\nDifferential Revision: D31238177\n\nPulled By: soulitzer\n\nfbshipit-source-id: f068d4cbe7ffb094930b16cddb210583b9b7b2c4", "pr_number": "65040", "files_changed": ["test/test_autograd.py", "test/test_ops.py", "torch/autograd/gradcheck.py", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["cla signed"]}, "c7ef620a14": {"title": "[quant] Add imports to the torch/ao/quantization/__init__.py (#64911)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/64911\n\nThe import statements that involve the `quantize.py` were not added to the module level __init__ file. Those imports are necessary to mimic the behavior of the old import locations. Otherwise, the user would need to change their import statements to `from torch.ao.quantization.quantize import quantize` (instead of `from torch.ao.quantization import quantize`.\n\nAnother change in this diff is that we don't use `__all__` anymore. The all dunder was never used in quantization anyway, and just creates a potential bug when using `from ... import *`.\nghstack-source-id: 139342483\n\nTest Plan: `buck test mode/dev //caffe2/test:quantization`\n\nReviewed By: vkuzo\n\nDifferential Revision: D30897663\n\nfbshipit-source-id: a7b4919a191755e3ba690a79ce3362889f416689", "pr_number": "64911", "files_changed": ["torch/ao/quantization/__init__.py"], "labels": ["cla signed"]}, "5349ea921b": {"title": "Migrate THCIntegerDivider.cuh to ATen (#65745)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/65745\n\nTest Plan: Imported from OSS\n\nReviewed By: navahgar\n\nDifferential Revision: D31257937\n\nfbshipit-source-id: 283693525859b7a77a116df0c227653763911a42", "pr_number": "65745", "files_changed": ["aten/src/ATen/cuda/detail/IntegerDivider.cuh", "aten/src/ATen/cuda/detail/OffsetCalculator.cuh", "aten/src/ATen/native/cuda/SpectralOps.cu", "aten/src/ATen/test/cuda_integer_divider_test.cu", "aten/src/THC/CMakeLists.txt", "aten/src/THC/THCIntegerDivider.cuh"], "labels": ["module: porting", "open source", "cla signed", "ciflow/default"]}, "2c29ec2a41": {"title": "Remove \"SciPioneer\" from PT Distributed code owners (#65862)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65862\n\nghstack-source-id: 139378782\n\nTest Plan: N/A\n\nReviewed By: rohan-varma\n\nDifferential Revision: D31291340\n\nfbshipit-source-id: 65d6a82c57dd50d8a4241e9442d73002590989d9", "pr_number": "65862", "files_changed": ["CODEOWNERS"], "labels": ["cla signed"]}, "ea776fa034": {"title": "Update CODEOWNERS for optim (#65773)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/65773\n\nReviewed By: mrshenli\n\nDifferential Revision: D31269749\n\nPulled By: albanD\n\nfbshipit-source-id: 1ec35d2396797b8e97a7122e2b3a9021f8fcf0a0", "pr_number": "65773", "files_changed": ["CODEOWNERS"], "labels": ["cla signed"]}, "541eb1db63": {"title": "Add cuSPARSE descriptors and update CSR addmm (#60838)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/60838\n\nRewrote `addmm_out_sparse_csr_dense_cuda` implementation using new cusparse descriptors.\n\n`addmm` now works without conversions with both 32-bit and 64-bit indices.\nThe dense tensors can have a row- or column-major layout. If the dense tensors are a contiguous slice of a larger tensor, the storage is used directly without temporary copies.\n\nTest Plan: Imported from OSS\n\nReviewed By: pbelevich\n\nDifferential Revision: D30643191\n\nPulled By: cpuhrsch\n\nfbshipit-source-id: 5555f5b59b288daa3a3987d322a93dada63b46c8", "pr_number": "60838", "files_changed": ["BUILD.bazel", "aten/src/ATen/cuda/CUDADataType.h", "aten/src/ATen/cuda/CUDASparse.h", "aten/src/ATen/cuda/CUDASparseDescriptors.cpp", "aten/src/ATen/cuda/CUDASparseDescriptors.h", "aten/src/ATen/cuda/CuSparseHandlePool.cpp", "aten/src/ATen/native/LinearAlgebraUtils.h", "aten/src/ATen/native/sparse/cuda/SparseBlasImpl.cpp", "aten/src/ATen/native/sparse/cuda/SparseBlasImpl.h", "aten/src/ATen/native/sparse/cuda/SparseCsrTensorMath.cu", "test/test_sparse_csr.py", "torch/testing/_internal/common_cuda.py"], "labels": ["module: sparse", "open source", "cla signed", "ciflow/default", "ciflow/cuda", "ciflow/win"]}, "24f59fa20b": {"title": "[ci] fix softmax bc check (#65952)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/65952\n\nTest Plan: Imported from OSS\n\nReviewed By: soulitzer\n\nDifferential Revision: D31320441\n\nPulled By: suo\n\nfbshipit-source-id: ddd2ccca523d7ed31b231d924fbd6206525f16cf", "pr_number": "65952", "files_changed": ["test/backward_compatibility/check_backward_compatibility.py"], "labels": ["ciflow/default"]}, "8f3983254b": {"title": "[MicroBench] Added a micro benchmark for prefix sum (#65790)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65790\n\nHere are the results of the benchmark:\n\n* ATen - version that calls `at::cumsum`\n* NNC - a simple prefix-sum loop implemented in NNC (not vectorized)\n* Local - a C++ implementation of the simple prefix-sum loop\n* LocalAVX2 - a vectorized C++ implementation of prefix-sum, only using AVX2\n* LocalAVX512 - a vectorized C++ implementation of prefix-sum, using AVX512.\n\nThe vectorized implementations are from the paper \"Parallel Prefix Sum with SIMD\" in ADMS' 20.\n\n```\n$ OMP_NUM_THREADS=1 ./buck-out/opt/gen/caffe2/benchmarks/cpp/tensorexpr/tensorexpr_bench --benchmark_filter=PrefixSumBench\nRun on (36 X 1601 MHz CPU s)\n2021-09-28 23:13:12\n------------------------------------------------------------------------------------------\nBenchmark                                   Time           CPU Iterations UserCounters...\n------------------------------------------------------------------------------------------\nPrefixSumBench/ATen/64                   1289 ns       1289 ns     543199 GB/s=397.069M/s\nPrefixSumBench/ATen/256                  1867 ns       1867 ns     374232 GB/s=1096.8M/s\nPrefixSumBench/ATen/1024                 4169 ns       4169 ns     167889 GB/s=1.9649G/s\nPrefixSumBench/ATen/4096                14137 ns      14136 ns      49266 GB/s=2.31806G/s\nPrefixSumBench/ATen/16384               49887 ns      49883 ns      13988 GB/s=2.6276G/s\nPrefixSumBench/ATen/65536              193742 ns     193686 ns       3628 GB/s=2.7069G/s\nPrefixSumBench/ATen/262144             764803 ns     764774 ns        917 GB/s=2.74219G/s\nPrefixSumBench/ATen/1048576           3040653 ns    3040277 ns        231 GB/s=2.75916G/s\nPrefixSumBench/Local/64                   586 ns        586 ns    1197003 GB/s=873.244M/s\nPrefixSumBench/Local/256                 1077 ns       1077 ns     646265 GB/s=1.90143G/s\nPrefixSumBench/Local/1024                3050 ns       3050 ns     229458 GB/s=2.68579G/s\nPrefixSumBench/Local/4096               11910 ns      11910 ns      58953 GB/s=2.75132G/s\nPrefixSumBench/Local/16384              43204 ns      43202 ns      16081 GB/s=3.03393G/s\nPrefixSumBench/Local/65536             167966 ns     167966 ns       4154 GB/s=3.12139G/s\nPrefixSumBench/Local/262144            667631 ns     667613 ns       1048 GB/s=3.14127G/s\nPrefixSumBench/Local/1048576          2654785 ns    2654631 ns        264 GB/s=3.15999G/s\nPrefixSumBench/NNC/64                     642 ns        642 ns    1095277 GB/s=797.442M/s\nPrefixSumBench/NNC/256                   1139 ns       1138 ns     617214 GB/s=1.799G/s\nPrefixSumBench/NNC/1024                  3103 ns       3103 ns     225531 GB/s=2.63979G/s\nPrefixSumBench/NNC/4096                 12053 ns      12052 ns      58084 GB/s=2.71883G/s\nPrefixSumBench/NNC/16384                43227 ns      43225 ns      16192 GB/s=3.03231G/s\nPrefixSumBench/NNC/65536               168065 ns     168056 ns       4153 GB/s=3.11972G/s\nPrefixSumBench/NNC/262144              668974 ns     668921 ns       1045 GB/s=3.13513G/s\nPrefixSumBench/NNC/1048576            2657464 ns    2657341 ns        263 GB/s=3.15677G/s\nPrefixSumBench/LocalAVX2/64               523 ns        523 ns    1351308 GB/s=979.537M/s\nPrefixSumBench/LocalAVX2/256              755 ns        755 ns     927762 GB/s=2.71159G/s\nPrefixSumBench/LocalAVX2/1024            1759 ns       1759 ns     400355 GB/s=4.65609G/s\nPrefixSumBench/LocalAVX2/4096            6708 ns       6706 ns     103959 GB/s=4.88649G/s\nPrefixSumBench/LocalAVX2/16384          22143 ns      22142 ns      31229 GB/s=5.91951G/s\nPrefixSumBench/LocalAVX2/65536          83649 ns      83642 ns       8350 GB/s=6.26828G/s\nPrefixSumBench/LocalAVX2/262144        330433 ns     330427 ns       2133 GB/s=6.34679G/s\nPrefixSumBench/LocalAVX2/1048576      1302301 ns    1302179 ns        537 GB/s=6.44198G/s\nPrefixSumBench/LocalAVX512/64             474 ns        474 ns    1459151 GB/s=1080.8M/s\nPrefixSumBench/LocalAVX512/256            576 ns        576 ns    1217442 GB/s=3.55524G/s\nPrefixSumBench/LocalAVX512/1024           994 ns        994 ns     703387 GB/s=8.24434G/s\nPrefixSumBench/LocalAVX512/4096          3642 ns       3641 ns     190646 GB/s=8.99857G/s\nPrefixSumBench/LocalAVX512/16384        10140 ns      10140 ns      68947 GB/s=12.9267G/s\nPrefixSumBench/LocalAVX512/65536        35739 ns      35736 ns      19567 GB/s=14.6711G/s\nPrefixSumBench/LocalAVX512/262144      156415 ns     156413 ns       4467 GB/s=13.4078G/s\nPrefixSumBench/LocalAVX512/1048576     613952 ns     613876 ns       1144 GB/s=13.665G/s\n```\n\nTest Plan: Imported from OSS\n\nReviewed By: bertmaher\n\nDifferential Revision: D31253849\n\nPulled By: navahgar\n\nfbshipit-source-id: f33e7be787c86a09e90babddd66b16e2e0777eb4", "pr_number": "65790", "files_changed": ["benchmarks/cpp/tensorexpr/CMakeLists.txt", "benchmarks/cpp/tensorexpr/bench_prefix_sum.cpp"], "labels": ["cla signed"]}, "70f9f58a71": {"title": "Add __module__ to torch.dtype.__dict__ (#65182)", "body": "Summary:\ntorch.dtype.__reduce__ returns a string, which causes Pickle to look\nup the object by module and name. In order to find the right module,\nPickle looks for __module__ on the object; if it doesn't find that, it\nfalls back to searching sys.modules.\n\nPreviously, torch.dtype instances did not have a `__module__`\nattribute, so pickling dtypes would fall back to a search of\nsys.module.\n\nInstances of normal Python objects have a `__module__` attribute\nbecause normal Python classes have a `__module__` key in their\n`__dict__`. Imitate that by populating one in `torch.dtype`.\n\nWe set the field in `tp_dict` before calling `PyType_Ready` (instead\nof afterwards) because of the doc warning against mutating a type's\ndictionary once initialized:\nhttps://docs.python.org/3/c-api/typeobj.html#c.PyTypeObject.tp_dict\n\nfixes https://github.com/pytorch/pytorch/issues/65077\n\n ---\n\nI didn't add any tests because I didn't see any obvious places with similar tests for pickling or dtype objects. Let me know if I missed the right place, or should start one.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65182\n\nReviewed By: mrshenli\n\nDifferential Revision: D31310530\n\nPulled By: ezyang\n\nfbshipit-source-id: 20cd713ce175a709d6ce47459c3891162ce29d77", "pr_number": "65182", "files_changed": ["torch/csrc/Dtype.cpp"], "labels": ["triaged", "open source", "cla signed"]}, "6285348f06": {"title": "Implement n-dimensional hermitian FFTs (#63890)", "body": "Summary:\nCloses https://github.com/pytorch/pytorch/issues/59127\n\ncc mruberry peterbell10 walterddr\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/63890\n\nReviewed By: ngimel\n\nDifferential Revision: D30761909\n\nPulled By: mruberry\n\nfbshipit-source-id: 06e1e4dc65726f35c99a74f18b9fa36eb7d694a5", "pr_number": "63890", "files_changed": ["aten/src/ATen/native/SpectralOps.cpp", "aten/src/ATen/native/native_functions.yaml", "test/test_spectral_ops.py", "torch/fft/__init__.py", "torch/overrides.py", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["triaged", "open source", "module: fft", "cla signed", "ci/master", "ciflow/slow-gradcheck", "ciflow/default"]}, "b3da2afebe": {"title": "Clarified difference in behavior of `empty_strided` and `as_strided` (#64568)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/64568\n\nFix: #64389\n\nTest Plan: Imported from OSS\n\nReviewed By: mrshenli\n\nDifferential Revision: D31299999\n\nPulled By: mruberry\n\nfbshipit-source-id: dd538ffa7cc1267ab6472806f4216b170dd0faad", "pr_number": "64568", "files_changed": ["torch/_torch_docs.py"], "labels": ["open source", "cla signed"]}, "f6dfac6974": {"title": "Migrate THCCachingHostAllocator to ATen (#65746)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65746\n\nThis also removes the cudaHostAllocator field on THCState, since there\ndoesn't seem to be an API anywhere for customizing it.\n\nTest Plan: Imported from OSS\n\nReviewed By: mrshenli\n\nDifferential Revision: D31236630\n\nPulled By: ngimel\n\nfbshipit-source-id: 2a8e756222ae70565e77f8e7139d60ec5be32276", "pr_number": "65746", "files_changed": ["BUILD.bazel", "aten/src/ATen/cuda/CachingHostAllocator.cpp", "aten/src/ATen/cuda/CachingHostAllocator.h", "aten/src/ATen/cuda/PinnedMemoryAllocator.cpp", "aten/src/ATen/cuda/PinnedMemoryAllocator.h", "aten/src/ATen/native/cuda/Copy.cu", "aten/src/THC/CMakeLists.txt", "aten/src/THC/THC.h", "aten/src/THC/THCCachingHostAllocator.cpp", "aten/src/THC/THCCachingHostAllocator.h", "aten/src/THC/THCGeneral.cpp", "aten/src/THC/THCGeneral.h.in", "aten/src/THC/THCGeneral.hpp", "torch/csrc/cuda/Module.cpp"], "labels": ["module: porting", "open source", "cla signed", "ciflow/default"]}, "08df4c2b3c": {"title": "slow_conv2d grad_input: avoid dispatch in parallel region (#65725)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65725\n\nSee gh-56794\n\nAvoid dispatch inside of parallel_for by:\n1. Replacing Tensor slicing with TensorAccessor\n2. Call `grad_input.zero_()` only once, outside of the parallel region\n3. Replace `at::mm` with a `gemm` call\n\nTest Plan: Imported from OSS\n\nReviewed By: saketh-are\n\nDifferential Revision: D31257876\n\nPulled By: ngimel\n\nfbshipit-source-id: f2902edeccd161431c1dfb1ab3e165d039ec259d", "pr_number": "65725", "files_changed": ["aten/src/ATen/native/ConvolutionMM2d.cpp"], "labels": ["open source", "cla signed", "ciflow/default"]}, "ea0de37d2e": {"title": "[PyTorch] Avoid string construction from const char* and speedup empty string creation if error messages are suppressed (#65939)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65939\n\nThis change includes 2 separate optimizations.\n\n1. Provide an overload of `debugString(const char*, ...)` in addition to `debugString(std::string, ...)` for cases where `const char*` is passed in to avoid `std::string` construction in cases where `STRIP_ERROR_MESSAGES` is also defined and the caller is passing in a `const char*`\n2. Return `std::string(\"\", 0)` instead of `\"\"` since the former triggers no call to `std::basic_string`'s constructor whereas the latter does. [Godbolt Link](https://godbolt.org/z/oTExed5h8). However, I'm surprosed by this since the man page for [std::basic_string](https://en.cppreference.com/w/cpp/string/basic_string/basic_string) clearly states that the constexpr overload is since C++20, and I am building using `-Os -std=c++17`\n\nGodbolt Screenshot:\n\n{F667311023}\n\nghstack-source-id: 139507542\n\nTest Plan:\nCI and local build via:\n\n```\nbuck build //xplat/caffe2/fb/lite_predictor:lite_predictor\n```\n\nReviewed By: swolchok\n\nDifferential Revision: D31312942\n\nfbshipit-source-id: aa24abbfe1c16419f235d037595321982614c5ea", "pr_number": "65939", "files_changed": ["aten/src/ATen/core/library.cpp"], "labels": ["cla signed", "ciflow/default"]}, "8b1aa85388": {"title": "[sparsity] Change API to take FQNs as configuration (#65296)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65296\n\nThe original API described in the https://github.com/pytorch/pytorch/issues/59835\nassumed that the per-layer configuration would take a module/layer\nreference. However, a more useful approach is to refer to the layers\nby their fully qualified names (FQN). That allows us to store the\nconfiguration in a file without serializing the models.\n\nWe define a layer's FQN as it's \"path\" within a model. For example,\nif one can refer to a model using `model.layer0.sublayerX`, the FQN\nof the sublayerX is `'layer0.sublayerX'`.\n\nTest Plan:\n```\npython test/test_ao_sparsity.py -- TestBaseSparsifier\nbuck test mode/opt //caffe2:test -- TestBaseSparsifier\n```\n\nReviewed By: gchanan\n\nDifferential Revision: D31186830\n\nPulled By: z-a-f\n\nfbshipit-source-id: d8d87f1c054e5c10d470e67837476a11e0a9b1d4", "pr_number": "65296", "files_changed": ["test/ao/sparsity/test_sparsifier.py", "torch/ao/sparsity/sparsifier/base_sparsifier.py"], "labels": ["cla signed"]}, "c27b427cd9": {"title": "[sparsity] Add m-out-of-n support in the WeightNormSparsifier (#65295)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65295\n\nThe m-out-of-n is implemented as follows:\n\n1. Compute the blocks that need to be sparsified using the weight-norm criterion\n2. Within each block below the threshold find the smallest absolute value elements\n3. Zero out only the smallest values within each block\n\nm-out-of-n describes sparsification scheme where in a block with \"n\" elements, only \"m\" of them would be zeroed-out.\nBlock sparsity, with the whole block being all zeros, is a special case of m-out-n: If m==n, the whole block is reset.\n\nThis echoes the implementation described in the https://github.com/pytorch/pytorch/issues/59835,\nas well as meets the support of the nVidia cusparselt requirements.\nTo support the CUDA sparsity (2/4), one would need to set the sparsity_level to 1.0.\nThat translates to all blocks of shape 1x4 within a tensor will sprasify with 2-out-4 scheme.\n\nTest Plan: Imported from OSS\n\nReviewed By: vkuzo\n\nDifferential Revision: D31186828\n\nPulled By: z-a-f\n\nfbshipit-source-id: 7bd3e2707915b90f4831859781fc6e25f716c618", "pr_number": "65295", "files_changed": ["test/ao/sparsity/test_sparsifier.py", "torch/ao/sparsity/sparsifier/weight_norm_sparsifier.py"], "labels": ["cla signed"]}, "dac35b3592": {"title": "pytorch quantization ao migration phase 2: torch/jit (#65829)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65829\n\nRenames `torch.quantization` to `torch.ao.quantization` in `torch/jit` folder.\n\n```\nfind caffe2/torch/jit/ -type f -name \"*.py\" -print0 | xargs -0 sed -i \"s/torch\\.quantization/torch.ao.quantization/g\"\n```\n\nTest Plan: CI\n\nReviewed By: z-a-f\n\nDifferential Revision: D31273365\n\nfbshipit-source-id: 350eb116148d91b967d428b54413caee4fd68438", "pr_number": "65829", "files_changed": ["torch/jit/_recursive.py", "torch/jit/quantized.py"], "labels": ["oncall: jit", "fb-exported", "cla signed", "ciflow/default"]}, "8595b6eeed": {"title": "Avoid UB when indexing into size-0 tensors (#65878)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65878\n\nIf we attempt to compute an offset into an empty tensor we trigger UB, since\nwe'd be adding an offset to a nullptr, which is UB\n(https://reviews.llvm.org/D67122) even if we never use the pointer.\n\nSince indexing into an empty tensor yields an empty tensor anyways, let's just\nreturn the underlying (null) data ptr in this case.\n\nghstack-source-id: 139448496\n\nTest Plan:\nr-barnes originally pointed this out to me in a failing TE fuser test:\nhttps://www.internalfb.com/intern/testinfra/diagnostics/5910974579561425.281475022329152.1632898053/\n```\nbuck test mode/dev //caffe2/test:jit -- --exact 'caffe2/test:jit - test_unsupported_nn_functional_pad_circular_cpu_float32 (test_jit_fuser_te.TestNNCOpInfoCPU)'\n```\n\nBut it turns out it's easily triggered by anything that tries to operate on a\nslice of a size-0 tensor:\n```\ndef test_pad(self):\n    F.pad(torch.ones(0, 3, 3), (1, 2), 'circular')\n\ndef test_index(self):\n    input = torch.zeros(0, 3, 3)\n    out = torch.zeros(0, 3, 6)\n    out[..., 1:4] = input[..., 0:3]\n\ndef test_add(self):\n    torch.ones(0, 2)[:, 1] + torch.ones(0, 1)\n```\n\nWhat's the right place for these sort of operator corner-case tests?  Should\nthey be/are they part of OpInfo?\n\nReviewed By: jamesr66a\n\nDifferential Revision: D31296914\n\nfbshipit-source-id: 0ef52ad311dceeed985498f8d9390bc6fbaefbfc", "pr_number": "65878", "files_changed": ["c10/core/TensorImpl.h"], "labels": ["cla signed"]}, "53c0d91db9": {"title": "Make autograd codegen for differentiable outputs safer to use (#65823)", "body": "Summary:\nThis PR adds raising an error when `len(output_differentiability) != len(outputs)`\n\nNotes in derivatives.yml tell that\n> 'output_differentiability' and value a list of the same length as the number of outputs from the forward function.\n\nbut it was not enforced in codegen leading to confusion and unexpected bugs https://github.com/pytorch/pytorch/issues/65061#issuecomment-930271126.\n\ncc ezyang albanD zou3519 gqchen pearu nikitaved soulitzer Lezcano Varal7\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65823\n\nReviewed By: mrshenli\n\nDifferential Revision: D31307312\n\nPulled By: albanD\n\nfbshipit-source-id: caeb949e9249310dffd237e77871e6d0d784e298", "pr_number": "65823", "files_changed": ["tools/autograd/derivatives.yaml", "tools/codegen/api/autograd.py"], "labels": ["module: autograd", "open source", "module: codegen", "cla signed", "ciflow/default"]}, "383c0a3858": {"title": "Fix internal assert failure for torch.all and torch.any with requires_grad=True (#65714)", "body": "Summary:\nThis PR fixes https://github.com/pytorch/pytorch/issues/58547.\nI added an OpInfo-based test that fails on master and passes with the\nproposed changes.\n\ncc ezyang albanD zou3519 gqchen pearu nikitaved soulitzer Lezcano Varal7 mruberry\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65714\n\nReviewed By: saketh-are, mruberry\n\nDifferential Revision: D31248307\n\nPulled By: albanD\n\nfbshipit-source-id: 041eaa9b744c3043f78dd8ae5f457f67c311df4f", "pr_number": "65714", "files_changed": ["test/test_ops.py", "tools/autograd/derivatives.yaml", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["module: autograd", "module: tests", "triaged", "open source", "cla signed", "ciflow/default"]}, "21eebc9fd6": {"title": "[PyTorch][easy] Use copy-and-move instead of copy-and-swap in IValue::operator= (#65826)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65826\n\nShould be marginally more efficient.\nghstack-source-id: 139315050\n\nTest Plan: CI\n\nReviewed By: ezyang\n\nDifferential Revision: D31272489\n\nfbshipit-source-id: 7c309d67a0ec0ada35a5b62497bac374538394a9", "pr_number": "65826", "files_changed": ["aten/src/ATen/core/ivalue.h"], "labels": ["cla signed", "ciflow/default"]}, "6e8ffd191e": {"title": "Fix typo in name of LayerNormBackwardCUDAKernel (#66000)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66000\n\nSaw this in nvprof and I'm just a little too nitpicky to let it slide!\nghstack-source-id: 139547271\n\nTest Plan: CI\n\nReviewed By: xiaomengy\n\nDifferential Revision: D31340262\n\nfbshipit-source-id: ab48dc99c34a74585e66800b4bbcccc6aabbaff2", "pr_number": "66000", "files_changed": ["aten/src/ATen/native/cuda/layer_norm_kernel.cu", "caffe2/operators/layer_norm_op.cu"], "labels": ["cla signed"]}, "ad889d0b5e": {"title": "Revert D30634700: [pytorch][PR] Fix typo in tensor docs", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD30634700 (https://github.com/pytorch/pytorch/commit/d937473709956ddb87257360445f9b2d0b2fec55)\n\nOriginal commit changeset: e8952be20966\n\nfbshipit-source-id: b18694e332023abcdf17ec1900b81b00d21f1014", "pr_number": null, "files_changed": ["docs/source/tensors.rst"], "labels": []}, "d9a95e66f0": {"title": "Upload test failures to RDS (#65873)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/65873\n\nTest Plan: Imported from OSS\n\nReviewed By: janeyx99\n\nDifferential Revision: D31296520\n\nPulled By: driazati\n\nfbshipit-source-id: 0bd3fb6b62e49c7177199001fda0e7b124a22ab2", "pr_number": "65873", "files_changed": ["tools/stats/print_test_stats.py"], "labels": ["cla signed", "ciflow/default"]}, "23caeb3f71": {"title": "model_dump: Add a helper to produce html with a single call (#66005)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66005\n\nghstack-source-id: 139342091\n\nTest Plan: Unit test, and used in a notebook.\n\nReviewed By: dhruvbird\n\nDifferential Revision: D31281091\n\nfbshipit-source-id: 1e4d0713b9796a3d182de9e676c3b3c3b1610d6e", "pr_number": "66005", "files_changed": ["test/test_model_dump.py", "torch/utils/model_dump/__init__.py"], "labels": ["cla signed", "ciflow/default"]}, "e1d963e8fc": {"title": "model_dump: Fix memory computation when both constants and data tensors are present (#66006)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66006\n\nPreviously, this was resulting in a key collision and a crash.\nghstack-source-id: 139342089\n\nTest Plan: Ran webdriver test locally.\n\nReviewed By: dhruvbird\n\nDifferential Revision: D31281092\n\nfbshipit-source-id: f31311726c681d6d7e0504ff8e84c888af9054f0", "pr_number": "66006", "files_changed": ["test/test_model_dump.py", "torch/utils/model_dump/code.js"], "labels": ["cla signed", "ciflow/default"]}, "10f6294281": {"title": "Fix shape inference dim_type for Clip, Mean, Div (#65996)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/65996\n\nTest Plan:\nFacebook\n```\nbuck build caffe2/caffe2/opt:bound_shape_inference_test && ./buck-out/gen/caffe2/caffe2/opt/bound_shape_inference_test --gtest_filter=*Clip*\n```\n```\nbuck build caffe2/caffe2/opt:bound_shape_inference_test && ./buck-out/gen/caffe2/caffe2/opt/bound_shape_inference_test --gtest_filter=*Div*\n```\n```\nbuck build caffe2/caffe2/opt:bound_shape_inference_test && ./buck-out/gen/caffe2/caffe2/opt/bound_shape_inference_test --gtest_filter=*Mean*\n```\n\nReviewed By: yinghai\n\nDifferential Revision: D31121298\n\nfbshipit-source-id: f366d8f4d4d0be159b62bfaafc42ca924c05e022", "pr_number": "65996", "files_changed": ["caffe2/opt/bound_shape_inference_test.cc", "caffe2/opt/bound_shape_inferencer.cc", "caffe2/opt/bound_shape_inferencer.h"], "labels": ["fb-exported", "cla signed"]}, "eb3b9fe719": {"title": "[XROS][ML] System specific adjustments for UTs to work. (#65245)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65245\n\nBuilding and running c10 and qnnpack tests on XROS.\n\nNotable changes:\n- Adding #if define(_XROS_) in few places not supported by XROS\n- Changing Threadpool to abstract class\nghstack-source-id: 139513579\n\nTest Plan: Run c10 and qnnpack tests on XROS.\n\nReviewed By: veselinp, iseeyuan\n\nDifferential Revision: D30137333\n\nfbshipit-source-id: bb6239b935187fac712834341fe5a8d3377762b1", "pr_number": "65245", "files_changed": ["c10/core/GeneratorImpl.cpp", "c10/macros/Macros.h", "c10/util/Logging.cpp", "c10/util/Type.cpp", "caffe2/utils/threadpool/ThreadPool.cc", "caffe2/utils/threadpool/ThreadPool.h", "caffe2/utils/threadpool/pthreadpool-cpp.cc", "caffe2/utils/threadpool/pthreadpool.h", "caffe2/utils/threadpool/pthreadpool_impl.cc"], "labels": ["cla signed"]}, "5ef350d7cc": {"title": "Revert D31359010: [pytorch][PR] Fix cang-tidy regressions caused by #65954", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD31359010 (https://github.com/pytorch/pytorch/commit/c269f471f45be536bdbdc41ac681ccb730877ece)\n\nOriginal commit changeset: dce4b91a9891\n\nfbshipit-source-id: 085417432b6748d3672b9b7141460f47d1c17a7f", "pr_number": null, "files_changed": ["CMakeLists.txt", "test/benchmark_utils/test_benchmark_utils.py", "torch/csrc/deploy/loader.cpp", "torch/csrc/deploy/test_deploy.cpp", "torch/utils/benchmark/utils/timeit_template.cpp", "torch/utils/benchmark/utils/valgrind_wrapper/timer_callgrind_template.cpp"], "labels": []}, "0fc6bd2e47": {"title": "[gpu ne eval] disable adam decay unit test for gpu (#66056)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66056\n\nkeep running into this unrelated failure when landing diffs regarding the gpu inference project,\ndisabling this operator unit test in gpu because it doesn't exist\n\nRuntimeError: [enforce fail at operator.cc:277] op. Cannot create operator of type 'SmartDecaySparseAdam' on the device 'CUDA'. Verify that implementation for the corresponding device exist. It might also happen if the binary is not linked with the operator implementation code. If Python frontend is used it might happen if dyndep.InitOpsLibrary call is missing. Operator def: input: \"param\" input: \"mom1\" input: \"mom2\" input: \"last_seen\" input: \"indices\" input: \"grad\" input: \"lr\" input: \"iter\" output: \"param\" output: \"mom1\" output: \"mom2\" output: \"last_seen\" name: \"\" type: \"SmartDecaySparseAdam\" arg { name: \"beta1\" f: 0 } arg { name: \"beta2\" f: 0.9 } arg { name: \"epsilon\" f: 1e-05 } device_option { device_type: 1 }\n\nhttps://www.internalfb.com/intern/testinfra/diagnostics/5910974579962988.562949996565057.1633122845/\n\nTest Plan: sandcastle\n\nReviewed By: jianyuh\n\nDifferential Revision: D31364731\n\nfbshipit-source-id: 7fbd994cbe7f6ca116f5f34506a1ed7f14759bdf", "pr_number": "66056", "files_changed": ["caffe2/python/operator_test/adam_test.py"], "labels": ["fb-exported", "cla signed", "ciflow/default"]}, "c7748fc172": {"title": "Added validation of mode parameter in AveragedModel (#65921)", "body": "Summary:\nDiscussion: https://github.com/pytorch/pytorch/pull/65495#issuecomment-930460469\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65921\n\nReviewed By: albanD\n\nDifferential Revision: D31310105\n\nPulled By: prabhat00155\n\nfbshipit-source-id: 417691832a7c793744830c11e0ce53e3972d21a3", "pr_number": "65921", "files_changed": ["torch/optim/swa_utils.py"], "labels": ["cla signed"]}, "40948a935d": {"title": "Fix LLVM-12 UB in generate_proposals_op.cc (#66009)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66009\n\nFixes\n```\ntest_trace_c10_ops (jit.test_tracer.TestTracer) ... third-party-buck/platform009/build/eigen/include/Eigen/src/Core/Block.h:374:24: runtime error: applying non-zero offset 4 to null pointer\n    #0 0x7f5228f72227 in Eigen::internal::BlockImpl_dense<Eigen::Map<Eigen::Array<float, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> >, -1, -1, false, true>::BlockImpl_dense(Eigen::Map<Eigen::Array<float, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> >&, long, long, long, long) third-party-buck/platform009/build/eigen/include/Eigen/src/Core/Block.h:374\n    #1 0x7f5228f7212c in Eigen::BlockImpl<Eigen::Map<Eigen::Array<float, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> >, -1, -1, false, Eigen::Dense>::BlockImpl(Eigen::Map<Eigen::Array<float, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> >&, long, long, long, long) third-party-buck/platform009/build/eigen/include/Eigen/src/Core/Block.h:166\n    #2 0x7f5228f720dc in Eigen::Block<Eigen::Map<Eigen::Array<float, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> >, -1, -1, false>::Block(Eigen::Map<Eigen::Array<float, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> >&, long, long, long, long) third-party-buck/platform009/build/eigen/include/Eigen/src/Core/Block.h:142\n    #3 0x7f5229b0e059 in Eigen::DenseBase<Eigen::Map<Eigen::Array<float, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> > >::FixedBlockXpr<internal::get_fixed_value<int>::value, internal::get_fixed_value<long>::value>::Type Eigen::DenseBase<Eigen::Map<Eigen::Array<float, -1, -1, 1, -1, -1>, 0, Eigen::Stride<0, 0> > >::block<int, long>(long, long, int, long) third-party-buck/platform009/build/eigen/include/Eigen/src/Core/../plugins/BlockMethods.h:98\n    #4 0x7f5229b0c5ca in caffe2::GenerateProposalsOp<caffe2::CPUContext>::RunOnDevice() caffe2/caffe2/operators/generate_proposals_op.cc:348\n```\nAlso cleans up some data type and const issues around the area.\n\nTest Plan: Sandcastle\n\nReviewed By: xush6528\n\nDifferential Revision: D31343046\n\nfbshipit-source-id: fd9096c8e47a0aad529c72fd313f64ca98dcb80b", "pr_number": "66009", "files_changed": ["caffe2/operators/generate_proposals_op.cc"], "labels": ["fb-exported", "cla signed", "ciflow/default"]}, "89ed9bdaee": {"title": "[Static Runtime] Fix bug of creating output aliases in aten::embedding_bag (#65516)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65516\n\nThis change fixes a bug that Static Runtime's `aten::embedding_bag` out variant implementation creates aliases in its managed output tensors.\n\nManaged output tensors should never be an alias with each other since writing to them can illegally overwrite others' contents unintentionally, and this exact problem was causing the bug at T97393697, causing SR to return wrong return values.\n\nThis bug is detected in inline_cvr/remote_ro by a DCHECK, `verify_no_memory_overlap` (introduced by D30211705 (https://github.com/pytorch/pytorch/commit/3fb33b38b9edfb294b32b61e6c6133822e48f215)), but wasn't found so far since our testing didn't include running the model in the debug mode. Fortunately this bug is not hitting production since the aliases outputs are not used in production.\n\nThis change fixes the root cause from `_embedding_bag_cpu_impl_out`  by replacing alias creation with copying.\n\nNote that this change also includes a fundamental change in Static Runtime's unit testing: `testStaticRuntime` exercises the given graph 3 times:\n 1. profile run\n 2. run using the profile to allocate managed tensors\n 3. reuse the managed tensors -- newly added\n\nAdding 3 reveals this bug with a new unittest `EmbeddingBagWithManagedOutput`.\n\nTest Plan:\n- Confirmed that the crash experienced by `StaticRuntime.EmbeddingBagWithManagedOutput` disappears with this change (crash paste: P459807248).\n\n- Added `StaticRuntime.EmbeddingBagWithManagedOutput` to detect the same problem in the future.\n\nReviewed By: hlu1\n\nDifferential Revision: D31104345\n\nfbshipit-source-id: 7bddf9cd82b400d18d8ce1bf15e29b815ef9ba8f", "pr_number": "65516", "files_changed": ["aten/src/ATen/native/EmbeddingBag.cpp", "benchmarks/static_runtime/test_static_runtime.cc", "benchmarks/static_runtime/test_utils.cc"], "labels": ["oncall: jit", "fb-exported", "cla signed"]}, "b6d5f1ee70": {"title": "Allow None to pass through for vmap (#65565)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65565\n\nDoes jax allow this?\n\nTest Plan: Imported from OSS\n\nReviewed By: mrshenli\n\nDifferential Revision: D31236258\n\nPulled By: soulitzer\n\nfbshipit-source-id: 80460b355fc32ecbba8151e1f3179f076a927f9d", "pr_number": "65565", "files_changed": ["torch/_vmap_internals.py"], "labels": ["cla signed", "ciflow/default"]}, "73901b099d": {"title": "Add batched_grad parameter to `autograd.grad` (#65564)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65564\n\n- wrap the call into engine with vmap if `batched_grad` is `True`\n- improves the comment on the call to engine (somewhat addressing https://github.com/pytorch/pytorch/issues/41659)\n- borrows the message from functional.jacobian's vectorized argument concerning usage of the vmap feature\n- adds basic test (further testing is done when we replace the usage in vectorized jacobian computation)\n\nTODO:\n - create an issue tracking this\n\nTest Plan: Imported from OSS\n\nReviewed By: albanD\n\nDifferential Revision: D31236259\n\nPulled By: soulitzer\n\nfbshipit-source-id: b33e6b26ea98fa9f70c44da08458fc54ba4df0f7", "pr_number": "65564", "files_changed": ["test/test_autograd.py", "torch/autograd/__init__.py"], "labels": ["cla signed", "ciflow/default"]}, "8f5631b859": {"title": "Refactor functional api vectorized jacobian to use batched grad parameter (#65566)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65566\n\nThis doesn't simplify vectorized jacobian computation, but is good to consolidate logic and helps us to test the logic\n\nTest Plan: Imported from OSS\n\nReviewed By: albanD\n\nDifferential Revision: D31236257\n\nPulled By: soulitzer\n\nfbshipit-source-id: 00ca0aa6519bed5f9ee2c7be4daa8872af5e92cd", "pr_number": "65566", "files_changed": ["torch/autograd/functional.py"], "labels": ["cla signed", "ciflow/default"]}, "df475aa1dc": {"title": "Update Vulkan runner in benchmark binary to handle non-tensor inputs (#66123)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66123\n\nSome models may take in a list of tensors as inputs, thus the bundled inputs will contain `IValues` that are of the type `c10::List`. For Vulkan models, every tensor in the `IValue` list has to be converted to a vulkan tensor first, and this case is not currently handled by the Vulkan model wrapper in the benchmark binary.\n\nThis diff introduces `IValue` type checking to the input processor of the Vulkan model wrapper, and adds support for Tensor and List types.\n\nTest Plan:\n```\n# Build the binary\ncd ~/fbsource\nbuck build -c ndk.custom_libcxx=false -c pt.enable_qpl=0 //xplat/caffe2:ptmobile_compareAndroid\\#android-arm64 --show-output\n# Push it to the device\nadb push buck-out/gen/xplat/caffe2/ptmobile_compareAndroid\\#android-arm64 /data/local/tmp/compare_models\n\n# Run the benchmark binary\nBENCH_CMD=\"/data/local/tmp/compare_models\"\nBENCH_CMD+=\" --model=$PATH_TO_MODEL\"\nBENCH_CMD+=\" --refmodel=$PATH_TO_REFERENCE_MODEL\"\nBENCH_CMD+=\" --input_type=float --input_dims=$MODEL_INPUT_SIZE\"\nBENCH_CMD+=\" --iter=100\"\nBENCH_CMD+=\" --tolerance 1e-5\"\n```\n\nReviewed By: beback4u\n\nDifferential Revision: D31276862\n\nfbshipit-source-id: 1d9abf958963da6ecad641202f0458402bee5ced", "pr_number": "66123", "files_changed": ["binaries/speed_benchmark_torch.cc"], "labels": ["fb-exported", "cla signed", "ciflow/default"]}, "aa80f05d2d": {"title": "Remove sync in Embedding caused by unique (#66091)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/66091\n\nReviewed By: albanD\n\nDifferential Revision: D31385576\n\nPulled By: ngimel\n\nfbshipit-source-id: e656d4d9c38b705c71853ca295f977d1cddc61a1", "pr_number": "66091", "files_changed": ["aten/src/ATen/native/cuda/Embedding.cu"], "labels": ["open source", "cla signed", "ciflow/default"]}, "1db78c30c9": {"title": "Fix LLVM-12 concat_split_op.h error (#66060)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66060\n\nFixes\n```\ntestTumHistoryAdditionalLaser (caffe2.caffe2.fb.layers.tests.tum_history_test.TestTumHistory) ... caffe2/caffe2/operators/concat_split_op.h:363:74: runtime error: applying non-zero offset 8 to null pointer\n    #0 0x7f8f39d29795 in caffe2::ConcatOp<caffe2::CPUContext>::RunOnDevice() caffe2/caffe2/operators/concat_split_op.h:363\n    #1 0x7f8f39c4978d in caffe2::Operator<caffe2::CPUContext>::Run(int) caffe2/caffe2/core/operator.h:987\n    #2 0x7f8f381fe9c9 in caffe2::SimpleNet::Run() caffe2/caffe2/core/net_simple.cc:67\n    #3 0x7f8f38ee488e in caffe2::Workspace::RunNet(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) caffe2/caffe2/core/workspace.cc:289\n```\n\nTest Plan: Sandcastle\n\nReviewed By: dzhulgakov, xush6528\n\nDifferential Revision: D31366205\n\nfbshipit-source-id: 566aa519677c9d371189e4b1f81d595732861efc", "pr_number": "66060", "files_changed": ["caffe2/operators/concat_split_op.h"], "labels": ["fb-exported", "cla signed", "ciflow/default"]}, "bda3230b62": {"title": "slow_conv2d grad_weight: call gemm directly (#65726)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65726\n\nThis PR isn't strictly necessary since grad_weight doesn't use\nparallel_for. However, this does reduce the function overhead and will\nmake it easier to parallelize in the future.\n\nTest Plan: Imported from OSS\n\nReviewed By: albanD\n\nDifferential Revision: D31257877\n\nPulled By: ngimel\n\nfbshipit-source-id: d8ea97cc1f43d8d9dfff355ae27c9d982838b57e", "pr_number": "65726", "files_changed": ["aten/src/ATen/native/ConvolutionMM2d.cpp"], "labels": ["open source", "cla signed", "ciflow/default"]}, "92d0b7e99c": {"title": "[deploy] fix typo in `registerModuleSource` (#66107)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66107\n\nlol\n\nTest Plan: Imported from OSS\n\nReviewed By: zdevito\n\nDifferential Revision: D31385631\n\nPulled By: suo\n\nfbshipit-source-id: a3307e2862f7951c160776eb8edb18329c937ed1", "pr_number": "66107", "files_changed": ["torch/csrc/deploy/deploy.cpp", "torch/csrc/deploy/deploy.h", "torch/csrc/deploy/test_deploy.cpp", "torch/csrc/deploy/test_deploy_python_ext.cpp"], "labels": ["cla signed", "ciflow/default"]}, "0d020effab": {"title": "[quant] Fix the parts that were missing after initial migration (#66058)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66058\n\nAfter the initial migration from `torch.quantization` to `torch.ao.quantization`, some of the files did not change.\nThis happened because the migration was done in parallel, and some of the files were landed while the others were still in the original location.\nThis is the last fix in the AO migration phase 1, which completely enables the ao.quantization namespace.\n\nTest Plan: `python test/test_quantization.py`\n\nReviewed By: vkuzo\n\nDifferential Revision: D31366066\n\nPulled By: z-a-f\n\nfbshipit-source-id: bf4a74885be89d098df2d87e685795a2a64026c5", "pr_number": "66058", "files_changed": ["torch/ao/__init__.py", "torch/ao/nn/sparse/quantized/dynamic/linear.py", "torch/ao/ns/_numeric_suite.py", "torch/ao/ns/_numeric_suite_fx.py", "torch/ao/ns/fx/graph_matcher.py", "torch/ao/ns/fx/graph_passes.py", "torch/ao/ns/fx/pattern_utils.py", "torch/ao/ns/fx/utils.py", "torch/ao/quantization/_correct_bias.py", "torch/ao/quantization/_learnable_fake_quantize.py", "torch/ao/quantization/fake_quantize.py", "torch/ao/quantization/fuse_modules.py", "torch/ao/quantization/fx/_equalize.py", "torch/ao/quantization/fx/prepare.py", "torch/ao/quantization/fx/qconfig_utils.py", "torch/ao/quantization/fx/quantization_patterns.py", "torch/ao/quantization/observer.py", "torch/ao/quantization/qconfig.py", "torch/ao/quantization/quantize.py", "torch/ao/quantization/quantize_fx.py", "torch/ao/quantization/quantize_jit.py"], "labels": ["cla signed", "ciflow/default", "fx"]}, "588c1787ba": {"title": "Update link to example pytorch/examples (#66095)", "body": "Summary:\n`https://github.com/goldsborough/examples/tree/cpp/cpp` -> `https://github.com/pytorch/examples/tree/master/cpp`\nAs C++ examples in  https://github.com/pytorch/examples are more update\n\nPartially addresses https://github.com/pytorch/pytorch/issues/65388\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66095\n\nReviewed By: janeyx99\n\nDifferential Revision: D31382888\n\nPulled By: malfet\n\nfbshipit-source-id: 8884c7795386249dea07cbe66783fa1dd963e07c", "pr_number": "66095", "files_changed": ["docs/cpp/source/frontend.rst", "docs/cpp/source/index.rst"], "labels": ["cla signed", "ciflow/default"]}, "a3bbaf227c": {"title": "Revert D31227448: [pytorch][PR] fixing sorting in stride indices", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD31227448 (https://github.com/pytorch/pytorch/commit/da0e29edd4def41c9d305468765db3d92e31419b)\n\nOriginal commit changeset: 51e3cd903757\n\nfbshipit-source-id: a752a4df70281aa0eaaeb1afdd88395b08276da8", "pr_number": null, "files_changed": ["aten/src/ATen/core/type.cpp", "aten/src/ATen/test/CMakeLists.txt", "aten/src/ATen/test/stride_properties_test.cpp"], "labels": []}, "e7747795c9": {"title": "[PyTorch Edge] Reduce dispatch table size further for a trimmed build (#66112)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66112\n\nEliminate Metal and Vulkan Dispatch Keys.\n\nTest Plan: Build + Sandcastle\n\nDifferential Revision: D31298307\n\nfbshipit-source-id: 31302fc626382db7997e5058750fa85458c9cbc1", "pr_number": "66112", "files_changed": ["c10/core/DispatchKey.h"], "labels": ["cla signed", "ciflow/default"]}, "7452b65144": {"title": "Remove unused `dump` method from VSX vec256 methods (#66085)", "body": "Summary:\nFollow up after https://github.com/pytorch/pytorch/pull/63533\n\nProbably fixes https://github.com/pytorch/pytorch/issues/65956\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66085\n\nReviewed By: ngimel\n\nDifferential Revision: D31382898\n\nPulled By: malfet\n\nfbshipit-source-id: f3d97b0f2c7f1207827773ae85e2739f1d54b9c7", "pr_number": "66085", "files_changed": ["aten/src/ATen/cpu/vec/vec256/vsx/vec256_complex_double_vsx.h", "aten/src/ATen/cpu/vec/vec256/vsx/vec256_complex_float_vsx.h", "aten/src/ATen/cpu/vec/vec256/vsx/vec256_double_vsx.h", "aten/src/ATen/cpu/vec/vec256/vsx/vec256_float_vsx.h", "aten/src/ATen/cpu/vec/vec256/vsx/vec256_quint8_vsx.h"], "labels": ["module: POWER", "cla signed", "ciflow/bazel"]}, "68555339d7": {"title": "test_utils.py: Add another retry to test_download_url_to_file (#66159)", "body": "Summary:\nFixes one of the flakiness concerns mentioned https://github.com/pytorch/pytorch/issues/65439#issuecomment-934686485\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66159\n\nReviewed By: ngimel\n\nDifferential Revision: D31406485\n\nPulled By: janeyx99\n\nfbshipit-source-id: cf7834cdab58360ecef1748075d52969de2e0778", "pr_number": "66159", "files_changed": ["test/test_utils.py"], "labels": ["cla signed", "ciflow/default"]}, "f062def486": {"title": "Revert D31260343: [pytorch][PR] Add hash and int128 utils for Lazy Tensor Core", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD31260343 (https://github.com/pytorch/pytorch/commit/e94fea08d0590e131f9762b000216b44f8c892da)\n\nOriginal commit changeset: 8bb1194188e3\n\nfbshipit-source-id: 3d0d5377d71ed928015bcb2105801be368e38cd8", "pr_number": null, "files_changed": ["BUILD.bazel", "c10/util/int128.cpp", "c10/util/int128.h", "caffe2/CMakeLists.txt", "test/cpp/lazy/CMakeLists.txt", "test/cpp/lazy/test_misc.cpp", "tools/build_variables.bzl", "torch/csrc/lazy/core/hash.cpp", "torch/csrc/lazy/core/hash.h"], "labels": []}, "eeabab03e7": {"title": "[DataParallel] Log API Usage for tracking (#66038)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66038\n\nWill help track workflows for DP deprecation. Tested via standalone DP\nscript.\n\nTest Plan: CI\n\nReviewed By: mrshenli\n\nDifferential Revision: D31356975\n\nfbshipit-source-id: c0a3ac3a1faed794e3362f3f3a19a6fb800587a7", "pr_number": "66038", "files_changed": ["torch/nn/parallel/data_parallel.py"], "labels": ["cla signed", "ciflow/default"]}, "43e26d0086": {"title": "[deploy] Improve error messaging for create_movable (#65955)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65955\n\nThis diff makes sure to give clear error message when user tries to create obj from obj that lives in different session\n\nTest Plan: buck test //caffe2/torch/csrc/deploy:test_deploy\n\nReviewed By: suo\n\nDifferential Revision: D31323045\n\nfbshipit-source-id: e7bd6f76afeb0285847bc11881185a164f80e3f0", "pr_number": "65955", "files_changed": ["torch/csrc/deploy/deploy.cpp", "torch/csrc/deploy/interpreter/interpreter_impl.h", "torch/csrc/deploy/test_deploy.cpp"], "labels": ["fb-exported", "cla signed", "ciflow/default"]}, "931352c68d": {"title": "Make handle_torch_function_no_python_arg_parser public (#66054)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66054\n\nI need this function in functorch to support the ability of custom\njitted kernels to invoke torch_function when applicable.\n\nTest Plan: functorch unit tests\n\nReviewed By: qihqi, ngimel\n\nDifferential Revision: D31416599\n\nPulled By: bertmaher\n\nfbshipit-source-id: 90b57badd6a6b9d505ebfc436869b962b55c66d7", "pr_number": "66054", "files_changed": ["torch/csrc/utils/python_arg_parser.h"], "labels": ["cla signed", "ciflow/default"]}, "747a5782e3": {"title": "[quant][fx] Don't assume bias is a keyword argument (#61647)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/61647\n\n`prepare_fx` currently assumes that bias is always a positional argument to\nconvolutions, and only a keyword argument to other functions. This happens to work\ntoday due to a quirk in how `__torch_function__` is handled for python\nfunctions but shouldn't be considered stable.\n\nInstead, we should support `bias` for both positional and keyword forms.\n\ncc jerryzh168 jianyuh raghuramank100 jamesr66a vkuzo\n\nTest Plan: Imported from OSS\n\nReviewed By: ngimel\n\nDifferential Revision: D31401360\n\nPulled By: albanD\n\nfbshipit-source-id: 1e2f53d80e2176b870f326dc498e251e2386136e", "pr_number": "61647", "files_changed": ["torch/ao/quantization/fx/prepare.py", "torch/ao/quantization/fx/quantization_patterns.py", "torch/ao/quantization/fx/utils.py"], "labels": ["oncall: quantization", "open source", "oncall: fx", "cla signed", "ciflow/default"]}, "b8e1999253": {"title": "[quant] Add op benchmark for GPU FakeQuantizePerChannel with float zero_points (#66183)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66183\n\nAdd a GPU benchmark for fakeQuant, similar to #65241\nghstack-source-id: 139810414\n\nTest Plan: https://pxl.cl/1QjJM\n\nReviewed By: b-koopman\n\nDifferential Revision: D31288158\n\nfbshipit-source-id: 65526248b5c7b70f0bc32a86b08f50b4cbc7a83d", "pr_number": "66183", "files_changed": ["benchmarks/operator_benchmark/pt/quantization_test.py"], "labels": ["cla signed", "ciflow/default"]}, "bfaaac6392": {"title": "Ignore register_rds errors (#66185)", "body": "Summary:\nNetwork communications are flaky by nature, test should be marked as\nskipped if network ops can not be completed for some reason\n\nFixes https://github.com/pytorch/pytorch/issues/66184\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66185\n\nReviewed By: seemethere\n\nDifferential Revision: D31423193\n\nPulled By: malfet\n\nfbshipit-source-id: 96c3a123c65913f44ea78b30a03e8e7eda164afe", "pr_number": "66185", "files_changed": ["test/test_import_stats.py"], "labels": ["cla signed", "ciflow/default"]}, "623ac7eabb": {"title": "slow_conv3d: Avoid dispatch in parallel region (#65737)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65737\n\nSee gh-56794\n\nAvoid dispatch inside of parallel_for by:\n- Replacing Tensor slicing with TensorAccessor\n- Copy bias into output only once, outside of the parallel region\n- Replaces `addmm_` and `baddbmm_` with direct calls to gemm.\n\nTest Plan: Imported from OSS\n\nReviewed By: albanD\n\nDifferential Revision: D31257874\n\nPulled By: ngimel\n\nfbshipit-source-id: 20b94daa13082fb1e39eaa8144bfa4c611b61bab", "pr_number": "65737", "files_changed": ["aten/src/ATen/native/CPUBlas.cpp", "aten/src/ATen/native/CPUBlas.h", "aten/src/ATen/native/ConvolutionMM3d.cpp", "aten/src/ATen/native/Unfold3d.cpp", "aten/src/ATen/native/Unfold3d.h", "aten/src/ATen/native/mkl/LinearAlgebra.cpp", "aten/src/ATen/native/mkl/LinearAlgebra.h"], "labels": ["open source", "cla signed", "ciflow/default"]}, "6d7fab5929": {"title": "[Static Runtime][easy] Clone scripts do not use aten::add (#66161)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66161\n\n`aten::add` is not guaranteed to be bit exact with the JIT interpreter. This was causing non-deterministic test failures on master.\n\nTest Plan: `buck test caffe2/benchmarks/static_runtime:static_runtime_cpptest`\n\nReviewed By: hlu1\n\nDifferential Revision: D31406764\n\nfbshipit-source-id: d968cb1bdb8f33934682ef3712a1341a3aacf18e", "pr_number": "66161", "files_changed": ["benchmarks/static_runtime/test_scripts.h"], "labels": ["fb-exported", "cla signed", "ciflow/default"]}, "90db214d4b": {"title": "support counter-based fused rowwise adagrad (#66177)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66177\n\nAs title, with additional change to enable counter for SparseAdagrad.\n\nTest Plan:\nbuck test //caffe2/caffe2/fb/net_transforms/tests:fuse_sparse_ops_test\n\nTesting with canary packages\n\nbaseline: f297789852\n\ncounter run: f297789912\n\nReviewed By: jspark1105\n\nDifferential Revision: D30903029\n\nfbshipit-source-id: 3ed89a7da409fd820fd0b44950407c20fa2018a5", "pr_number": "66177", "files_changed": ["caffe2/sgd/adagrad_op.h", "caffe2/sgd/rowwise_adagrad_fused.cc", "caffe2/sgd/rowwise_adagrad_fused.h"], "labels": ["fb-exported", "cla signed", "ciflow/default"]}, "7cc121dbcd": {"title": "slow_conv3d grad_input: Avoid dispatch in parallel region (#65757)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65757\n\nSee gh-56794\n\nAvoid dispatch inside of parallel_for by:\n- Replacing Tensor slicing with TensorAccessor\n- Replaces `bmm` and `mm` with direct calls to gemm.\n\nTest Plan: Imported from OSS\n\nReviewed By: albanD\n\nDifferential Revision: D31257878\n\nPulled By: ngimel\n\nfbshipit-source-id: e6aad2d5ae7fa432bd27af2b1a8b0dcef1fc6653", "pr_number": "65757", "files_changed": ["aten/src/ATen/native/ConvolutionMM3d.cpp"], "labels": ["open source", "cla signed", "ciflow/default"]}, "78209b93b3": {"title": "Don't build shared library for AOT Compiler (#66227)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66227\n\nBuilding a shared library for AOT Compiler is not necessary as it's included in libtorch. Also having this built as a shared library was affecting android builds and we don't need to build AOT Compiler for mobile builds\n\nBefore fix:\n```\n(pytorch)  ~/local/pytorch master\n\u2514\u2500 $ ANDROID_NDK=/opt/android_ndk/r20/ BUILD_PYTORCH_MOBILE=1 ANDROID_ABI=armeabi-v7a ./scripts/build_android.sh -DBUILD_BINARY=ON\nBuild with ANDROID_ABI[armeabi-v7a], ANDROID_NATIVE_API_LEVEL[21]\nBash: GNU bash, version 5.0.11(1)-release (x86_64-redhat-linux-gnu)\nPython: 3.9.7 (default, Sep 16 2021, 13:09:58)\n[GCC 7.5.0]\nCaffe2 path: /data/users/priyaramani/pytorch\nUsing Android NDK at /opt/android_ndk/r20/\n.\n.\nFAILED: lib/libaot_compiler.so\n: && /opt/android_ndk/r20/toolchains/llvm/prebuilt/linux-x86_64/bin/clang++ --target=armv7-none-linux-androideabi21 --gcc-toolchain=/opt/android_ndk/r20/toolchains/llvm/prebuilt/linux-x86_64 --sysroot=/opt/and\nroid_ndk/r20/toolchains/llvm/prebuilt/linux-x86_64/sysroot -fPIC -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -fno-addrsig -march=armv7-a -mt\nhumb -Wa,--noexecstack -Wformat -Werror=format-security -frtti -fexceptions  -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DUSE_KINETO -DLIBKINETO_NOCUPTI -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -\nDBUILD_LITE_INTERPRETER -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bound\ns -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -W\nno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -Wno-invalid-partial-specialization -Wno-typedef-redefinition -Wno-unknown-warning-option -Wno-unused-private-field -Wno-inconsistent-miss\ning-override -Wno-aligned-allocation-unavailable -Wno-c++14-extensions -Wno-constexpr-not-const -Wno-missing-braces -Qunused-arguments -fcolor-diagnostics -Wno-unused-but-set-variable -Wno-maybe-uninitialized\n-fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -g0 -Oz -DNDEBUG  -Wl,--exclude-libs,libgcc.a -Wl,--exclude-libs,libatomic.a -static-libstdc++ -Wl,--build-id -Wl,--warn-shared-text\nrel -Wl,--fatal-warnings -Wl,--exclude-libs,libunwind.a -Wl,--no-undefined -Qunused-arguments -Wl,-z,noexecstack  -rdynamic -shared -Wl,-soname,libaot_compiler.so -o lib/libaot_compiler.so caffe2/torch/CMakeFi\nles/aot_compiler.dir/csrc/jit/mobile/nnc/aot_compiler.cpp.o  -latomic -lm && :\ncaffe2/torch/CMakeFiles/aot_compiler.dir/csrc/jit/mobile/nnc/aot_compiler.cpp.o:aot_compiler.cpp:function at::from_blob(void*, c10::ArrayRef<long long>, c10::TensorOptions const&): error: undefined reference t\no 'at::TensorMaker::make_tensor()'\n.\n.\ncaffe2/torch/CMakeFiles/aot_compiler.dir/csrc/jit/mobile/nnc/aot_compiler.cpp.o:aot_compiler.cpp:function torch::jit::mobile::nnc::Function::Function(): error: undefined reference to 'c10::AnyType::get()'\nclang++: error: linker command failed with exit code 1 (use -v to see invocation)\n```\n\nAfter fix:\n```\n(pytorch)  ~/local/pytorch master\n\u2514\u2500 $ ANDROID_NDK=/opt/android_ndk/r20/ BUILD_PYTORCH_MOBILE=1 ANDROID_ABI=armeabi-v7a ./scripts/build_android.sh -DBUILD_BINARY=ON\nBuild with ANDROID_ABI[armeabi-v7a], ANDROID_NATIVE_API_LEVEL[21]\nBash: GNU bash, version 5.0.11(1)-release (x86_64-redhat-linux-gnu)\nPython: 3.9.7 (default, Sep 16 2021, 13:09:58)\n[GCC 7.5.0]\nCaffe2 path: /data/users/priyaramani/pytorch\nUsing Android NDK at /opt/android_ndk/r20/\n.\n.\n-- Build files have been written to: /data/users/priyaramani/pytorch/build_android\nWill install headers and libs to /data/users/priyaramani/pytorch/build_android/install for further Android project usage.\n[2/3] Install the project...\n-- Install configuration: \"Release\"\nInstallation completed, now you can copy the headers/libs from /data/users/priyaramani/pytorch/build_android/install to your Android project directory.\n```\n\nTest Plan: Imported from OSS\n\nReviewed By: ljk53, axitkhurana\n\nDifferential Revision: D31450970\n\nPulled By: priyaramani\n\nfbshipit-source-id: 87e48033f1db46fef112bae1239a09a2365620d2", "pr_number": "66227", "files_changed": ["binaries/CMakeLists.txt", "torch/CMakeLists.txt"], "labels": ["cla signed", "ciflow/default"]}, "1d586e78c6": {"title": "`*_solve` methods: implements forward AD (#65546)", "body": "Summary:\nThis PR adds forward AD for `*_solve` methods.\nAdditionally, `cholesky_solve` gets OpInfo + a bug fix when wrong leading dimensions could be passed to LAPACK,\nand `lu_solve` gets forward AD with 2x`lu_solve` instead of 1x`lu_solve` + 2x`triangular_solve`.\n\ncc ezyang albanD zou3519 gqchen pearu nikitaved soulitzer Lezcano Varal7 jianyuh mruberry walterddr IvanYashchuk xwang233\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65546\n\nReviewed By: dagitses\n\nDifferential Revision: D31431847\n\nPulled By: albanD\n\nfbshipit-source-id: 0e343e0d9da3c3d2051fca215fad289d77275251", "pr_number": "65546", "files_changed": ["aten/src/ATen/native/BatchLinearAlgebra.cpp", "tools/autograd/derivatives.yaml", "torch/csrc/autograd/FunctionsManual.cpp", "torch/csrc/autograd/FunctionsManual.h", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["module: autograd", "open source", "module: linear algebra", "cla signed", "ciflow/slow-gradcheck", "ciflow/default"]}, "6c54971cd9": {"title": "Open Registration for torch::deploy Builtins (#65953)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65953\n\nPreviously if people want to add a torch::deploy builtin, they need to change torch::deploy internal code (interpreter_impl.cpp) to register the python part as frozen modules and C++ part as builtin modules. This is not convenient and error prone. We want to add open registration support for torch::deploy builtins so that people only need to add one effective line of code in there *library code* to complete the registration.\n\nHere is an example to registry numpy as torch::deploy builtins:\n  REGISTER_TORCH_DEPLOY_BUILTIN(numpy, numpy_frozen_modules, <list of name, PyInit function pairs>)\n\nThis diff supports open registration of frozen modules. It's the first step to achieve the plan above.\nghstack-source-id: 139888306\n\nTest Plan: Run tests in test_deploy.cpp and test_builtin_registry.cpp\n\nReviewed By: suo\n\nDifferential Revision: D31321562\n\nfbshipit-source-id: 6445bd8869f1bb7126b4c96cf06c31145f0e9445", "pr_number": "65953", "files_changed": ["torch/csrc/deploy/interpreter/builtin_registry.cpp", "torch/csrc/deploy/interpreter/builtin_registry.h", "torch/csrc/deploy/interpreter/interpreter_impl.cpp", "torch/csrc/deploy/interpreter/test_builtin_registry.cpp"], "labels": ["cla signed", "ciflow/default"]}, "3f30526ff2": {"title": "Remove THCAllocator (#65942)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65942\n\nThis one is a bit weird. The class is called `THCIpcDeleter` but it\nactually has nothing IPC-specific. It just converts\n`std::shared_ptr` + `void*` into a `c10::DataPtr`. Instead, moving\nthe `DataPtr` conversion into the actual IPC code allows 2 memory\nallocations to be elided by merging 3 separate deletion contexts\ninto one.\n\nTest Plan: Imported from OSS\n\nReviewed By: dagitses\n\nDifferential Revision: D31386278\n\nPulled By: ngimel\n\nfbshipit-source-id: 5722beed9dcf680f0eb6bbff30405cff47b21962", "pr_number": "65942", "files_changed": ["aten/src/THC/CMakeLists.txt", "aten/src/THC/THC.h", "aten/src/THC/THCAllocator.cpp", "aten/src/THC/THCAllocator.h", "aten/src/THC/THCGeneral.cpp", "torch/csrc/CudaIPCTypes.h", "torch/csrc/generic/StorageSharing.cpp"], "labels": ["module: porting", "open source", "cla signed", "ciflow/default"]}, "1e4bcbdddb": {"title": "[Bootcamp][Pytorch Core] Add test for complex numbers for vanilla SGD (#66230)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66230\n\nAdding test to ensure Vanilla SGD behaves as if complex numbers are two real numbers in R^2 as per issue 65711 on github\nhttps://github.com/pytorch/pytorch/issues/65711\nghstack-source-id: 139918862\n\nTest Plan:\n```buck test mode/dev caffe2/test:optim -- 'test_sgd_complex'```\n\nhttps://pxl.cl/1QHvX\n\nReviewed By: albanD\n\nDifferential Revision: D31449289\n\nfbshipit-source-id: da8b00421085796a23b643e73f96b19b5b560a32", "pr_number": "66230", "files_changed": ["test/test_optim.py"], "labels": ["cla signed", "ciflow/default"]}, "2f1ab477f1": {"title": "Speed up DataTypeToTypeMeta (#66113)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66113\n\nFor a benchmark compiled in opt-mode in which the lookup items were shuffled and then the items were looked up round-robin fashion 10M times (for a total of 140M lookups) compiled in opt-mode we see:\n```\nFunction           Container            Time (ms) Multiplier\nTypeMetaToDataType if-chain                   233         1x\nTypeMetaToDataType std::vector                795      3.41x\nTypeMetaToDataType std::map                  1566      6.72x\nTypeMetaToDataType std::unordered_map        2136      9.17x\n\nDataTypeToTypeMeta switch                     102         1x\nDataTypeToTypeMeta std::vector                666      6.53x\nDataTypeToTypeMeta std::map                  1212      11.9x\nDataTypeToTypeMeta std::unordered_map        1539      15.1x\nDataTypeToTypeMeta folly::F14FastMap         1789      17.5x\n```\nFrom this, we draw two conclusions:\n1. Using a complex container like `std::map` is worse than using a simple vector lookup here (there aren't enough items for the Big-O to assert itself).\n2. Using any container at all is a mistake. (Unless we pull in more exotic reasoning like invalidating the code cache or preventing inlining.)\n\nTest Plan: Sandcastle\n\nReviewed By: dzhulgakov\n\nDifferential Revision: D31375117\n\nfbshipit-source-id: 0b310c6c2e94080d125c82fb7c2b43ab869adbcb", "pr_number": "66113", "files_changed": ["caffe2/core/types.cc", "caffe2/core/types.h"], "labels": ["fb-exported", "cla signed", "ciflow/default"]}, "a8c0b362ce": {"title": "[pytorch][PR] Add hash and int128 utils for Lazy Tensor Core\" (#66181)", "body": "Summary:\nThese utils are prerequisites for Lazy Node base class.\n- set up new torch/csrc/lazy, test/cpp/lazy dirs\n- add source files to build_variables.bzl in new lazy_core_sources var\n- create new test_lazy binary\n\nFixes https://github.com/pytorch/pytorch/issues/65636\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66181\n\nOriginal commit changeset: 3d0d5377d71e\n\nTest Plan:\nRun PyTorch XLA corresponding PR in XLA CI:\nhttps://github.com/pytorch/xla/pull/3148/files\n\nReviewed By: suo\n\nDifferential Revision: D31416438\n\nfbshipit-source-id: 58a6a49c5bc30134bc6bae2e42778f359b9a8f40", "pr_number": "66181", "files_changed": ["BUILD.bazel", "c10/util/int128.cpp", "c10/util/int128.h", "caffe2/CMakeLists.txt", "test/cpp/lazy/CMakeLists.txt", "test/cpp/lazy/test_misc.cpp", "tools/build_variables.bzl", "torch/csrc/lazy/core/hash.cpp", "torch/csrc/lazy/core/hash.h"], "labels": ["fb-exported", "cla signed", "ciflow/default", "ciflow/all"]}, "2e4e5b0264": {"title": "Add inplace_variant for resize_ OpInfo (#66135)", "body": "Summary:\nEnable testing of `torch.Tensor.resize_`.\nThe negative view test is skipped as the test doesn't work with resize_ see\nhttps://github.com/pytorch/pytorch/issues/65945.\n\ncc mruberry\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66135\n\nReviewed By: dagitses\n\nDifferential Revision: D31444263\n\nPulled By: mruberry\n\nfbshipit-source-id: 00c7fe05df28fba01508b31adb3ed4fdcf4d0326", "pr_number": "66135", "files_changed": ["torch/testing/_internal/common_methods_invocations.py"], "labels": ["module: tests", "triaged", "open source", "cla signed", "ciflow/default"]}, "e6a4f746c2": {"title": "slow_conv3d: Use at::sum for grad_bias accumulation (#65758)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65758\n\nThe same change has been made in conv2d, the proper algorithm is both\nfaster and gives more precision.\n\nTest Plan: Imported from OSS\n\nReviewed By: albanD\n\nDifferential Revision: D31257872\n\nPulled By: ngimel\n\nfbshipit-source-id: 6ff3a7a00a05b66f83d45cc820bd0c230cb8de6d", "pr_number": "65758", "files_changed": ["aten/src/ATen/native/ConvolutionMM3d.cpp"], "labels": ["open source", "cla signed", "ciflow/default"]}, "f445ed19b2": {"title": "OpInfo for 2d fft functions (#66128)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66128\n\ncc mruberry peterbell10\n\nTest Plan: Imported from OSS\n\nReviewed By: dagitses\n\nDifferential Revision: D31450217\n\nPulled By: mruberry\n\nfbshipit-source-id: 1952fc60c5d5f454966c43f5710b8b97a9794d0e", "pr_number": "66128", "files_changed": ["test/test_spectral_ops.py", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["open source", "module: fft", "cla signed", "ciflow/default"]}, "5e7d8ec846": {"title": "Support Registering a Variable Length List of Builtin Modules for torch::deploy Builtin Libraries (#66021)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66021\n\nA builtin library consists of a list of frozen modules and a list of builtin modules. For tensorrt, it's quite simple since we only have a single builtin module tensorrt.tensorrt. But it can be complex for libraries like numpy which contains multiple builtin modules (np.core._multiarray_umath, np.random.mtrand etc.) if we want to add it as a torch::deploy builtin. We enhance the macro that registers builtin libraries to accept a variable length of builtin modules. We can use this macro to register frozentorch, frozenpython, tensorrt for now and can also use it to register libraries like numpy later on.\n\nThe enhanced macro now looks as follows. Although we don't need to worry about back-compatibility for now,  but this enhanced version is fully compatible with the previous version. The previous version is just a special case when the library contains no builtin modules.\n\n ```\nREGISTER_TORCH_DEPLOY_BUILTIN(library_name_without_quote, frozen_modules_list,\n    builtin_module_name_1, builtin_module_init_function_1, ...,\n    builtin_module_name_N, builtin_module_init_function_N)\n```\nghstack-source-id: 140007970\n\nTest Plan:\n1. Play around with interactive_embedded_interpreter.cpp to import torch._C, tensorrt.tensorrt etc inside the embedded interpreter.\n2. Enhance test_builtin_registry.cpp\n3. Run test_deploy.cpp and test_deploy_gpu.cpp\n\nReviewed By: suo\n\nDifferential Revision: D31349390\n\nfbshipit-source-id: 70a1fcf660341180fc4d5195aed15ceb07c2bef7", "pr_number": "66021", "files_changed": ["torch/csrc/deploy/interactive_embedded_interpreter.cpp", "torch/csrc/deploy/interpreter/builtin_registry.cpp", "torch/csrc/deploy/interpreter/builtin_registry.h", "torch/csrc/deploy/interpreter/interpreter_impl.cpp", "torch/csrc/deploy/interpreter/test_builtin_registry.cpp"], "labels": ["cla signed", "ciflow/default"]}, "0e2d1b221a": {"title": "[Bootcamp][Pytorch Core] Add testing for complex non-vanilla SGD", "body": "Summary: Adding test to ensure non-Vanilla SGD behaves as if complex numbers are two real numbers in R^2 as per issue 65711 on github\n\nTest Plan:\n```buck test mode/dev caffe2/test:optim -- 'test_sgd_complex'```\n\nhttps://pxl.cl/1QLxw\n\nReviewed By: albanD\n\nDifferential Revision: D31477212\n\nfbshipit-source-id: 500678e561a05ac96759223b4c87a37cab26c6a6", "pr_number": null, "files_changed": ["test/test_optim.py"], "labels": []}, "86de09e49a": {"title": "Upgrade to ubuntu:trusty-20190515 (#63468)", "body": "Summary:\nSecurity Upgrade to ubuntu:trusty-20190515\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/63468\n\nReviewed By: ngimel\n\nDifferential Revision: D31393552\n\nPulled By: malfet\n\nfbshipit-source-id: 4e2399e3cddc1d549c08c82c08015e00569c19bc", "pr_number": "63468", "files_changed": ["caffe2/contrib/docker-ubuntu-14.04/Dockerfile"], "labels": ["triaged", "open source", "cla signed"]}, "20f2e55d4f": {"title": "Rename cuda/Resize.cu to cuda/Resize.cpp (#65943)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65943\n\nThese files don't require nvcc to compile.\n\nTest Plan: Imported from OSS\n\nReviewed By: albanD\n\nDifferential Revision: D31386277\n\nPulled By: ngimel\n\nfbshipit-source-id: 1066ee87fa795e2c7969447fbce1fe2633fb9680", "pr_number": "65943", "files_changed": ["aten/src/ATen/native/Resize.h", "aten/src/ATen/native/cuda/Normalization.cu", "aten/src/ATen/native/cuda/Resize.cpp", "aten/src/ATen/native/cuda/Resize.cu", "aten/src/ATen/native/cuda/Resize.cuh", "aten/src/ATen/native/cuda/Resize.h", "aten/src/ATen/native/cuda/TensorFactories.cu", "aten/src/ATen/native/cuda/TensorShapeCUDA.cpp", "aten/src/THC/THCTensor.cpp"], "labels": ["module: porting", "open source", "cla signed", "ciflow/default"]}, "e1817d895f": {"title": "[BE] Cleanup python_function.cpp (#66296)", "body": "Summary:\n- Delete unused `var_input_idx`\n- Fix `uninitialized variable` clang-tidy warning by setting `PyObject* input` to PyNone\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66296\n\nReviewed By: janeyx99\n\nDifferential Revision: D31491016\n\nPulled By: malfet\n\nfbshipit-source-id: 08267144be0cd049d122580cdf81cf586c3e30a6", "pr_number": "66296", "files_changed": ["torch/csrc/autograd/python_function.cpp"], "labels": ["ciflow/default"]}, "d5033410b1": {"title": "Parallel: Deduplicate parallel functions in different backends (#65326)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65326\n\nparallel_for and parallel_reduce currently share some common code in\nall backends, specifically for detecting if it should run in parallel\nor not. This moves all the backend-specific code into a single\n`internal::invoke_parallel` function and makes the `parallel_`\nfunctions common to all backends.\n\nTest Plan: Imported from OSS\n\nReviewed By: ezyang\n\nDifferential Revision: D31124495\n\nfbshipit-source-id: 65c3d2af42a8860cc4d6349566085c9fa8d8c6f0", "pr_number": "65326", "files_changed": ["aten/src/ATen/Parallel-inl.h", "aten/src/ATen/Parallel.h", "aten/src/ATen/ParallelNative.cpp", "aten/src/ATen/ParallelNative.h", "aten/src/ATen/ParallelNativeTBB.h", "aten/src/ATen/ParallelOpenMP.h"], "labels": ["open source", "cla signed", "ciflow/default", "ciflow/cpu"]}, "bd9eee4e65": {"title": "TBB: Use static partitioner to match OpenMP scheduling (#65327)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65327\n\nShould fix https://github.com/pytorch/pytorch/issues/64571\n\nTest Plan: Imported from OSS\n\nReviewed By: dagitses\n\nDifferential Revision: D31474116\n\nPulled By: malfet\n\nfbshipit-source-id: 8c4264d4778c6caf58261e3f70d72decd134128d", "pr_number": "65327", "files_changed": ["aten/src/ATen/ParallelNativeTBB.h", "test/test_tensor_creation_ops.py", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["open source", "cla signed", "ciflow/default", "ciflow/cpu"]}, "4af913a7cf": {"title": "fixed minor issues for index_add in docs (#65806)", "body": "Summary:\nHi, I'm looking forward to contributing to PyTorch, so starting with a minor fix in the documentation for `index_add`.\n\nCurrently, in the documentation for `index_add_` (please see https://pytorch.org/docs/master/generated/torch.Tensor.index_add_.html#torch.Tensor.index_add_):\n\n1. `tensor` attribute was pointing to `torch.tensor` class, which IMO - is (thought may not be a big deal) unintentional.\n2. `dim` attribute is pointing to `torch.Tensor.dim`, which again IMO - is unintentional.\n\nThis PR suggests a correction for the first point above, to rename `tensor` attribute to `input` so that it doesn't point to `torch.tensor` class. (I've verified that others ops like `scatter` use `input`, so this should not break the consistency in the documentation). I couldn't find an appropriate fix for the second point above, since renaming `dim` to something else will break the consistency (as almost all others op in PyTorch use `dim` as the attribute name).\n\nI may be wrong here, so please let me know if there is any feedback or an alternate fix for this.\n\n_Note:_ I plan to fix this behavior for `index_copy_` (https://pytorch.org/docs/master/generated/torch.Tensor.index_copy_.html#torch.Tensor.index_copy_) once and if this PR is approved.\n\nTo the reviewers, please help me tag the correct person who could help review this PR.\n\ncc: krshrimali mruberry zou3519\n\ncc brianjo mruberry\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65806\n\nReviewed By: dagitses, mruberry\n\nDifferential Revision: D31431182\n\nPulled By: zou3519\n\nfbshipit-source-id: 66ced9677ac3bc71d672d13366f9f567ecea0a2d", "pr_number": "65806", "files_changed": ["torch/_tensor_docs.py"], "labels": ["module: docs", "open source", "cla signed", "ciflow/default"]}, "8d6d448238": {"title": "Add HPU for Autograd Fallback (#65605)", "body": "Summary:\nFixes #{issue number}\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65605\n\nReviewed By: albanD\n\nDifferential Revision: D31373899\n\nPulled By: ezyang\n\nfbshipit-source-id: 894f62dc44b0532f152dc97b839eecfbaed25e8c", "pr_number": "65605", "files_changed": ["aten/src/ATen/core/VariableFallbackKernel.cpp"], "labels": ["triaged", "open source", "cla signed"]}, "dc37547c44": {"title": "Opinfos for avg_pooling (#64214)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/64214\n\nAdded OpInfos for:\n- F.adapative_avg_pool{1, 3}d\n- F.avg_pool{1, 3}d\n\nThe 2d variants already had OpInfos.\n\nTest Plan: - run tests\n\nReviewed By: albanD, mruberry\n\nDifferential Revision: D30667797\n\nPulled By: zou3519\n\nfbshipit-source-id: 53f5cd02070de5b7db4abb017d727376b59288df", "pr_number": "64214", "files_changed": ["torch/testing/_internal/common_methods_invocations.py"], "labels": ["cla signed", "ciflow/default"]}, "ece0221854": {"title": "Rename int to long, add more C++ types. (#66108)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66108\n\nBC-breaking change: intT is now longT (which aligns it more accurately with how\nthe types are referred to in C++).  The benefit for this is we can idiomatically\nexpress all C++ dtypes (with intT now mapping to int32_t).  These types are needed\nfor ufunc codegen in a latter patch.\n\nSigned-off-by: Edward Z. Yang <ezyang@fb.com>\n\nTest Plan: Imported from OSS\n\nReviewed By: albanD\n\nDifferential Revision: D31385761\n\nPulled By: ezyang\n\nfbshipit-source-id: ec6f3a0953794313470dbe14911f23ac116be425", "pr_number": "66108", "files_changed": ["tools/autograd/gen_autograd_functions.py", "tools/autograd/gen_inplace_or_view_type.py", "tools/autograd/load_derivatives.py", "tools/codegen/api/types.py"], "labels": ["cla signed", "ciflow/default"]}, "0cad2c0615": {"title": "Move intraop_launch_future from Parallel.h (#64166)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/64166\n\nTest Plan: Imported from OSS\n\nReviewed By: gchanan\n\nDifferential Revision: D30728585\n\nPulled By: dagitses\n\nfbshipit-source-id: 75a41418ae9218bec9bac27597051295222b6eee", "pr_number": "64166", "files_changed": ["aten/src/ATen/Parallel-inl.h", "aten/src/ATen/Parallel.h", "aten/src/ATen/ParallelFuture.h", "aten/src/ATen/ParallelNative.cpp", "aten/src/ATen/ParallelNative.h", "aten/src/ATen/ParallelNativeTBB.cpp", "aten/src/ATen/ParallelNativeTBB.h", "aten/src/ATen/ParallelOpenMP.cpp", "aten/src/ATen/test/test_parallel.cpp", "binaries/at_launch_benchmark.cc"], "labels": ["open source", "cla signed", "ciflow/default"]}, "201174cb91": {"title": "Revert D31389480: [pytorch][PR] Allow external CUDA streams to be set as current", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD31389480 (https://github.com/pytorch/pytorch/commit/61f0bb70c1cd63a2dd396c5774d055551914e492)\n\nOriginal commit changeset: 2b2f40e5452c\n\nfbshipit-source-id: c6631e51abcf3819732f981f646cb77b91569c7d", "pr_number": null, "files_changed": ["aten/src/ATen/test/cuda_stream_test.cpp", "c10/cuda/CUDAStream.cpp"], "labels": []}, "0020a151c6": {"title": "slow_conv3d grad_weight: call gemm directly (#65759)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/65759\n\nTest Plan: Imported from OSS\n\nReviewed By: dagitses\n\nDifferential Revision: D31257873\n\nPulled By: ngimel\n\nfbshipit-source-id: 1612c0be10b2aa269c807c7b9f5470172ed68dc1", "pr_number": "65759", "files_changed": ["aten/src/ATen/native/ConvolutionMM3d.cpp"], "labels": ["open source", "cla signed", "ciflow/default"]}, "bc1dec9b81": {"title": "Migrate THCStorage_resizeBytes to ATen (#65944)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/65944\n\nTest Plan: Imported from OSS\n\nReviewed By: albanD\n\nDifferential Revision: D31386276\n\nPulled By: ngimel\n\nfbshipit-source-id: a2b28bc09d11a856fdd3796d3df6f96613f13437", "pr_number": "65944", "files_changed": ["aten/src/ATen/native/cuda/Resize.cpp", "aten/src/ATen/native/cuda/Resize.h", "aten/src/THC/THCStorage.cpp"], "labels": ["module: porting", "open source", "cla signed", "ciflow/default"]}, "0be36d798b": {"title": "Remove Tensor.h include from TensorIterator.h (#64167)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/64167\n\nTest Plan: Imported from OSS\n\nReviewed By: saketh-are\n\nDifferential Revision: D30728579\n\nPulled By: dagitses\n\nfbshipit-source-id: 3888da00c9c8030013c8f4b39d300fe671defb05", "pr_number": "64167", "files_changed": ["aten/src/ATen/MemoryOverlap.cpp", "aten/src/ATen/MemoryOverlap.h", "aten/src/ATen/NamedTensorUtils.h", "aten/src/ATen/TensorIterator.cpp", "aten/src/ATen/TensorIterator.h", "aten/src/ATen/native/DistributionTemplates.h", "aten/src/ATen/native/cpu/RangeFactoriesKernel.cpp", "aten/src/ATen/native/cuda/IndexKernel.cu", "aten/src/ATen/native/cuda/ReduceLogicKernel.cu", "aten/src/ATen/templates/NativeMetaFunctions.h"], "labels": ["open source", "cla signed", "ciflow/default"]}, "8a02d3e5d0": {"title": "Wextra fix for Tensorshape.cpp (#66320)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66320\n\nFixes\n```\nstderr: caffe2/aten/src/ATen/native/TensorShape.cpp:619:36: error: comparison of integers of different signs: 'size_t' (aka 'unsigned long') and 'long' [-Werror,-Wsign-compare]\n    for (size_t offset = 0; offset < numel; offset++) {\n                            ~~~~~~ ^ ~~~~~\nstderr: caffe2/aten/src/ATen/native/TensorShape.cpp:619:36: error: comparison of integers of different signs: 'size_t' (aka 'unsigned long') and 'long' [-Werror,-Wsign-compare]\n    for (size_t offset = 0; offset < numel; offset++) {\n                            ~~~~~~ ^ ~~~~~\n```\n\nTest Plan: Sandcastle\n\nReviewed By: ngimel\n\nDifferential Revision: D31505374\n\nfbshipit-source-id: 0fc393dacd72a8b29c0d82561f730cc047b38f0c", "pr_number": "66320", "files_changed": ["aten/src/ATen/native/TensorShape.cpp"], "labels": ["fb-exported", "ciflow/default"]}, "2daae532bd": {"title": "[ao_migration] torch/nn/qat: torch.quantization -> torch.ao.quantization (#65902)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65902\n\nThis changes the imports in the `caffe2/torch/nn/qat` to include the new import locations.\n\n```\ncodemod -d torch/nn/qat --extensions py 'torch.quantization' 'torch.ao.quantization'\n```\n\nTest Plan: `python test/run_test.py`\n\nReviewed By: jerryzh168\n\nDifferential Revision: D31301196\n\nfbshipit-source-id: ff237790d74cd3b3b5be642a997810f4f439a1d8", "pr_number": "65902", "files_changed": ["torch/nn/qat/modules/conv.py", "torch/nn/qat/modules/linear.py"], "labels": ["cla signed", "ciflow/default"]}, "a28b038af4": {"title": "[ao_migration] torch/nn/intrinsic: torch.quantization -> torch.ao.quantization (#65903)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65903\n\nThis changes the imports in the `caffe2/torch/nn/intrinsic` to include the new import locations.\n\n```\ncodemod -d torch/nn/intrinsic --extensions py 'torch.quantization' 'torch.ao.quantization'\n```\n\nTest Plan: `python test/run_test.py`\n\nReviewed By: albanD\n\nDifferential Revision: D31301195\n\nfbshipit-source-id: a5a9d84cb1ac33df6c90ee03cda3e2f1c5d5ff51", "pr_number": "65903", "files_changed": ["torch/nn/intrinsic/qat/modules/conv_fused.py"], "labels": ["cla signed", "ciflow/default"]}, "4a302a3074": {"title": "Wextra fix for CUDAApplyUtils.cuh (#66323)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66323\n\nFixes\n```\n/data/sandcastle/boxes/eden-trunk-hg-fbcode-fbsource/fbcode/caffe2/aten/src/ATen/cuda/CUDAApplyUtils.cuh:310:48: error: comparison of integers of different signs: 'unsigned long' and 'int' [-Werror,-Wsign-compare]\n  const IndexType bOffset = sizeof...(Offsets) < n ?\n                            ~~~~~~~~~~~~~~~~~~ ^ ~\n/data/sandcastle/boxes/eden-trunk-hg-fbcode-fbsource/fbcode/caffe2/aten/src/ATen/cuda/CUDAApplyUtils.cuh:306:48: error: comparison of integers of different signs: 'unsigned long' and 'int' [-Werror,-Wsign-compare]\n  const IndexType aOffset = sizeof...(Offsets) < n ?\n                            ~~~~~~~~~~~~~~~~~~ ^ ~\n```\n\nTest Plan: Sandcastle\n\nReviewed By: ngimel\n\nDifferential Revision: D31505428\n\nfbshipit-source-id: 326fa8f41f2b200981eddc5cab035b18536cd24e", "pr_number": "66323", "files_changed": ["aten/src/ATen/cuda/CUDAApplyUtils.cuh"], "labels": ["fb-exported", "ciflow/default"]}, "566922bbcd": {"title": "clean up mypy nit in torch/jit/_recursive.py (#66253)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66253\n\nThis was initially broken in #65829 and unbroken in #66003, this PR cleans\nit up by removing the mypy ignore line.\n\nTest Plan:\n```\nmypy torch/jit/_recursive.py --no-incremental\n```\n\nImported from OSS\n\nReviewed By: supriyar\n\nDifferential Revision: D31475100\n\nfbshipit-source-id: 46ab2ede72c08b926f4f9a6b03b1a1375b884c8a", "pr_number": "66253", "files_changed": ["torch/jit/_recursive.py"], "labels": ["ciflow/default"]}, "904fbadaff": {"title": "Fix merge conflict in bc tests (#66356)", "body": "Summary:\nBC test currently borken on trunk\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66356\n\nReviewed By: malfet\n\nDifferential Revision: D31523340\n\nPulled By: janeyx99\n\nfbshipit-source-id: a8d1ff697f017c710f70a76b5bb6a2f89d7637c7", "pr_number": "66356", "files_changed": ["test/backward_compatibility/check_backward_compatibility.py"], "labels": ["ciflow/default"]}, "85b562dd2b": {"title": "Fix type checking errors in fx/utils.py (#66311)", "body": "Summary:\n- [x] Fix the Pyre type checking errors in `torch/quantization/fx/utils.py`\n```\ntorch/quantization/fx/utils.py:490:4 Incompatible variable type [9]: target_module_type is declared to have type `Type[nn.modules.module.Module]` but is used as type `None`.\n```\nFixes the issue: [MLH-Fellowship/pyre-check/issues/75](https://github.com/MLH-Fellowship/pyre-check/issues/75)\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66311\n\nReviewed By: pradeep90\n\nDifferential Revision: D31506399\n\nPulled By: 0xedward\n\nfbshipit-source-id: 3d866fba6005452378d4a2613b8689fa2d7a8b67", "pr_number": "66311", "files_changed": ["torch/ao/quantization/fx/utils.py"], "labels": ["open source", "cla signed", "ciflow/default"]}, "fb5a80ffd8": {"title": "[jit] Don't force refcount bumps from getTypePtr (#66282)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66282\n\nNow that a bunch of the `FooType::get()` functions return a const reference, we can forward that behavior through `getTypePtr()` using return type deduction.\n\nTest Plan: Inspect assembly for List_test.cpp before/after the rest of the change; reference counting is no longer in the happy path.\n\nReviewed By: hlu1, JasonHanwen\n\nDifferential Revision: D31486117\n\nfbshipit-source-id: 863b677bb6685452a5b325d327bdc2a0a09627bf", "pr_number": "66282", "files_changed": ["aten/src/ATen/core/Dict_inl.h", "aten/src/ATen/core/List_inl.h", "aten/src/ATen/core/List_test.cpp", "aten/src/ATen/core/jit_type.h", "aten/src/ATen/core/op_registration/infer_schema.h"], "labels": ["cla signed", "ciflow/default"]}, "1763c25414": {"title": "[PyTorch][jit] Fix excess refcounting in TupleType::compare (#66286)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66286\n\nNo need to take refcount bumps on each comparator call.\n\nTest Plan: CI, review\n\nReviewed By: hlu1, JasonHanwen\n\nDifferential Revision: D31487058\n\nfbshipit-source-id: 98d2447ac27a12695cb0ebe1e279a6b50744ff4f", "pr_number": "66286", "files_changed": ["aten/src/ATen/core/jit_type.h", "aten/src/ATen/core/type.cpp"], "labels": ["cla signed", "ciflow/default"]}, "4cb4d11e0b": {"title": "Disable \"-Wignored-qualifiers\" for vec256_bfloat16.h (#66279)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66279\n\nThis error appears when compiling with \"-Wextra\" and cannot be resolved by fixing the code since the return type of the instrinic being passed to `map` is fixed.\n\nFixes:\n```\ncaffe2/aten/src/ATen/cpu/vec/vec256/vec256_bfloat16.h:204:28: error: 'const' type qualifier on return type has no effect [-Werror,-Wignored-qualifiers]\n  Vectorized<BFloat16> map(const __m256 (*const vop)(__m256)) const {\n                           ^~~~~~\ncaffe2/aten/src/ATen/cpu/vec/vec256/vec256_bfloat16.h:204:28: error: 'const' type qualifier on return type has no effect [-Werror,-Wignored-qualifiers]\n  Vectorized<BFloat16> map(const __m256 (*const vop)(__m256)) const {\n                           ^~~~~~\n```\n\nTest Plan: Sandcastle\n\nReviewed By: ngimel\n\nDifferential Revision: D31480888\n\nfbshipit-source-id: 919c0d48c8ce13ce1106a9df124a077945e36707", "pr_number": "66279", "files_changed": ["aten/src/ATen/cpu/vec/vec256/vec256_bfloat16.h"], "labels": ["fb-exported", "ciflow/default"]}, "109aa135e6": {"title": "Remove apparently unnecessary std::remove_cv_t (#66254)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66254\n\n`std::decay_t` already implies dropping the const\n\nTest Plan: Sandcastle\n\nReviewed By: malfet\n\nDifferential Revision: D31465856\n\nfbshipit-source-id: 851cdb9194354fe9a89b3a37a4463a43dbbcd77a", "pr_number": "66254", "files_changed": ["c10/util/TypeIndex.h"], "labels": ["fb-exported", "ciflow/default"]}, "9539e6216b": {"title": "Quantization docs: add pages for Numeric Suite (Eager and FX) (#66222)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66222\n\nDescription:\n1. creates doc pages for Eager and FX numeric suites\n2. adds a link from main quantization doc to (1)\n3. formats docblocks in Eager NS to render well\n4. adds example code and docblocks to FX numeric suite\n\nTest Plan:\n```\ncd docs\nmake html\npython -m http.server\n// renders well\n```\n\nReviewed By: jerryzh168\n\nDifferential Revision: D31447610\n\nPulled By: vkuzo\n\nfbshipit-source-id: 441170c4a6c3ddea1e7c7c5cc2f1e1cd5aa65f2f", "pr_number": "66222", "files_changed": ["docs/source/quantization.rst", "docs/source/torch.ao.ns._numeric_suite.rst", "docs/source/torch.ao.ns._numeric_suite_fx.rst", "torch/ao/ns/_numeric_suite.py", "torch/ao/ns/_numeric_suite_fx.py", "torch/ao/ns/fx/utils.py"], "labels": ["cla signed", "ciflow/default", "fx"]}, "84326ef059": {"title": "Remove native_functions.yaml dependency from binary ops (#64169)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/64169\n\nTest Plan: Imported from OSS\n\nReviewed By: gchanan\n\nDifferential Revision: D30728586\n\nPulled By: dagitses\n\nfbshipit-source-id: 17d645b6712815d1967b9ff83eecc4d16833ee6b", "pr_number": "64169", "files_changed": ["aten/src/ATen/Context.h", "aten/src/ATen/native/BinaryOps.h", "aten/src/ATen/native/cpu/BinaryOpsKernel.cpp", "aten/src/ATen/native/cuda/BinaryAddSubKernel.cu", "aten/src/ATen/native/cuda/BinaryBitwiseOpsKernels.cu", "aten/src/ATen/native/cuda/BinaryGeometricKernels.cu", "aten/src/ATen/native/cuda/BinaryLogicalOpsKernels.cu", "aten/src/ATen/native/cuda/BinaryMiscOpsKernels.cu", "aten/src/ATen/native/cuda/BinaryMulDivKernel.cu", "aten/src/ATen/native/cuda/BinaryRemainderKernel.cu", "aten/src/ATen/native/cuda/BinaryShiftOpsKernels.cu", "aten/src/ATen/native/cuda/CompareEQKernel.cu", "aten/src/ATen/native/cuda/CompareGEKernel.cu", "aten/src/ATen/native/cuda/CompareGTKernel.cu", "aten/src/ATen/native/cuda/CompareLEKernel.cu", "aten/src/ATen/native/cuda/CompareLTKernel.cu", "aten/src/ATen/native/cuda/CompareNEKernel.cu", "aten/src/ATen/native/cuda/CopysignKernel.cu", "aten/src/ATen/native/cuda/GcdLcmKernel.cu", "aten/src/ATen/native/cuda/IGammaKernel.cu", "aten/src/ATen/native/cuda/LogAddExpKernel.cu", "aten/src/ATen/native/cuda/MaxMinElementwiseKernel.cu", "aten/src/ATen/native/cuda/StepKernel.cu", "aten/src/ATen/native/cuda/ZetaKernel.cu"], "labels": ["open source", "cla signed", "ciflow/default"]}, "27f193af64": {"title": "Automated submodule update: kineto (#59674)", "body": "Summary:\nThis is an automated pull request to update the first-party submodule for [pytorch/kineto](https://github.com/pytorch/kineto).\n\nNew submodule commit: https://github.com/pytorch/kineto/commit/6f9c0eeff519ac7365fec427829404c9741ef391\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/59674\n\nTest Plan: Ensure that CI jobs succeed on GitHub before landing.\n\nReviewed By: larryliu0820\n\nDifferential Revision: D28977762\n\nfbshipit-source-id: d441d4d46a7044cc05eb8b21e59471deee312e02", "pr_number": "59674", "files_changed": ["third_party/kineto"], "labels": ["triaged", "open source", "cla signed", "ciflow/default"]}, "ad0accdecd": {"title": "Revert D31447610: Quantization docs: add pages for Numeric Suite (Eager and FX)", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD31447610 (https://github.com/pytorch/pytorch/commit/9539e6216bc122b604e8fb55c075d1d525c2522b)\n\nOriginal commit changeset: 441170c4a6c3\n\nfbshipit-source-id: b49bff54405cdb8465397077e38506a36b277921", "pr_number": null, "files_changed": ["docs/source/quantization.rst", "docs/source/torch.ao.ns._numeric_suite.rst", "docs/source/torch.ao.ns._numeric_suite_fx.rst", "torch/ao/ns/_numeric_suite.py", "torch/ao/ns/_numeric_suite_fx.py", "torch/ao/ns/fx/utils.py"], "labels": []}, "df1858bea5": {"title": "Revert D31447611: Quantization documentation: move backend section down", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD31447611 (https://github.com/pytorch/pytorch/commit/309a8cf46c2b551d04d03df1d6181e0c350356dc)\n\nOriginal commit changeset: 537b146559bc\n\nfbshipit-source-id: c400aef9a2ea5d18f8076879fe6354be7a6732f1", "pr_number": null, "files_changed": ["docs/source/quantization.rst"], "labels": []}, "037ac2330e": {"title": "Revert D31447616: Quantization docs: consilidate all API references on a single page", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD31447616 (https://github.com/pytorch/pytorch/commit/fe86f0e068e07c0e61d79533afa10b1fcd634db1)\n\nOriginal commit changeset: 2f9c4dac2b2f\n\nfbshipit-source-id: 673368e87399f0a25441688bb9356de5a2f3e66e", "pr_number": null, "files_changed": ["docs/source/quantization-support.rst", "docs/source/quantization.rst"], "labels": []}, "10633460ce": {"title": "Revert D31447614: Create a documentation page for `torch.ao.quantization.QConfig`", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD31447614 (https://github.com/pytorch/pytorch/commit/7332ed13edc509d1fc79e34cb5a83310978b68c5)\n\nOriginal commit changeset: 5d9dd2a4e864\n\nfbshipit-source-id: 6ac15a956222ca61f7fbb75ed36bcc58b23f0f36", "pr_number": null, "files_changed": ["docs/source/quantization.rst", "docs/source/torch.quantization.qconfig.rst", "docs/source/torch.quantization.rst", "torch/ao/quantization/qconfig.py"], "labels": []}, "b85fd4c54f": {"title": "Revert D31447613: Create separate documentation pages for quantization observers and fake_quants", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD31447613 (https://github.com/pytorch/pytorch/commit/f0fa3d1110fdd085b040ef82c8a547b205b98e38)\n\nOriginal commit changeset: 63b4cf518bad\n\nfbshipit-source-id: 67de592d1e12a5149cdb22b0725caad063f94476", "pr_number": null, "files_changed": ["docs/source/quantization.rst", "docs/source/torch.quantization.fake_quantize.rst", "docs/source/torch.quantization.observer.rst", "docs/source/torch.quantization.rst", "torch/ao/quantization/fake_quantize.py", "torch/ao/quantization/observer.py"], "labels": []}, "9971113340": {"title": "Revert D31447612: Create a documentation page for FX graph mode quantization APIs", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD31447612 (https://github.com/pytorch/pytorch/commit/a89ac3138e37d847f1c1e4c5beeb6303768ebedf)\n\nOriginal commit changeset: 07d0a6137f15\n\nfbshipit-source-id: f2cba7d835011500580b4ab9cff72171280ee18b", "pr_number": null, "files_changed": ["docs/source/quantization.rst", "docs/source/torch.quantization.quantize_fx.rst", "torch/ao/quantization/quantize_fx.py"], "labels": []}, "bc06eefebe": {"title": "[reland] Allow external CUDA streams to be set as current (#66324)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66324\n\nFixes https://github.com/pytorch/pytorch/issues/65822.\n\nReland of https://github.com/pytorch/pytorch/pull/65914.\nghstack-source-id: 140105651\n\nTest Plan: Added tests\n\nReviewed By: ngimel\n\nDifferential Revision: D31506134\n\nfbshipit-source-id: ff56203a120befdb282e974309478ac11aa56652", "pr_number": "66324", "files_changed": ["aten/src/ATen/test/cuda_stream_test.cpp", "c10/cuda/CUDAStream.cpp"], "labels": ["cla signed", "ciflow/default", "ciflow/win"]}, "0348148725": {"title": "Update link to qnnpack in quantization doc. (#66226)", "body": "Summary:\nThe old repo has been archived.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66226\n\nReviewed By: vkuzo\n\nDifferential Revision: D31534712\n\nPulled By: ezyang\n\nfbshipit-source-id: 4d7f070c8547aeb25464c72b25ed21f209821bc2", "pr_number": "66226", "files_changed": ["docs/source/quantization.rst"], "labels": ["open source", "cla signed", "ciflow/default"]}, "1b40daac74": {"title": "pinv: forward/backward AD which is Frechet-defined in a rank-preserving neighborhood. (#66092)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/65911. Also enables complex support/tests for `linalg_pinv` in OpInfo.\n\ncc ezyang albanD zou3519 gqchen pearu nikitaved soulitzer Lezcano Varal7 jianyuh mruberry walterddr IvanYashchuk xwang233\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66092\n\nReviewed By: ejguan\n\nDifferential Revision: D31503072\n\nPulled By: albanD\n\nfbshipit-source-id: 52018e826826ae62beaad76becb5edf880be253f", "pr_number": "66092", "files_changed": ["aten/src/ATen/native/native_functions.yaml", "tools/autograd/derivatives.yaml", "tools/autograd/gen_variable_type.py", "torch/csrc/autograd/FunctionsManual.cpp", "torch/csrc/autograd/FunctionsManual.h", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["module: autograd", "open source", "module: linear algebra", "complex_autograd", "cla signed", "ciflow/slow-gradcheck", "ciflow/default"]}, "4775419850": {"title": "[BE] Address feedback from #66296 (#66315)", "body": "Summary:\nAlso use range loop instead of regular one\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66315\n\nReviewed By: albanD\n\nDifferential Revision: D31503730\n\nPulled By: malfet\n\nfbshipit-source-id: f5568f7f28e15a9becd27986dd061a6fcae34651", "pr_number": "66315", "files_changed": ["torch/csrc/autograd/python_function.cpp"], "labels": ["ciflow/default"]}, "d3b29afbb6": {"title": "Remove old code that is unused in test/ (#66331)", "body": "Summary:\n.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66331\n\nReviewed By: gchanan\n\nDifferential Revision: D31533549\n\nPulled By: albanD\n\nfbshipit-source-id: 5addd11edc4199a88f10f0ff236be59ec2289903", "pr_number": "66331", "files_changed": ["test/optim/compare.sh", "test/optim/test.lua", "test/optim/test.py", "test/optim/tests.json"], "labels": ["ciflow/default"]}, "c66847afbe": {"title": "Add workaround for nvcc header dependecies bug (#62550)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/62550\n\nI noticed that running the build twice in a row resulted in ~80 CUDA files being\nrebuilt. Running `ninja -d explain` shows\n```\nninja explain: TH/generic/THStorage.h is dirty\nninja explain: TH/generic/THStorageCopy.h is dirty\nninja explain: THC/generic/THCStorage.h is dirty\nninja explain: THC/generic/THCStorageCopy.h is dirty\nninja explain: TH/generic/THTensor.h is dirty\nninja explain: THC/generic/THCTensor.h is dirty\nninja explain: THC/generic/THCTensorCopy.h is dirty\nninja explain: THC/generic/THCTensorMath.h is dirty\nninja explain: THC/generic/THCTensorMathMagma.h is dirty\nninja explain: THC/generic/THCTensorMathPairwise.h is dirty\nninja explain: THC/generic/THCTensorScatterGather.h is dirty\n```\n\nconsidering `ninja` is working relative to the `build` folder, these files don't\nactually exist. I traced this back to the output of `nvcc -MD` containing\npaths relative to the include directory, instead of being absolute.\n\nThis adds a little script to launch the compiler then resolve any relative paths\nin the `.d` file before `ninja` looks at it. To use it, I run the build with\n```\nexport CMAKE_CUDA_COMPILER_LAUNCHER=\"python;`pwd`/tools/nvcc_fix_deps.py;ccache\"\n```\n\nThere are some possible pit-falls here. The same relative path might work for\ntwo include directories, and the compiler could pick a different one. Or,\nthe compiler might have additional implicit include directories that are needed\nto resolve the path. However, this has worked perfectly in my testing and it's\ncompletely opt-in so should be fine.\n\nTest Plan: Imported from OSS\n\nReviewed By: ejguan\n\nDifferential Revision: D31503351\n\nPulled By: malfet\n\nfbshipit-source-id: b184c4526679d976b93829b5715cafcb1c7db2ae", "pr_number": "62550", "files_changed": ["CONTRIBUTING.md", "tools/nvcc_fix_deps.py"], "labels": ["open source", "cla signed", "ci/no-build", "ciflow/default"]}, "221c308389": {"title": "Wextra fix for LossCTC.cpp (#66381)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66381\n\nFixes\n```\nstderr: caffe2/aten/src/ATen/native/cudnn/LossCTC.cpp:83:37: error: comparison of integers of different signs: 'size_t' (aka 'unsigned long') and 'const long' [-Werror,-Wsign-compare]\n  TORCH_CHECK(input_lengths_.size() == batch_size, \"input_lengths needs to have size to match batch_size\");\n              ~~~~~~~~~~~~~~~~~~~~~ ^  ~~~~~~~~~~\n```\n\nTest Plan: Sandcastle\n\nReviewed By: ngimel\n\nDifferential Revision: D31510217\n\nfbshipit-source-id: e3585e08650950c08d80d347dfae375aedf2ceaf", "pr_number": "66381", "files_changed": ["aten/src/ATen/native/cudnn/LossCTC.cpp"], "labels": ["fb-exported", "ciflow/default"]}, "acb0157a3d": {"title": "Specialization for `c10::util:get_type_index<std::string>` (#66290)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66290\n\nAdd full specialization for std::string type index\n\nIt slightly speeds up compilation as well as solves the ambiguity how template instantiations implemented in inline namespaces are rendered during `__PRETTY_FUNCTION__` computation.\n\nNot sure what `#pragma` controls this behaviour, but when code is compiled by clang-12+ using libstdc++, `__PRETTY_PRINT__`, sometimes resolve `std::string` to `std::basic_string<char>` and sometimes to `std::__cxx11::basic_string<char>`, even though in the object file symbol is always inside `std::__cxx11::` namespace, which might break caffe2 serialization code that depends on dynamic hash generation\n\nTemplate name resolution were debugged using https://gist.github.com/malfet/c83b9ebd35730ebf8bac7af42682ea37\n\n(Note: this ignores all push blocking failures!)\n\nTest Plan: CI\n\nReviewed By: r-barnes\n\nDifferential Revision: D31490050\n\nfbshipit-source-id: 127091574cf6b92c7ec3f972821e4e76f5f626a9", "pr_number": "66290", "files_changed": ["c10/util/TypeIndex.h"], "labels": ["fb-exported", "ciflow/default"]}, "998cb98844": {"title": "[PyTorch][jit] Cache TupleType objects in getTypePtr (#66340)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66340\n\nFor functions that take `std::vector`s with `std::tuple`s in them, `getTypePtr` can get hit on every call, in which case creating a new `TupleType` object every time is expensive.\nghstack-source-id: 140143104\n\nTest Plan: CI\n\nReviewed By: hlu1, JasonHanwen\n\nDifferential Revision: D31514792\n\nfbshipit-source-id: 23652ca90ba1259afc05e953b99ce1fe1bebcc2b", "pr_number": "66340", "files_changed": ["aten/src/ATen/core/jit_type.h"], "labels": ["cla signed", "ciflow/default"]}, "8c468ce00b": {"title": "[PyTorch][JIT] Return a reference from caching specializations of getTypePtr (#66342)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66342\n\n`decltype(auto)` in D31486117 (https://github.com/pytorch/pytorch/commit/fb5a80ffd80997eabf724282e62a7eaf4a78c2ad) wasn't the right choice in these specializations, because it will *still* deduce a copy.\nSee https://godbolt.org/z/GjbcPE1c4 for example.\nghstack-source-id: 140144199\n\nTest Plan: CI, added new static_assert to make sure we got it right for std::tuple in particular\n\nReviewed By: hlu1, JasonHanwen\n\nDifferential Revision: D31514960\n\nfbshipit-source-id: cae722aa34345b590c46eae478229cb5f4b0d7dc", "pr_number": "66342", "files_changed": ["aten/src/ATen/core/jit_type.h", "aten/src/ATen/core/type.cpp"], "labels": ["cla signed", "ciflow/default"]}, "08fab7ae13": {"title": "Wextra fix for Integration.cpp (#66321)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66321\n\nFixes\n```\nstderr: caffe2/aten/src/ATen/native/Integration.cpp:62:27: error: comparison of integers of different signs: 'size_t' (aka 'unsigned long') and 'int64_t' (aka 'long') [-Werror,-Wsign-compare]\n    if (curr_shape.size() >= target_n_dim)\n        ~~~~~~~~~~~~~~~~~ ^  ~~~~~~~~~~~~\nstderr: caffe2/aten/src/ATen/native/Integration.cpp:62:27: error: comparison of integers of different signs: 'size_t' (aka 'unsigned long') and 'int64_t' (aka 'long') [-Werror,-Wsign-compare]\n    if (curr_shape.size() >= target_n_dim)\n        ~~~~~~~~~~~~~~~~~ ^  ~~~~~~~~~~~~\n```\n\nTest Plan: Sandcastle\n\nReviewed By: ngimel\n\nDifferential Revision: D31505347\n\nfbshipit-source-id: 100b76215f78c3ce75bf4a993715a6767189747d", "pr_number": "66321", "files_changed": ["aten/src/ATen/native/Integration.cpp"], "labels": ["fb-exported", "ciflow/default"]}, "49f1605392": {"title": "[RFC] Reduce logging noise from AdagradOptimizer (#66443)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66443\n\nFor some reason, this logging is adding noise to a lot of flow jobs. I am not sure if this is actually needed.\nThis is called from the __init__ so it's logged all the time and logs all key:values the current local symbol.\n\nTest Plan: N/A\n\nReviewed By: chowarfb\n\nDifferential Revision: D31534372\n\nfbshipit-source-id: bed032b66fed548c97a6f66b1b9e905fd2738851", "pr_number": "66443", "files_changed": ["caffe2/python/optimizer.py"], "labels": ["fb-exported", "ciflow/default"]}, "ae5a9a451f": {"title": "Do not enforce unused vars rule for torch_deploy (#66447)", "body": "Summary:\nFollowup after  https://github.com/pytorch/pytorch/pull/66041\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66447\n\nReviewed By: seemethere\n\nDifferential Revision: D31554356\n\nPulled By: malfet\n\nfbshipit-source-id: 6638324dcf658f4b244da285b4360ff2e2e2c013", "pr_number": "66447", "files_changed": ["torch/CMakeLists.txt"], "labels": ["ciflow/default", "ciflow/scheduled"]}, "88ed93c2ca": {"title": "Fix type checking errors in torch/quantization/fx/qconfig_utils.py (#66428)", "body": "Summary:\n- [x] Fix the Pyre type checking errors in `torch/quantization/fx/qconfig_utils.py`\n```\ntorch/quantization/fx/qconfig_utils.py:241:46 Incompatible variable type [9]: prepare_custom_config_dict is declared to have type `Dict[str, typing.Any]` but is used as type `None`.\ntorch/quantization/fx/qconfig_utils.py:267:46 Incompatible variable type [9]: convert_custom_config_dict is declared to have type `Dict[str, typing.Any]` but is used as type `None`.\ntorch/quantization/fx/qconfig_utils.py:284:43 Incompatible variable type [9]: fuse_custom_config_dict is declared to have type `Dict[str, typing.Any]` but is used as type `None`.\n```\nFixes the issue: [MLH-Fellowship/pyre-check/issues/73](https://github.com/MLH-Fellowship/pyre-check/issues/73)\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66428\n\nReviewed By: grievejia\n\nDifferential Revision: D31545215\n\nPulled By: 0xedward\n\nfbshipit-source-id: 767ae7888854c2eec2ecf14855a5b011110b9271", "pr_number": "66428", "files_changed": ["torch/ao/quantization/fx/qconfig_utils.py"], "labels": ["open source", "ciflow/default"]}, "565cf47abf": {"title": "Quantization docs: add pages for Numeric Suite (Eager and FX) (#66380)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66380\n\nDescription:\n1. creates doc pages for Eager and FX numeric suites\n2. adds a link from main quantization doc to (1)\n3. formats docblocks in Eager NS to render well\n4. adds example code and docblocks to FX numeric suite\n\nTest Plan:\n```\ncd docs\nmake html\npython -m http.server\n// renders well\n```\n\nReviewed By: jerryzh168\n\nDifferential Revision: D31543173\n\nPulled By: vkuzo\n\nfbshipit-source-id: feb291bcbe92747495f45165f738631fa5cbffbd", "pr_number": "66380", "files_changed": ["docs/source/quantization.rst", "docs/source/torch.ao.ns._numeric_suite.rst", "docs/source/torch.ao.ns._numeric_suite_fx.rst", "torch/ao/ns/_numeric_suite.py", "torch/ao/ns/_numeric_suite_fx.py", "torch/ao/ns/fx/utils.py"], "labels": ["ciflow/default"]}, "d8532e3524": {"title": "[PyTorch] Split c10 Type.cpp into two files to allow targets to include one of them (#66445)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66445\n\n`Type.cpp` implements `demangle()` function based on the macro `HAS_DEMANGLE`. This diff splits it into two `.cpps` so that we can add either one into the build target. This change follows the patternof `flags_use_no_gflags.cpp` and `flags_use_gflags.cpp`.\n\nTest Plan: Rely on CI\n\nReviewed By: iseeyuan\n\nDifferential Revision: D31551432\n\nfbshipit-source-id: f8b11783e513fa812228ec873459ad3043ff9147", "pr_number": "66445", "files_changed": ["c10/macros/Macros.h", "c10/util/Type.cpp", "c10/util/Type_demangle.cpp", "c10/util/Type_no_demangle.cpp"], "labels": ["fb-exported", "ciflow/default"]}, "2d1552824a": {"title": "Revert D31386275: Migrate THCState to ATen", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD31386275 (https://github.com/pytorch/pytorch/commit/a6774d6e1f3a88f931a3786b7c2617a74bdaa50e)\n\nOriginal commit changeset: 5c1f1bbe8c3d\n\nfbshipit-source-id: bea4e80fb0bdc57e8bb6a8ee781afd224adf4ed0", "pr_number": null, "files_changed": ["aten/src/ATen/cuda/PeerToPeerAccess.cpp", "aten/src/ATen/cuda/PeerToPeerAccess.h", "aten/src/ATen/cuda/detail/CUDAHooks.cpp", "aten/src/ATen/cuda/detail/CUDAHooks.h", "aten/src/ATen/native/cuda/Copy.cu", "aten/src/ATen/native/cuda/Resize.cpp", "aten/src/THC/CMakeLists.txt", "aten/src/THC/THCGeneral.cpp", "aten/src/THC/THCGeneral.h.in", "aten/src/THC/THCGeneral.hpp", "aten/src/THC/THCTensorMathMagma.cpp"], "labels": []}, "18e4688199": {"title": "[Pytorch Edge] Improve bundled inputs name error handling (#65856)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65856\n\nOccasionally functions dont have this __name__ variable set and have name set instead? Not sure why this happens, but this should catch it.\n\nTest Plan: ci\n\nReviewed By: iseeyuan\n\nDifferential Revision: D31286787\n\nfbshipit-source-id: 8a339541215329b6e9ff43ef77363be41f19c5ca", "pr_number": "65856", "files_changed": ["torch/utils/bundled_inputs.py"], "labels": ["fb-exported", "cla signed", "ciflow/default"]}, "1841f76cc0": {"title": "Remove native_functions.yaml dependency from unary ops (#64170)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/64170\n\nTest Plan: Imported from OSS\n\nReviewed By: gchanan, ezyang\n\nDifferential Revision: D30728578\n\nPulled By: dagitses\n\nfbshipit-source-id: 70baa90d0834e68324504c74064a1d1790193483", "pr_number": "64170", "files_changed": ["aten/src/ATen/native/UnaryOps.h", "aten/src/ATen/native/cpu/DistributionKernels.cpp", "aten/src/ATen/native/cpu/UnaryOpsKernel.cpp", "aten/src/ATen/native/cuda/AbsKernel.cu", "aten/src/ATen/native/cuda/UnaryComplexKernels.cu", "aten/src/ATen/native/cuda/UnaryFractionKernels.cu", "aten/src/ATen/native/cuda/UnaryGammaKernels.cu", "aten/src/ATen/native/cuda/UnaryGeometricKernels.cu", "aten/src/ATen/native/cuda/UnaryLogKernels.cu", "aten/src/ATen/native/cuda/UnaryOpsKernel.cu", "aten/src/ATen/native/cuda/UnarySignKernels.cu", "aten/src/ATen/native/cuda/UnarySpecialOpsKernel.cu", "tools/build_variables.bzl"], "labels": ["open source", "cla signed", "ciflow/default"]}, "8674a3c6e3": {"title": "Remove native_functions.yaml dependency from PowKernel (#64171)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/64171\n\nTest Plan: Imported from OSS\n\nReviewed By: gchanan\n\nDifferential Revision: D30728583\n\nPulled By: dagitses\n\nfbshipit-source-id: ea6891a3598eead93daea620b94e50d3a3b248cf", "pr_number": "64171", "files_changed": ["aten/src/ATen/native/Pow.h", "aten/src/ATen/native/cpu/PowKernel.cpp", "aten/src/ATen/native/cuda/PowKernel.cu"], "labels": ["open source", "cla signed", "ciflow/default"]}, "213ac4e59c": {"title": "Remove native_functions.yaml dependency from PointwiseOps (#64172)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/64172\n\nTest Plan: Imported from OSS\n\nReviewed By: gchanan\n\nDifferential Revision: D30728584\n\nPulled By: dagitses\n\nfbshipit-source-id: 2ae9686ac7c312e2d470d26a3cad12afcf7ef47b", "pr_number": "64172", "files_changed": ["aten/src/ATen/native/PointwiseOps.h", "aten/src/ATen/native/cpu/PointwiseOpsKernel.cpp", "aten/src/ATen/native/cuda/PointwiseOpsKernel.cu"], "labels": ["open source", "cla signed", "ciflow/default"]}, "8818dda237": {"title": "Fix lstsq to work with inputs that require grad (#66426)", "body": "Summary:\nI updated `sample_inputs_linalg_lstsq` and `test_nondifferentiable`\nnow correctly reveals the failure. The internal assert error was thrown\nbecause autograd attempts to mark integer tensor as differentiable.\n\nFixes https://github.com/pytorch/pytorch/issues/66420.\n\ncc ezyang albanD zou3519 gqchen pearu nikitaved soulitzer Lezcano Varal7\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66426\n\nReviewed By: ejguan\n\nDifferential Revision: D31550942\n\nPulled By: albanD\n\nfbshipit-source-id: 4a0ca60e62c5e9bb96af5020541da2d09ea3e405", "pr_number": "66426", "files_changed": ["tools/autograd/derivatives.yaml", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["module: autograd", "open source", "ciflow/default"]}, "702fb1de72": {"title": "[fx2trt] open source tests for acc tracer (#66302)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66302\n\nJust move files, ossci can be setup later\n\nTest Plan:\nbuck run //caffe2/test:test_fx_acc_tracer\n\ntestinprod\n\nReviewed By: 842974287\n\nDifferential Revision: D31495087\n\nfbshipit-source-id: f182c7438e3e80ba98924990682cb45a99b9967c", "pr_number": "66302", "files_changed": ["test/fx_acc/test_acc_tracer.py"], "labels": ["fb-exported", "ciflow/default"]}, "17e79bc76c": {"title": "remove is_reference from all is_output_quantized (#66456)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/66456\n\nTest Plan: Imported from OSS\n\nReviewed By: jerryzh168\n\nDifferential Revision: D31562633\n\nPulled By: rahxephon89\n\nfbshipit-source-id: 85c73a23e90ba9c1406f4027d447fbbe4576e39a", "pr_number": "66456", "files_changed": ["torch/ao/quantization/fx/convert.py", "torch/ao/quantization/fx/prepare.py", "torch/ao/quantization/fx/quantization_patterns.py"], "labels": ["ciflow/default"]}, "47c531b6e8": {"title": "[jit] Compare object identity first in ClassType::operator== (#65347)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65347\n\nThis check is much cheaper than anything involving actually inspecting object fields (i.e., the cost is low), and if it succeeds we can skip the expensive (e.g., it involves locking a weak_ptr and then destroying the resulting shared_ptr)  function body. It almost entirely eliminates time spent in this function during model loading according to perf.\nghstack-source-id: 140148561\n\nTest Plan: Specifically I profiled static runtime startup for the ctr_mobile_feed model and saw self time in this function go from 2-3% to 0.36%.\n\nReviewed By: ejguan\n\nDifferential Revision: D31057279\n\nfbshipit-source-id: efb6bdc0957b680112ac282e85dc1b06b1b6c0bd", "pr_number": "65347", "files_changed": ["aten/src/ATen/core/jit_type.h"], "labels": ["cla signed", "ciflow/default"]}, "9984f4bb8b": {"title": "Remove native_functions.yaml dependency from some reduction operators (#64173)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/64173\n\nThis one also required restructuring the code a bit to move the kernel\ncode into seperate files. So, I've mainly focused on CUDA which is\nwhere the real build-time issues are.\n\nTest Plan: Imported from OSS\n\nReviewed By: jbschlosser, ezyang\n\nDifferential Revision: D30728581\n\nPulled By: dagitses\n\nfbshipit-source-id: a69eea5b4100d16165a02660dde200c8f648683d", "pr_number": "64173", "files_changed": ["aten/src/ATen/core/Tensor.cpp", "aten/src/ATen/core/TensorBase.h", "aten/src/ATen/native/LinearAlgebra.h", "aten/src/ATen/native/ReduceAllOps.h", "aten/src/ATen/native/ReduceOps.h", "aten/src/ATen/native/TensorCompare.h", "aten/src/ATen/native/cpu/Reduce.h", "aten/src/ATen/native/cpu/SumKernel.cpp", "aten/src/ATen/native/cuda/Equal.cpp", "aten/src/ATen/native/cuda/LinearAlgebra.cu", "aten/src/ATen/native/cuda/Reduce.cuh", "aten/src/ATen/native/cuda/ReduceLogicKernel.cu", "aten/src/ATen/native/cuda/ReduceMinMaxKernel.cu", "aten/src/ATen/native/cuda/ReduceMomentKernel.cu", "aten/src/ATen/native/cuda/ReduceNormKernel.cu", "aten/src/ATen/native/cuda/ReduceOps.cpp", "aten/src/ATen/native/cuda/ReduceOps.h", "aten/src/ATen/native/cuda/ReduceSumProdKernel.cu", "aten/src/ATen/native/cuda/TensorCompare.cpp", "aten/src/ATen/native/cuda/TensorCompare.cu", "caffe2/CMakeLists.txt"], "labels": ["open source", "cla signed", "ciflow/default"]}, "3ac2c74896": {"title": "Revert D31082208: Use shared CUPTI by default", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD31082208 (https://github.com/pytorch/pytorch/commit/8b0eae5aa86dac184486d0d9481fff8a25aa7853)\n\nOriginal commit changeset: 14f66af92084\n\nfbshipit-source-id: 0faff00832b7f79d476fd1f9f505142a548a76db", "pr_number": null, "files_changed": ["CMakeLists.txt", "cmake/Dependencies.cmake"], "labels": []}, "d32736e317": {"title": "Make permission errors more human readable (#66492)", "body": "Summary:\n`_mkdir_p` feels like a remnant of Python-2 era, add `exist_ok` argument and re-raise OSError to make it more human readable.\n\nAfter the change attempt to build PyTorch in a folder that does not have write permissions will result in:\n```\n% python3.6 setup.py develop\nBuilding wheel torch-1.10.0a0+git9509e8a\n-- Building version 1.10.0a0+git9509e8a\nTraceback (most recent call last):\n  File \"/Users/nshulga/git/pytorch-worktree/tools/setup_helpers/cmake.py\", line 21, in _mkdir_p\n    os.makedirs(d, exist_ok=True)\n  File \"/opt/homebrew/Cellar/python36/3.6.2+_254.20170915/Frameworks/Python.framework/Versions/3.6/lib/python3.6/os.py\", line 220, in makedirs\n    mkdir(name, mode)\nPermissionError: [Errno 13] Permission denied: 'build'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"setup.py\", line 895, in <module>\n    build_deps()\n  File \"setup.py\", line 370, in build_deps\n    cmake=cmake)\n  File \"/Users/nshulga/git/pytorch-worktree/tools/build_pytorch_libs.py\", line 63, in build_caffe2\n    rerun_cmake)\n  File \"/Users/nshulga/git/pytorch-worktree/tools/setup_helpers/cmake.py\", line 225, in generate\n    _mkdir_p(self.build_dir)\n  File \"/Users/nshulga/git/pytorch-worktree/tools/setup_helpers/cmake.py\", line 23, in _mkdir_p\n    raise RuntimeError(f\"Failed to create folder {os.path.abspath(d)}: {e.strerror}\") from e\nRuntimeError: Failed to create folder /Users/nshulga/git/pytorch-worktree/build: Permission denied\n```\n\nFixes https://github.com/pytorch/pytorch/issues/65920\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66492\n\nReviewed By: seemethere\n\nDifferential Revision: D31578820\n\nPulled By: malfet\n\nfbshipit-source-id: afe8240983100ac0a26cc540376b9dd71b1b53af", "pr_number": "66492", "files_changed": ["tools/setup_helpers/cmake.py"], "labels": ["ciflow/default"]}, "40794dbb25": {"title": "add backend_config_dict to checkGraphModeFxOp (#66499)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/66499\n\nTest Plan: Imported from OSS\n\nReviewed By: jerryzh168\n\nDifferential Revision: D31582518\n\nPulled By: rahxephon89\n\nfbshipit-source-id: b8107bb7140517f2dc32bf692c6b916536ea35c3", "pr_number": "66499", "files_changed": ["torch/testing/_internal/common_quantization.py"], "labels": ["ciflow/default"]}, "08f3823647": {"title": "Sparse CSR CUDA: add `addmv_out` (#61407)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/61407\n\nThis PR adds `addmv_out_sparse_csr_cuda`. The operation is used to\ncompute matrix-vector multiplication. Since structured_delegate is used\nwe only need to implement the out variant, the in-place and normal\nvariants are autogenerated.\nWorking on this PR revealed that float16 (and probably bfloat16) inputs\ndo not work correctly in cusparse, therefore for this case `addmm` is\nused with squeezes and unsqueezes.\n\ncc nikitaved pearu cpuhrsch IvanYashchuk ngimel\n\nTest Plan: Imported from OSS\n\nReviewed By: malfet\n\nDifferential Revision: D31584499\n\nPulled By: ngimel\n\nfbshipit-source-id: 4c507791471ada88969116b88eeaaba7a7536431", "pr_number": "61407", "files_changed": ["BUILD.bazel", "aten/src/ATen/cuda/CUDASparseDescriptors.cpp", "aten/src/ATen/cuda/CUDASparseDescriptors.h", "aten/src/ATen/native/Blas.cpp", "aten/src/ATen/native/native_functions.yaml", "aten/src/ATen/native/sparse/cuda/SparseBlas.cpp", "aten/src/ATen/native/sparse/cuda/SparseBlasImpl.cpp", "aten/src/ATen/native/sparse/cuda/SparseBlasImpl.h", "test/test_sparse_csr.py", "torch/testing/_internal/common_device_type.py"], "labels": ["module: sparse", "module: cuda", "open source", "cla signed", "ciflow/default", "ciflow/cuda"]}, "8eb85b5027": {"title": "Remove THCNumerics (#66388)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/66388\n\nTest Plan: Imported from OSS\n\nReviewed By: mruberry\n\nDifferential Revision: D31547710\n\nPulled By: ngimel\n\nfbshipit-source-id: 20710328f2e5fc2e931a3f8ba9b4243acc310d54", "pr_number": "66388", "files_changed": ["aten/src/ATen/NumericUtils.h", "aten/src/ATen/native/cuda/AdaptiveAveragePooling3d.cu", "aten/src/ATen/native/cuda/AdaptiveMaxPooling2d.cu", "aten/src/ATen/native/cuda/AdaptiveMaxPooling3d.cu", "aten/src/ATen/native/cuda/DilatedMaxPool2d.cu", "aten/src/ATen/native/cuda/DilatedMaxPool3d.cu", "aten/src/ATen/native/cuda/FractionalMaxPool2d.cu", "aten/src/ATen/native/cuda/FractionalMaxPool3d.cu", "aten/src/ATen/native/cuda/ReduceMinMaxKernel.cu", "aten/src/ATen/native/cuda/Sorting.cu", "aten/src/ATen/native/cuda/SortingCommon.cuh", "aten/src/ATen/native/cuda/SortingRadixSelect.cuh", "aten/src/ATen/native/cuda/SummaryOps.cu", "aten/src/THC/CMakeLists.txt", "aten/src/THC/THCNumerics.cuh", "aten/src/THC/THCTensorMathReduce.cuh", "test/test_cpp_extensions_jit.py"], "labels": ["module: porting", "open source", "ciflow/default"]}, "80a3619823": {"title": "Remove THCTensorMathReduce.cuh (#66389)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/66389\n\nTest Plan: Imported from OSS\n\nReviewed By: ejguan\n\nDifferential Revision: D31547711\n\nPulled By: ngimel\n\nfbshipit-source-id: c181d14f66536b6873b5b14088312c6c70bf0855", "pr_number": "66389", "files_changed": ["aten/src/ATen/native/cuda/DistanceKernel.cu", "aten/src/ATen/native/cuda/SoftMax.cu", "aten/src/ATen/native/cuda/TensorTopK.cu", "aten/src/ATen/native/cuda/WeightNorm.cu", "aten/src/THC/CMakeLists.txt", "aten/src/THC/THCTensorMathReduce.cuh"], "labels": ["module: porting", "open source", "ciflow/default"]}, "9918fd8305": {"title": "[fx2trt] open source tests for converters (#66361)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66361\n\nossci will be setup later, fbonly ci is ready\n\nTest Plan:\nbuck run caffe2/test:fx2trt_test_linear\n\ntestinprod\n\nReviewed By: 842974287\n\nDifferential Revision: D31511082\n\nfbshipit-source-id: 9e2c50c83fdba822cd2488eb17b5787d8a57f087", "pr_number": "66361", "files_changed": ["test/fx2trt/converters/acc_op/test_adaptive_avgpool.py", "test/fx2trt/converters/acc_op/test_avgpool.py", "test/fx2trt/converters/acc_op/test_batchnorm.py", "test/fx2trt/converters/acc_op/test_binary_ops.py", "test/fx2trt/converters/acc_op/test_cat.py", "test/fx2trt/converters/acc_op/test_clamp.py", "test/fx2trt/converters/acc_op/test_convolution.py", "test/fx2trt/converters/acc_op/test_dequantize.py", "test/fx2trt/converters/acc_op/test_flatten.py", "test/fx2trt/converters/acc_op/test_gelu.py", "test/fx2trt/converters/acc_op/test_getitem.py", "test/fx2trt/converters/acc_op/test_layer_norm.py", "test/fx2trt/converters/acc_op/test_linear.py", "test/fx2trt/converters/acc_op/test_matmul.py", "test/fx2trt/converters/acc_op/test_max.py", "test/fx2trt/converters/acc_op/test_maximum.py", "test/fx2trt/converters/acc_op/test_maxpool.py", "test/fx2trt/converters/acc_op/test_min.py", "test/fx2trt/converters/acc_op/test_minimum.py", "test/fx2trt/converters/acc_op/test_narrow.py", "test/fx2trt/converters/acc_op/test_permute.py", "test/fx2trt/converters/acc_op/test_quantize_per_tensor.py", "test/fx2trt/converters/acc_op/test_relu.py", "test/fx2trt/converters/acc_op/test_reshape.py", "test/fx2trt/converters/acc_op/test_sigmoid.py", "test/fx2trt/converters/acc_op/test_size.py", "test/fx2trt/converters/acc_op/test_softmax.py", "test/fx2trt/converters/acc_op/test_split.py", "test/fx2trt/converters/acc_op/test_squeeze.py", "test/fx2trt/converters/acc_op/test_sum.py", "test/fx2trt/converters/acc_op/test_tanh.py", "test/fx2trt/converters/acc_op/test_tile.py", "test/fx2trt/converters/acc_op/test_topk.py", "test/fx2trt/converters/acc_op/test_unary_ops.py", "test/fx2trt/converters/acc_op/test_unsqueeze.py", "test/fx2trt/converters/vanilla/test_add.py", "test/fx2trt/converters/vanilla/test_convolution.py"], "labels": ["fb-exported", "ciflow/default"]}, "ecb7b38c00": {"title": "[PyTorch] Support additional arguments in Python record function (#65736)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65736\n\nWe ran into some limitations to extract PyTorch operator parameters through hooks or the execution graph. Some of these limitations are not due to the operator not exposing them, rather the inputs for these operators are already fused/processed in some cases (like embedding table). We want to be able to attach some metadata to the user scope record functions allowing the profilers to later extract these information.\n\nThe record function C++ API already supports taking inputs and outputs information. The corresponding Python interface does not support them and only allows a string name as record function parameter.\n\nThis diff adds support for user to optionally to add additional arguments to the record function in two ways.\n1. to remain backward compatible with `record_function_op`, we have added an optional string arg to the interface: `with record_function(name, arg_str)`.\n2. to support data dependency graph, we also have the new `torch.autograd._record_function_with_args_enter` and `torch.autograd._record_function_with_args_exit` functions to provide an interface where we can give additional tensor arguments. For now we imagine this can be used for debugging or analysis purpose. In this form, we currently support some basic data types as inputs: scalars, string, list, and tensor.\n\nExample usage:\n\n```\n# record_function operator with a name and optionally, a string for arguments.\nwith record_function(\"## TEST 1 ##\", \"[1, 2, 3]\"):\n    <actual module or operator>\n\n# more general form of record_function\na = _record_function_with_args_enter(\"## TEST 2 ##\", 1, False, 2.5, [u, u], \"hello\", u)\n<actual module or operator>\n_record_function_with_args_exit(a)\n\n```\nCorresponding outputs in execution graph:\n```\n    {\n      \"name\": \"## TEST 2 ##\", \"id\": 7, \"parent\": 3, \"fw_parent\": 0, \"scope\": 5, \"tid\": 1, \"fw_tid\": 0,\n      \"inputs\": [1,false,2.5,[6,6],\"hello\",6], \"input_shapes\": [[],[],[],[[3,4,5],[3,4,5]],[],[3,4,5]], \"input_types\": [\"Int\",\"Bool\",\"Double\",\"GenericList[Tensor(float),Tensor(float)]\",\"String\",\"Tensor(float)\"],\n      \"outputs\": [], \"output_shapes\": [], \"output_types\": []\n    },\n    {\n      \"name\": \"## TEST 1 ##\", \"id\": 3, \"parent\": 2, \"fw_parent\": 0, \"scope\": 5, \"tid\": 1, \"fw_tid\": 0,\n      \"inputs\": [\"1, 2, 3\"], \"input_shapes\": [[]], \"input_types\": [\"String\"],\n      \"outputs\": [], \"output_shapes\": [], \"output_types\": []\n    },\n```\n\nTest Plan:\n```\n=> buck build caffe2/test:profiler --show-output\n=> buck-out/gen/caffe2/test/profiler#binary.par test_profiler.TestRecordFunction\ntest_record_function (test_profiler.TestRecordFunction) ... Log file: /tmp/libkineto_activities_1651304.json\nNet filter:\nTarget net for iteration count:\nNet Iterations: 3\nINFO:2021-09-27 01:10:15 1651304:1651304 Config.cpp:424] Trace start time: 2021-09-27 01:10:30\nTrace duration: 500ms\nWarmup duration: 5s\nNet size threshold: 0\nGPU op count threshold: 0\nMax GPU buffer size: 128MB\nEnabled activities: cpu_op,user_annotation,external_correlation,cuda_runtime,cpu_instant_event\nManifold bucket: gpu_traces\nManifold object: tree/traces/clientAPI/0/1632730215/devvm2060.ftw0/libkineto_activities_1651304.json\nTrace compression enabled: 1\nINFO:2021-09-27 01:10:15 1651304:1651304 ActivityProfiler.cpp:536] Tracing starting in 14s\nINFO:2021-09-27 01:10:15 1651304:1651304 ActivityProfiler.cpp:48] Target net for iterations not specified - picking first encountered that passes net filter\nINFO:2021-09-27 01:10:15 1651304:1651304 ActivityProfiler.cpp:57] Tracking net PyTorch Profiler for 3 iterations\nINFO:2021-09-27 01:10:15 1651304:1651304 ActivityProfiler.cpp:126] Processing 1 CPU buffers\nINFO:2021-09-27 01:10:15 1651304:1651304 ActivityProfiler.cpp:686] Recorded nets:\nINFO:2021-09-27 01:10:15 1651304:1651304 ActivityProfiler.cpp:689] PyTorch Profiler: 1 iterations\nok\n\n----------------------------------------------------------------------\nRan 1 test in 0.021s\n\nOK\n```\n\nReviewed By: gdankel\n\nDifferential Revision: D31165259\n\nfbshipit-source-id: 15920aaef7138c666e5eca2a71c3bf33073eadc4", "pr_number": "65736", "files_changed": ["aten/src/ATen/record_function.cpp", "test/test_profiler.py", "torch/_C/_autograd.pyi", "torch/autograd/__init__.py", "torch/autograd/profiler.py", "torch/csrc/autograd/init.cpp", "torch/csrc/autograd/record_function_ops.cpp", "torch/csrc/autograd/record_function_ops.h"], "labels": ["fb-exported", "cla signed", "ciflow/default"]}, "87df043f63": {"title": "[Bootcamp][Pytorch]Add testing for complex parameters in Adagrad optimizer (#66501)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66501\n\nAdd testing for the Adagrad optimizer to ensure that it behaves as if complex numbers are two real numbers in R^2 as per issue 65711 on github\nghstack-source-id: 140414042\n\nTest Plan:\nbuck test mode/dev caffe2/test:optim -- 'test_adagrad_complex'\n\nhttps://pxl.cl/1R27M\n\nReviewed By: albanD\n\nDifferential Revision: D31584240\n\nfbshipit-source-id: 5c9938084566b8ea49cc8ff002789731f62fe87e", "pr_number": "66501", "files_changed": ["test/test_optim.py"], "labels": ["cla signed", "ciflow/default"]}, "6401658b08": {"title": "fix type error in hipify_python.py (#66164)", "body": "Summary:\n- [x] Fixed the Pyre type checking errors in `torch/utils/hipify/hipify_python.py`:\n```\ntorch/utils/hipify/hipify_python.py:196:8 Incompatible variable type [9]: clean_ctx is declared to have type `GeneratedFileCleaner` but is used as type `None`.\ntorch/utils/hipify/hipify_python.py:944:4 Incompatible variable type [9]: clean_ctx is declared to have type `GeneratedFileCleaner` but is used as type `None`.\n```\n\nFixing the issue: https://github.com/MLH-Fellowship/pyre-check/issues/78\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66164\n\nReviewed By: onionymous\n\nDifferential Revision: D31411443\n\nPulled By: 0xedward\n\nfbshipit-source-id: c69f8fb839ad1d5ba5e4a223e1322ae7207e1574", "pr_number": "66164", "files_changed": ["torch/utils/hipify/hipify_python.py"], "labels": ["triaged", "open source", "cla signed", "ciflow/default"]}, "84385c40e4": {"title": "Add output_mask (#66068)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/66068\n\nTest Plan: Imported from OSS\n\nReviewed By: dagitses\n\nDifferential Revision: D31431802\n\nPulled By: albanD\n\nfbshipit-source-id: 322aae5614dacb06fd45e513465b7a5cc11f4dbb", "pr_number": "66068", "files_changed": ["aten/src/ATen/native/GridSampler.cpp", "aten/src/ATen/native/cpu/GridSamplerKernel.cpp", "aten/src/ATen/native/cpu/GridSamplerKernel.h", "aten/src/ATen/native/cuda/GridSampler.cu", "aten/src/ATen/native/native_functions.yaml", "test/backward_compatibility/check_backward_compatibility.py", "tools/autograd/derivatives.yaml"], "labels": ["open source", "cla signed", "ciflow/default"]}, "f8d98b5a6d": {"title": "Compute input gradient only if required (CPU) (#66069)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/66069\n\nTest Plan: Imported from OSS\n\nReviewed By: dagitses\n\nDifferential Revision: D31431803\n\nPulled By: albanD\n\nfbshipit-source-id: d4caba5fa092e4ee7411502021836370082670b2", "pr_number": "66069", "files_changed": ["aten/src/ATen/native/cpu/GridSamplerKernel.cpp"], "labels": ["open source", "cla signed", "ciflow/default"]}, "8a40bb62f9": {"title": "Compute input gradient only if required (CUDA) (#66070)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/66070\n\nTest Plan: Imported from OSS\n\nReviewed By: dagitses\n\nDifferential Revision: D31431805\n\nPulled By: albanD\n\nfbshipit-source-id: 8c3de6632aaee168ec6fd7eb79a5af26973af9c5", "pr_number": "66070", "files_changed": ["aten/src/ATen/native/cuda/GridSampler.cu"], "labels": ["open source", "cla signed", "ciflow/default"]}, "a453ebc8ac": {"title": "Use interactive_embedded_interpreter to dynamicly loading various third-party libraries (#66512)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66512\n\nTLDR, we are able to use the interactive_embedded_interpreter (basically just torch::deploy interpreter with an interactive shell) to dynamicly load various third party libraries. We use the popular libraries numpy, scipy, regex, pandas for illustration purpose.\n\nA couple of changes need to be done for the interactive_embedded_interpreter:\n1, we need link with :embedded_interpreter_all rather than :embedded_interpreter so we can enable DEEPBIND and use our custom loader\n2, we provide a pylibRoot path to construct the InterpreterManager. The path will be added to the embedded interpreter's sys.path. Typically we can pass in the python library root path in a conda environment so torch::deploy interpreter can find all installed packages.\n3, we allow interactive_embedded_interpreter execute a script to ease recording the exploration of various python libraries.\nghstack-source-id: 140453213\n\nTest Plan:\nInstall numpy, scipy, regex, pandas in the conda environment or on the machine directly. Suppose /home/shunting/.local/lib/python3.8/site-packages/ is the root path for the installed libraries.\n\n- buck run mode/opt :interactive_embedded_interpreter -- --pylib_root=/home/shunting/.local/lib/python3.8/site-packages/ --pyscript=~/p7/iei_examples/try_regex.py\ncontent of try_regex.py:\n```\nimport regex\n\nprint(regex)\npat = r'(.+)\\1'\nprint(regex.match(pat, \"abcabc\"))\nprint(regex.match(pat, \"abcba\"))\n\nprint(\"bye\")\n```\n\n- buck run mode/opt :interactive_embedded_interpreter -- --pylib_root=/home/shunting/.local/lib/python3.8/site-packages/ --pyscript=~/p7/iei_examples/try_numpy.py\ncontent of try_numpy.py:\n```\nimport numpy as np\nprint(f\"numpy at {np}\")\na = np.random.rand(2, 3)\nb = np.random.rand(3, 2)\nprint(np.matmul(a, b))\n```\n\n- buck run mode/opt :interactive_embedded_interpreter -- --pylib_root=/home/shunting/.local/lib/python3.8/site-packages/ --pyscript=~/p7/iei_examples/try_scipy.py\ncontent of try_scipy.py:\n```\nimport numpy as np\nfrom scipy import linalg\n\nmat_a = np.array([[1, 0, 0, 0], [1, 1, 0, 0], [1, 2, 1, 0], [1, 3, 3, 1]])\nmat_b = linalg.inv(mat_a)\nprint(mat_b)\n```\n\n- buck run mode/opt :interactive_embedded_interpreter -- --pylib_root=/home/shunting/.local/lib/python3.8/site-packages/ --pyscript=~/p7/iei_examples/try_pandas.py\ncontent of try_pandas.py:\n```\nimport pandas as pd\nprint(f\"pandas at {pd}\")\ndf = pd.DataFrame({\n  \"col1\": [1, 2, 3, 4],\n  \"col2\": [2, 4, 8, 16],\n})\nprint(df)\n```\n\nReviewed By: suo\n\nDifferential Revision: D31587278\n\nfbshipit-source-id: c0b031c1fa71a77cdfeba1d04514f83127f79012", "pr_number": "66512", "files_changed": ["torch/csrc/deploy/deploy.cpp", "torch/csrc/deploy/deploy.h", "torch/csrc/deploy/interactive_embedded_interpreter.cpp"], "labels": ["cla signed", "ciflow/default"]}, "fdd9f49cf5": {"title": "add a note on numerical accuracy (#65947)", "body": "Summary:\nPer title\nFixes https://github.com/pytorch/pytorch/issues/54437\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65947\n\nReviewed By: albanD\n\nDifferential Revision: D31612445\n\nPulled By: ngimel\n\nfbshipit-source-id: 5c155891a088aef3b9813f253d0dc1ee4d51ae1c", "pr_number": "65947", "files_changed": ["docs/source/notes/numerical_accuracy.rst"], "labels": ["cla signed", "ciflow/default"]}, "f48f20e154": {"title": "Make ContainerHash compatible with const& types (#66497)", "body": "Summary:\n- this change should not impact existing use cases, but allows for\n  additional use cases where the container holds const types.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66497\n\nReviewed By: alanwaketan\n\nDifferential Revision: D31582242\n\nPulled By: wconstab\n\nfbshipit-source-id: 3a0e18b4afaf3c7ff93a0e3d09067ed066402b44", "pr_number": "66497", "files_changed": ["torch/csrc/lazy/core/hash.h"], "labels": ["ciflow/default"]}, "b792a77895": {"title": "Skip `interactive_embedded_interpreter.cpp` for clang-tidy (#66569)", "body": "Summary:\nFixes #{issue number}\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66569\n\nReviewed By: suo\n\nDifferential Revision: D31622885\n\nPulled By: malfet\n\nfbshipit-source-id: 61bad5ff3011f992cdd149724c935c098996d6a2", "pr_number": "66569", "files_changed": ["tools/linter/clang_tidy/__main__.py"], "labels": ["ciflow/default"]}, "c04bcde245": {"title": "Make empty* and *_like factory functions respect tensor subclasses (#65677)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/65243\n\ncc albanD\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65677\n\nReviewed By: dagitses\n\nDifferential Revision: D31432032\n\nPulled By: albanD\n\nfbshipit-source-id: 77f464974c7656c1206085aba9300471d7e0ef57", "pr_number": "65677", "files_changed": ["aten/src/ATen/native/native_functions.yaml", "test/test_python_dispatch.py", "tools/autograd/derivatives.yaml"], "labels": ["triaged", "open source", "cla signed", "ciflow/default", "module: __torch_dispatch__"]}, "86cf22cb1c": {"title": "Add OpInfo for torch.bucketize (#65821)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/65821\n\nReviewed By: malfet, mruberry\n\nDifferential Revision: D31386048\n\nPulled By: saketh-are\n\nfbshipit-source-id: fae7ec7b6b57436d87d38d421c5f3f52be4cdadd", "pr_number": "65821", "files_changed": ["aten/src/ATen/native/Bucketization.cpp", "aten/src/ATen/native/cuda/Bucketization.cu", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["cla signed", "ciflow/default", "ciflow/all"]}, "82986a17a6": {"title": "fix lint (#66572)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/66572\n\nTest Plan: Imported from OSS\n\nReviewed By: seemethere\n\nDifferential Revision: D31624043\n\nPulled By: suo\n\nfbshipit-source-id: 9db9cee3140d78c2a2f0c937be84755206fee1dd", "pr_number": "66572", "files_changed": ["aten/src/ATen/native/MathBitsFallback.h"], "labels": ["ciflow/default"]}, "37db650c9c": {"title": "[Static Runtime] Clone test does not use uninitialized memory (#66557)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66557\n\nThe test was previously using `at::empty_strided` to initialize one of its inputs. The contents of the tensor returned by this function are random, uninitialized memory. If we happened to get a NaN, this test would fail since `use_equalnan` was not set.\n\nTest Plan: `buck test caffe2/benchmarks/static_runtime:static_runtime_cpptest`\n\nReviewed By: hlu1\n\nDifferential Revision: D31611961\n\nfbshipit-source-id: 79a9476d0d6ce7a9f1412eefcef19bc2618c54b8", "pr_number": "66557", "files_changed": ["benchmarks/static_runtime/test_static_runtime.cc"], "labels": ["fb-exported", "cla signed", "ciflow/default"]}, "5f45927d15": {"title": "Autograd: Delay warnings until the end of backward execution (#66235)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/50209\n\nThis adds a new warning handler that stores all warnings in a shared\nqueue, which can be \"replayed\" at a later time and, crucially, on\nanother thread. Then, I use this inside the autograd engine to ensure\nthat warnings are processed by the handler registered on the main\nthread.\n\nFor testing, I also add an operator that always warns in the backward\npass and test that the warning is a normal Python warning.\n\ncc ezyang albanD zou3519 gqchen pearu nikitaved soulitzer Lezcano Varal7\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66235\n\nReviewed By: ejguan\n\nDifferential Revision: D31505413\n\nPulled By: albanD\n\nfbshipit-source-id: 1a7f60b038f55c20591c0748b9e86735b3fec2f9", "pr_number": "66235", "files_changed": ["aten/src/ATen/native/TestOps.cpp", "aten/src/ATen/native/native_functions.yaml", "c10/util/Exception.h", "test/test_autograd.py", "tools/autograd/derivatives.yaml", "tools/build_variables.bzl", "torch/csrc/Exceptions.cpp", "torch/csrc/Exceptions.h", "torch/csrc/autograd/FunctionsManual.cpp", "torch/csrc/autograd/FunctionsManual.h", "torch/csrc/autograd/engine.cpp", "torch/csrc/autograd/engine.h", "torch/csrc/autograd/utils/warnings.cpp", "torch/csrc/autograd/utils/warnings.h"], "labels": ["module: autograd", "triaged", "open source", "Merged", "cla signed", "ciflow/default"]}, "24202f7fb4": {"title": "Remove native_functions.yaml dependency from Activation.cu (#64499)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/64499\n\nThis moves the native functions into a separate Activation.cpp file,\nwhich calls into `launch_..._kernel` functions defined in `Activation.cu`.\nThe exception is `rrelu_with_noise` which is compilcated by the\nrandom number generation code, so I've moved it into its own file.\n\nTest Plan: Imported from OSS\n\nReviewed By: jbschlosser, ezyang\n\nDifferential Revision: D30867323\n\nPulled By: dagitses\n\nfbshipit-source-id: a4cd6f1fb1b1fed4cc356bf8b3778991ae2278ba", "pr_number": "64499", "files_changed": ["aten/src/ATen/cuda/ApplyGridUtils.cuh", "aten/src/ATen/cuda/CUDAApplyUtils.cuh", "aten/src/ATen/native/Activation.h", "aten/src/ATen/native/cpu/Activation.cpp", "aten/src/ATen/native/cuda/Activation.cpp", "aten/src/ATen/native/cuda/Activation.cu", "aten/src/ATen/native/cuda/Activation.h", "aten/src/ATen/native/cuda/RreluWithNoise.cu", "caffe2/CMakeLists.txt"], "labels": ["open source", "Merged", "cla signed", "ciflow/default"]}, "4e1c075542": {"title": "log_sigmoid: Use log1p for improved precision (#66441)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/20972\n\nlog_sigmoid calculates something like `log(1 + x)` where x is always a\npositive number less than one. This wastes floating point precision\nbecause the exponent always becomes zero. Instead, using\n`log1p(x)` gives the full mantissa precision around `x=0`.\n\nThis also fixes infinity propagation because the old code does,\n`exp(in - in)` when `in` is negative. Which for infinity, results in a\nNaN instead of 0.\n\ncc albanD mruberry jbschlosser walterddr\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66441\n\nReviewed By: bdhirsh\n\nDifferential Revision: D31619630\n\nPulled By: albanD\n\nfbshipit-source-id: e7867f3459a91e944b92f8ca42b6e0697b13f89b", "pr_number": "66441", "files_changed": ["aten/src/ATen/native/cpu/Activation.cpp", "aten/src/ATen/native/cuda/Activation.cu", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["module: nn", "open source", "Merged", "cla signed", "ciflow/default"]}, "e75de4f307": {"title": "remove a few unused THCTensor/Storage methods (#66555)", "body": "Summary:\nPer title\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66555\n\nReviewed By: mruberry\n\nDifferential Revision: D31620969\n\nPulled By: ngimel\n\nfbshipit-source-id: 1922ef523df473e8673a35c4a155b7b0cf000953", "pr_number": "66555", "files_changed": ["aten/src/THC/THCTensor.cpp", "aten/src/THC/THCTensor.hpp", "aten/src/THC/generic/THCStorage.cpp", "aten/src/THC/generic/THCStorage.h", "aten/src/THC/generic/THCTensor.cpp", "aten/src/THC/generic/THCTensor.h"], "labels": ["Merged", "cla signed", "ciflow/default"]}, "30d9fd9cf3": {"title": "Migrate USE_MAGMA config macro to ATen (#66390)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/66390\n\nTest Plan: Imported from OSS\n\nReviewed By: malfet, bdhirsh\n\nDifferential Revision: D31547712\n\nPulled By: ngimel\n\nfbshipit-source-id: 1b2ebc0d5b5d2199029274eabdd014f343cfbdd3", "pr_number": "66390", "files_changed": ["BUILD.bazel", "aten/src/ATen/CMakeLists.txt", "aten/src/ATen/cuda/CUDAConfig.h.in", "aten/src/ATen/cuda/detail/CUDAHooks.cpp", "aten/src/ATen/native/cuda/BatchLinearAlgebra.cpp", "aten/src/ATen/native/cuda/MiscUtils.h", "aten/src/THC/CMakeLists.txt", "aten/src/THC/THCGeneral.h", "aten/src/THC/THCGeneral.h.in", "aten/src/THC/THCTensorMathMagma.cpp"], "labels": ["module: porting", "open source", "Merged", "cla signed", "ciflow/default", "ciflow/cuda"]}, "160946e3f3": {"title": "Use `torch.empty()` instead of `torch.tensor()` in `torch.nn.Parameter` (#66486)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66486\n\nThe newly-introduced Python dispatcher mode (`__torch_dispatch__`) does not have support for `torch.tensor()` (see #64360) and this causes friction in the user experience if some `nn.Modules` use `torch.tensor()` either implicitly or explicitly.\n\nThis PR replaces calls to `torch.tensor()` in `Parameter`, `UninitializedParameter`, and `UninitializedBuffer` with an equivalent call to `torch.empty()` which serves the same purpose and is syntactically more readable.\nghstack-source-id: 140520931\n\nTest Plan: Since no behavioral change, run the existing unit and integration tests.\n\nReviewed By: pbelevich\n\nDifferential Revision: D31575587\n\nfbshipit-source-id: bd7bdeea54370f3e53dc13bd182b97d0f67146f5", "pr_number": "66486", "files_changed": ["torch/nn/parameter.py"], "labels": ["Merged", "cla signed", "ciflow/default"]}, "e1348973ac": {"title": "Add common_fx2trt.py (#66579)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66579\n\nDidn't commit this file in the PR that open sources fx2trt tests\n\nTest Plan: ci\n\nReviewed By: 842974287\n\nDifferential Revision: D31623354\n\nfbshipit-source-id: 6cedbe0f229da40499b83e6df28e16caca392d9c", "pr_number": "66579", "files_changed": ["torch/testing/_internal/common_fx2trt.py"], "labels": ["fb-exported", "Merged", "cla signed", "ciflow/default"]}, "fe41df3601": {"title": "Deprecate x.T on tensors of dimension other than 0 or 2 (#64180)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/64180\n\n**BC-breaking note:**\n\nThis PR deprecates the `Tensor.T` are not matrices. An upgrade guide is added to the\ndocumentation for `Tensor.T`.\n\nThis PR DOES NOT make this attribute to throw an error when called on a tensor of `dim != 2`,\nbut this will be its behavior in a future PyTorch release.\n\ncc mruberry rgommers pmeier asmeurer leofang AnirudhDagar asi1024 emcastillo kmaehashi heitorschueroff\n\nTest Plan: Imported from OSS\n\nReviewed By: bdhirsh\n\nDifferential Revision: D31610611\n\nPulled By: anjali411\n\nfbshipit-source-id: af8ff7e862790dda9f06921de005b3f6fd0803c3", "pr_number": "64180", "files_changed": ["aten/src/ATen/native/TensorShape.cpp", "torch/_tensor_docs.py"], "labels": ["open source", "module: deprecation", "Merged", "cla signed", "module: python array api", "ciflow/default"]}, "77f98ea5e0": {"title": "assert no duplicate yaml keys in codegen (#66238)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66238\n\nThe codegen should error if it sees two yaml entries with the same key. The default behavior of python's yaml loader is to overwrite duplicate keys with the new value.\n\nThis would have caught a nasty bug that showed up in https://github.com/pytorch/pytorch/pull/66225/files#r723796194.\n\nI tested it on that linked PR, to confirm that it errors correctly (and gives the line number containing the duplicate).\n\nTest Plan: Imported from OSS\n\nReviewed By: dagitses, albanD, sean-ngo\n\nDifferential Revision: D31464585\n\nPulled By: bdhirsh\n\nfbshipit-source-id: 5b35157ffa9a933bf4b344c4b9fe2878698370a3", "pr_number": "66238", "files_changed": ["tools/codegen/utils.py"], "labels": ["Merged", "cla signed", "ciflow/default"]}, "49a1d7bfcb": {"title": "[opinfo] elemwise parcel : isfinite, isinf, isposinf, isneginf, isnan, isreal (#66400)", "body": "Summary:\nAdds OpInfo for `isfinite, isinf, isposinf, isneginf, isnan, isreal`\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66400\n\nReviewed By: bdhirsh\n\nDifferential Revision: D31602998\n\nPulled By: mruberry\n\nfbshipit-source-id: 235cc414f373f014f4822a72deb1a04a58ad4a7c", "pr_number": "66400", "files_changed": ["test/test_jit_fuser_te.py", "test/test_unary_ufuncs.py", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["triaged", "open source", "Merged", "cla signed", "ciflow/default", "ciflow/all"]}, "76f3b07caf": {"title": "quantization docs: remove erroneous rebase artifact (#66577)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66577\n\nThere was a rebase artifact erroneously landed to quantization docs,\nthis PR removes it.\n\nTest Plan:\nCI\n\nImported from OSS\n\nReviewed By: soulitzer\n\nDifferential Revision: D31651350\n\nfbshipit-source-id: bc254cbb20724e49e1a0ec6eb6d89b28491f9f78", "pr_number": "66577", "files_changed": ["docs/source/quantization-support.rst"], "labels": ["Merged", "cla signed", "ciflow/default"]}, "833ede33ed": {"title": "Fix ubsan in concat_split_op.h (#66283)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66283\n\nFixes\n```\nUndefinedBehaviorSanitizer: nullptr-with-nonzero-offset caffe2/caffe2/operators/concat_split_op.h:185:52\n```\n\nTest Plan: Sandcastle\n\nReviewed By: swolchok\n\nDifferential Revision: D31486274\n\nfbshipit-source-id: 20128056f19cf814fdc3e6e144cf9208a4080d6a", "pr_number": "66283", "files_changed": ["caffe2/operators/concat_split_op.h"], "labels": ["fb-exported", "Merged", "cla signed", "ciflow/default"]}, "583217fe37": {"title": "changes for pytorch issue 55577 (#66571)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66571\n\nchanges for pytorch issue 55577\n\nTest Plan:\nRan test:\npython test/test_jit.py TestDict\n\nReviewed By: tugsbayasgalan\n\nDifferential Revision: D31622633\n\nfbshipit-source-id: 171c68a65b1d0bf769b3d95f103daba375e95335", "pr_number": "66571", "files_changed": ["test/jit/test_list_dict.py", "test/jit/test_module_containers.py", "test/jit/test_with.py", "torch/jit/frontend.py"], "labels": ["oncall: jit", "fb-exported", "Merged", "cla signed", "ciflow/default"]}, "6436bd3d5d": {"title": "Clarify topk doc (#65938)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/50331\n<img width=\"855\" alt=\"Screen Shot 2021-10-01 at 11 23 23 AM\" src=\"https://user-images.githubusercontent.com/17888388/136036611-f2bd9c77-61b4-4ab8-85eb-44f50c1e03d7.png\">\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65938\n\nReviewed By: bdhirsh\n\nDifferential Revision: D31314875\n\nPulled By: samdow\n\nfbshipit-source-id: bdd9425fd748710f8a64ed1989e1938dd358780f", "pr_number": "65938", "files_changed": ["torch/_torch_docs.py"], "labels": ["Merged", "cla signed", "ciflow/default"]}, "a58852fd44": {"title": "Fix fx2trt broken unit test (#66696)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66696\n\nD31511082 (https://github.com/pytorch/pytorch/commit/9918fd8305dd770b467d96d91b4533477c5628ed) moved unit test but didn't add proper target in build file, fix it in this diff.\n\nTest Plan: buck test mode/opt caffe2/test/fx2trt/converters/...\n\nReviewed By: 842974287\n\nDifferential Revision: D31667697\n\nfbshipit-source-id: 49e04afa323b27a1408c9bc2b5061b6529ced985", "pr_number": "66696", "files_changed": ["test/fx2trt/converters/acc_op/test_adaptive_avgpool.py", "test/fx2trt/converters/acc_op/test_avgpool.py", "test/fx2trt/converters/acc_op/test_batchnorm.py", "test/fx2trt/converters/acc_op/test_binary_ops.py", "test/fx2trt/converters/acc_op/test_cat.py", "test/fx2trt/converters/acc_op/test_clamp.py", "test/fx2trt/converters/acc_op/test_convolution.py", "test/fx2trt/converters/acc_op/test_dequantize.py", "test/fx2trt/converters/acc_op/test_flatten.py", "test/fx2trt/converters/acc_op/test_gelu.py", "test/fx2trt/converters/acc_op/test_getitem.py", "test/fx2trt/converters/acc_op/test_layer_norm.py", "test/fx2trt/converters/acc_op/test_linear.py", "test/fx2trt/converters/acc_op/test_matmul.py", "test/fx2trt/converters/acc_op/test_max.py", "test/fx2trt/converters/acc_op/test_maximum.py", "test/fx2trt/converters/acc_op/test_maxpool.py", "test/fx2trt/converters/acc_op/test_min.py", "test/fx2trt/converters/acc_op/test_minimum.py", "test/fx2trt/converters/acc_op/test_narrow.py", "test/fx2trt/converters/acc_op/test_permute.py", "test/fx2trt/converters/acc_op/test_quantize_per_tensor.py", "test/fx2trt/converters/acc_op/test_relu.py", "test/fx2trt/converters/acc_op/test_reshape.py", "test/fx2trt/converters/acc_op/test_sigmoid.py", "test/fx2trt/converters/acc_op/test_size.py", "test/fx2trt/converters/acc_op/test_softmax.py", "test/fx2trt/converters/acc_op/test_split.py", "test/fx2trt/converters/acc_op/test_squeeze.py", "test/fx2trt/converters/acc_op/test_sum.py", "test/fx2trt/converters/acc_op/test_tanh.py", "test/fx2trt/converters/acc_op/test_tile.py", "test/fx2trt/converters/acc_op/test_topk.py", "test/fx2trt/converters/acc_op/test_unary_ops.py", "test/fx2trt/converters/acc_op/test_unsqueeze.py", "test/fx2trt/converters/vanilla/test_add.py", "test/fx2trt/converters/vanilla/test_convolution.py"], "labels": ["fb-exported", "Merged", "cla signed", "ciflow/default", "fx"]}, "0b8dc0f04a": {"title": "add BFloat16 operators on CPU: logaddexp, logaddexp2, remainder (#63621)", "body": "Summary:\nFixes #{issue number}\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/63621\n\nReviewed By: H-Huang\n\nDifferential Revision: D31640811\n\nPulled By: mruberry\n\nfbshipit-source-id: 1fd061b65c196398738018eefc52bf459e424b1c", "pr_number": "63621", "files_changed": ["aten/src/ATen/native/cpu/BinaryOpsKernel.cpp", "test/test_binary_ufuncs.py", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["triaged", "open source", "Merged", "cla signed"]}, "bd25f92e81": {"title": "Fix Wextra issues in Half.h (#66643)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66643\n\nFixes:\n```\ncaffe2/c10/util/Half.h:456:14: error: comparison of integers of different signs: 'long' and 'unsigned long' [-Werror,-Wsign-compare]\n    return f > limit::max() ||\n           ~ ^ ~~~~~~~~~~~~\n```\n\nTest Plan: Sandcastle\n\nReviewed By: ngimel\n\nDifferential Revision: D31656816\n\nfbshipit-source-id: 7623d20e166a9e95a949ebd8b23793f24960cf07", "pr_number": "66643", "files_changed": ["c10/util/Half.h"], "labels": ["fb-exported", "Merged", "cla signed", "ciflow/default"]}, "b5b7d6a3a6": {"title": "EmbeddingBackward exclusive_scan thrust->cub (#66566)", "body": "Summary:\nFixes #{issue number}\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66566\n\nReviewed By: H-Huang\n\nDifferential Revision: D31637660\n\nPulled By: ngimel\n\nfbshipit-source-id: 8093432bb9a9b902bb6bab7da221f0bcd7e9fb34", "pr_number": "66566", "files_changed": ["aten/src/ATen/cuda/cub.cuh", "aten/src/ATen/native/cuda/Embedding.cu", "aten/src/ATen/native/cuda/EmbeddingBackwardKernel.cu", "aten/src/ATen/native/cuda/EmbeddingBag.cu", "aten/src/ATen/native/cuda/Indexing.cu", "aten/src/ATen/native/cuda/LegacyThrustHelpers.cu", "aten/src/ATen/native/cuda/Randperm.cu", "aten/src/ATen/native/cuda/Sort.cu", "aten/src/ATen/native/cuda/UniqueCub.cu"], "labels": ["triaged", "open source", "Merged", "cla signed", "ciflow/default"]}, "a25648953c": {"title": "Add `warn_only` kwarg to `use_deterministic_algorithms` (#66233)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/64883\n\nAdds a `warn_only` kwarg to `use_deterministic_algorithms`. When enabled, calling an operation that does not have a deterministic implementation will raise a warning, rather than an error.\n\n`torch.testing._internal.common_device_type.expectedAlertNondeterministic` is also refactored and documented in this PR to make it easier to use and understand.\n\ncc mruberry kurtamohler\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66233\n\nReviewed By: bdhirsh\n\nDifferential Revision: D31616481\n\nPulled By: mruberry\n\nfbshipit-source-id: 059634a82d54407492b1d8df08f059c758d0a420", "pr_number": "66233", "files_changed": ["aten/src/ATen/Context.cpp", "aten/src/ATen/Context.h", "test/test_torch.py", "torch/_C/__init__.pyi.in", "torch/__init__.py", "torch/csrc/Module.cpp", "torch/testing/_internal/common_device_type.py", "torch/testing/_internal/common_utils.py"], "labels": ["triaged", "module: determinism", "open source", "Merged", "cla signed", "ciflow/default"]}, "d1b6121935": {"title": "Revert D31656999: Add meta support to tensor range factories", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD31656999 (https://github.com/pytorch/pytorch/commit/7400f34b8e3692384ab19de0914271e01cb3fd88)\n\nOriginal commit changeset: 06e7f3655b94\n\nfbshipit-source-id: 2f9d8d1acbb01c5105ece73472e5c1f5f90886ee", "pr_number": null, "files_changed": ["aten/src/ATen/native/Histogram.cpp", "aten/src/ATen/native/RangeFactories.cpp", "aten/src/ATen/native/native_functions.yaml", "test/test_tensor_creation_ops.py"], "labels": []}, "06cfdfae0e": {"title": "Promote integral inputs to floating for `torch.logsumexp` (#63393)", "body": "Summary:\nFixed https://github.com/pytorch/pytorch/issues/56132, Integral inputs of `torch.logsumexp` would be promoted to the floating point type.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/63393\n\nReviewed By: ezyang\n\nDifferential Revision: D30512180\n\nPulled By: mruberry\n\nfbshipit-source-id: fbde3605c15b930411d0d1eb3a132b0088187097", "pr_number": "63393", "files_changed": ["aten/src/ATen/native/ReduceOps.cpp", "test/test_reductions.py", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["triaged", "open source", "Merged", "cla signed", "ciflow/default"]}, "1e2b2ee5ff": {"title": "sort_out_cuda: Use custom kernels to fill index tensors (#66668)", "body": "Summary:\nThese stable sorts currently use a combination of `at::arange`, view ops and `tensor.copy_` to fill in the initial values for the indices before calling into `CUB` to do the actual sort. This is somewhat inefficient because it requires 2 to 4 kernel launches, and the copies all use strided kernels instead of the more efficient contiguous kernels. Instead, a fairly straight-forward custom kernel is more efficient in terms of both CUDA and CPU runtime.\n\nIn a simple benchmark I profiled `a.sort(stable=True, dim=1)` for different shapes and single out the kernel invocations for intitializing the index tensors (i.e. the non-`cub` kernels). Note that when the batch dim is `<128` we call `segmented_sort_pairs_by_full_sort` instead of `segmented_sort_pairs`:\n\n| shape        | Master (us) | This PR (us) |\n|--------------|:-----------:|:------------:|\n| (100, 1000)  |    5.000    |     2.300    |\n| (1000, 100)  |    2.070    |     1.090    |\n| (100, 10000) |    87.34    |     26.47    |\n| (1000, 1000) |    28.63    |     20.27    |\n\nOf course for sufficiently large inputs, the overall runtime is dominated by the actual sort. But I have another motive of wanting to remove operator the calls from the middle of this kernel launch code. This change makes it easier to split the kernel code that needs to be compiled with `nvcc` into it's own file that doesn't include `Tensor.h`, similar to what I'm doing in https://github.com/pytorch/pytorch/issues/66620.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66668\n\nReviewed By: H-Huang\n\nDifferential Revision: D31693722\n\nPulled By: ngimel\n\nfbshipit-source-id: 5765926e4dbbc7a20d2940c098ed093b3de2204e", "pr_number": "66668", "files_changed": ["aten/src/ATen/native/cuda/Sort.cu"], "labels": ["triaged", "open source", "Merged", "cla signed", "ciflow/default"]}, "32ac001e4d": {"title": "Suppress deprecated copy in vec256_qint.h (#66646)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/66646\n\nTest Plan: Sandcastle\n\nReviewed By: ngimel\n\nDifferential Revision: D31660387\n\nfbshipit-source-id: a1ea9702a8b33f78a7201a1d9214065c2fb930b1", "pr_number": "66646", "files_changed": ["aten/src/ATen/cpu/vec/vec256/vec256_qint.h"], "labels": ["fb-exported", "Merged", "cla signed", "ciflow/default"]}, "8c5928bd78": {"title": "add frozen_numpy as a builtin library to torch::deploy (#66297)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66297\n\nLink register_numpy.cpp with the embedded interpreter will register numpy as a builtin library.\n\nTest Plan: Add unit test to test basic numpy functionality in torch::deploy like creating random matrices, matric multiplication.\n\nReviewed By: suo\n\nDifferential Revision: D31490434\n\nfbshipit-source-id: b052ce01fc64fb0efee846feb0acc1f107ba13e0", "pr_number": "66297", "files_changed": ["torch/csrc/deploy/interpreter/register_numpy.cpp", "torch/csrc/deploy/test_deploy.cpp"], "labels": ["Merged", "cla signed", "ciflow/default"]}, "8854817f44": {"title": "Implement Python Array API `asarray` function. (#60627)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/60627\n\nIn this PR, the core of `frombuffer` and `fromDLPack` onto _tensor_new.cpp_. `asarray`\nuses such refactored functions for interpreting the object as a tensor. We follow the\nPython Array API standard found:\n\nhttps://data-apis.org/array-api/latest/API_specification/creation_functions.html?highlight=asarray\n\nTest Plan: Imported from OSS\n\nReviewed By: H-Huang\n\nDifferential Revision: D31640510\n\nPulled By: mruberry\n\nfbshipit-source-id: d0869e0d73cb50023d5866b001dac5d34ca30dfd", "pr_number": "60627", "files_changed": ["docs/source/torch.rst", "test/test_buffer_protocol.py", "test/test_tensor_creation_ops.py", "tools/pyi/gen_pyi.py", "torch/_torch_docs.py", "torch/csrc/Module.cpp", "torch/csrc/autograd/python_torch_functions_manual.cpp", "torch/csrc/utils/tensor_new.cpp", "torch/csrc/utils/tensor_new.h", "torch/csrc/utils/tensor_numpy.cpp", "torch/csrc/utils/tensor_numpy.h", "torch/overrides.py"], "labels": ["open source", "Merged", "cla signed", "ciflow/default"]}, "719d43a2a2": {"title": "Revert D31547709: Remove THCGeneral.cpp", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD31547709 (https://github.com/pytorch/pytorch/commit/aa0c31876bbd0f9813ec6f000154a24c91beece9)\n\nOriginal commit changeset: 059c47621863\n\nfbshipit-source-id: e8c3597f2badbc5ecf356b381edea06a07331f24", "pr_number": null, "files_changed": ["BUILD.bazel", "aten/src/ATen/cuda/detail/CUDAHooks.cpp", "aten/src/ATen/native/cuda/Dropout.cu", "aten/src/ATen/native/cuda/TensorFactories.cu", "aten/src/ATen/native/cuda/Unique.cu", "aten/src/ATen/native/cudnn/Conv_v7.cpp", "aten/src/THC/CMakeLists.txt", "aten/src/THC/THCGeneral.cpp", "aten/src/THC/THCGeneral.h", "aten/src/THC/THCStorage.cpp", "torch/csrc/cuda/Stream.cpp", "torch/csrc/generic/StorageSharing.cpp", "torch/csrc/generic/serialization.cpp"], "labels": []}, "3b4cb9ddca": {"title": "Revert D31577488: Migrate THCState to ATen", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD31577488 (https://github.com/pytorch/pytorch/commit/65adf1dfa2905f9a8397a55b30c748e1707c848b)\n\nOriginal commit changeset: 90604f30854f\n\nfbshipit-source-id: 3d7e35b3d6ea94f2c999bcf821b33a9cf1db01ee", "pr_number": null, "files_changed": ["aten/src/ATen/cuda/PeerToPeerAccess.cpp", "aten/src/ATen/cuda/PeerToPeerAccess.h", "aten/src/ATen/cuda/detail/CUDAHooks.cpp", "aten/src/ATen/cuda/detail/CUDAHooks.h", "aten/src/ATen/native/cuda/Copy.cu", "aten/src/ATen/native/cuda/Resize.cpp", "aten/src/THC/CMakeLists.txt", "aten/src/THC/THCGeneral.cpp", "aten/src/THC/THCGeneral.h", "aten/src/THC/THCGeneral.hpp", "aten/src/THC/THCTensorMathMagma.cpp"], "labels": []}, "53aac4b6f3": {"title": "[PyTorch] Allow override for macro `HAS_DEMANGLE` (#66540)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66540\n\nCurrently the macro `HAS_DEMANGLE` is determined by compiler predefined macros. Here I'm adding an option to allow `HAS_DEMANGLE` to be defined in build files.\n\nTest Plan: Rely on CI\n\nReviewed By: poweic\n\nDifferential Revision: D31600007\n\nfbshipit-source-id: 76cf088b0f5ee940e977d3b213f1446ea64be036", "pr_number": "66540", "files_changed": ["c10/macros/Macros.h"], "labels": ["oncall: jit", "fb-exported", "Merged", "cla signed", "ciflow/default"]}, "f4a7273b5c": {"title": "Set test owners for module: ci (#66796)", "body": "Summary:\nAction based on RFC https://github.com/pytorch/pytorch/issues/66232\n\ncc seemethere malfet pytorch/pytorch-dev-infra\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66796\n\nReviewed By: seemethere\n\nDifferential Revision: D31732391\n\nPulled By: janeyx99\n\nfbshipit-source-id: b894eab8a4a8737165d1ba7b536e1232f6c07a8f", "pr_number": "66796", "files_changed": ["test/test_determination.py", "test/test_import_stats.py"], "labels": ["module: ci", "Merged", "cla signed", "ciflow/default"]}, "62e89f692f": {"title": "[doc] typo (#66754)", "body": "Summary:\nThis PR fixes a typo in the `torch/autograd/function.py` doc\n\n-----------------------\n\nAdditionally, the example at https://pytorch.org/docs/master/autograd.html#torch.autograd.Function doesn't quite compile:\n```\n'builtin_function_or_method' object has no attribute 'exp'\n```\neven though `i.exp()` is a valid function if `i` is a tensor.\n\nI changed it to:\n```\nresult = torch.exp(i)\n```\nbut python doesn't like it either:\n```\nTypeError: exp(): argument 'input' (position 1) must be Tensor, not builtin_function_or_method\n```\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66754\n\nReviewed By: albanD\n\nDifferential Revision: D31729400\n\nPulled By: soulitzer\n\nfbshipit-source-id: eef783bcdc8d4693a8b7f1ab581e948abc0f9b94", "pr_number": "66754", "files_changed": ["torch/autograd/function.py"], "labels": ["open source", "Merged", "cla signed", "ciflow/default"]}, "09c4e73c95": {"title": "[PyTorch] Fix unnecessary shared_ptr copies in FutureType (#66704)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66704\n\nMissing moves in the construction path.\nghstack-source-id: 140746391\n\nTest Plan: CI\n\nReviewed By: suo\n\nDifferential Revision: D31694296\n\nfbshipit-source-id: 3bed477c811069248611efdb57ad27c6ca233442", "pr_number": "66704", "files_changed": ["aten/src/ATen/core/jit_type.h"], "labels": ["Merged", "cla signed", "ciflow/default"]}, "eb1eefc399": {"title": "[PyTorch] Fix unnecessary shared_ptr copies in DictType (#66702)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66702\n\nMissing moves in the construction path and forced copies of the key & value type on access.\nghstack-source-id: 140744707\n\nTest Plan: CI\n\nReviewed By: suo\n\nDifferential Revision: D31693818\n\nfbshipit-source-id: 4c5d2359f58148744621abe81429e56e7889f754", "pr_number": "66702", "files_changed": ["aten/src/ATen/core/jit_type.h"], "labels": ["Merged", "cla signed", "ciflow/default"]}, "c373e188d8": {"title": "[PyTorch] Fix extra refcount bumps in unifyTypes (#66718)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66718\n\nSome missing moves and use of cast instead of castRaw (due to a previous automated fixup only being a partial fix).\nghstack-source-id: 140755229\n\nTest Plan: CI\n\nReviewed By: suo\n\nDifferential Revision: D31697115\n\nfbshipit-source-id: 86743f8982951a58638ba244b3a92d3737dde58b", "pr_number": "66718", "files_changed": ["aten/src/ATen/core/type.cpp"], "labels": ["Merged", "cla signed", "ciflow/default"]}, "6bde474066": {"title": "[PyTorch] Fix extra refcount bumps in matchTypeVariables (#66719)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66719\n\nSome cast that could be castRaw. Parameters did not need to force a refcount bump.\nghstack-source-id: 140756356\n\nTest Plan: CI\n\nReviewed By: suo\n\nDifferential Revision: D31697455\n\nfbshipit-source-id: 87a8cba221a7ae53f2a485acafd31622e9328ff0", "pr_number": "66719", "files_changed": ["aten/src/ATen/core/jit_type.h", "aten/src/ATen/core/type.cpp"], "labels": ["Merged", "cla signed", "ciflow/default"]}, "7fad47e522": {"title": "`torch.linalg.lstsq`: forward/backward AD support (#65054)", "body": "Summary:\nAs per title.\n\ncc ezyang albanD zou3519 gqchen pearu nikitaved soulitzer Lezcano Varal7 jianyuh mruberry walterddr IvanYashchuk xwang233\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65054\n\nReviewed By: zou3519\n\nDifferential Revision: D31729468\n\nPulled By: albanD\n\nfbshipit-source-id: ab7df824bc80128e7f64f6444c7a4baa4786c161", "pr_number": "65054", "files_changed": ["tools/autograd/derivatives.yaml", "tools/autograd/gen_variable_type.py", "torch/csrc/autograd/FunctionsManual.cpp", "torch/csrc/autograd/FunctionsManual.h", "torch/testing/_internal/common_device_type.py", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["module: autograd", "triaged", "open source", "module: linear algebra", "complex_autograd", "Merged", "cla signed", "ciflow/default"]}, "d5a25faf7a": {"title": "[PyTorch] Fix unnecessary shared_ptr copies in EnumType (#66714)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66714\n\nForced copy in getValueType and unnecessary use of cast over castRaw.\nghstack-source-id: 140752791\n\nTest Plan: CI\n\nReviewed By: suo\n\nDifferential Revision: D31696164\n\nfbshipit-source-id: fc2316617a61ca32f1fb952fb0af18b8784a606b", "pr_number": "66714", "files_changed": ["aten/src/ATen/core/jit_type.h"], "labels": ["Merged", "cla signed", "ciflow/default"]}, "393299b124": {"title": "[PyTorch] Fix unnecessary shared_ptr copies in RRefType (#66706)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66706\n\nMissing moves in the construction path.\nghstack-source-id: 140746585\n\nTest Plan: CI\n\nReviewed By: suo\n\nDifferential Revision: D31694356\n\nfbshipit-source-id: 8e2bf2dd41f3f65fc06e30ffd5fddd487d01aaa8", "pr_number": "66706", "files_changed": ["aten/src/ATen/core/jit_type.h"], "labels": ["Merged", "cla signed", "ciflow/default"]}, "1fcbd8fa15": {"title": "[PyTorch] Fix extra refcount bumps in tryEvalTypeVariables (#66722)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66722\n\nMissing move, s/cast/castRaw/, and take TypePtr arg by const ref because we only sometimes need to take ownership.\nghstack-source-id: 140757141\n\nTest Plan: CI\n\nReviewed By: suo\n\nDifferential Revision: D31697631\n\nfbshipit-source-id: 04afe13688c6e2aaf79157400c0a44021cb8179d", "pr_number": "66722", "files_changed": ["aten/src/ATen/core/jit_type.h", "aten/src/ATen/core/type.cpp"], "labels": ["Merged", "cla signed", "ciflow/default"]}, "8637556d23": {"title": "Migrate THCState to ATen (#66765)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66765\n\nThis guts `THCState` to simply be an empty struct, as well as:\n- moving `THCState_getPeerToPeerAccess` and its cache into `ATen`.\n- cleaning up dead code in `THCGeneral.cpp`\n- moving `THCudaInit` and `THCMagma_init` into `CUDAHooks::initCUDA`\n\nTest Plan: Imported from OSS\n\nReviewed By: zou3519\n\nDifferential Revision: D31721648\n\nPulled By: ngimel\n\nfbshipit-source-id: 772b24787656a95f9e3fcb287d912b1c3400f32d", "pr_number": "66765", "files_changed": ["aten/src/ATen/cuda/PeerToPeerAccess.cpp", "aten/src/ATen/cuda/PeerToPeerAccess.h", "aten/src/ATen/cuda/detail/CUDAHooks.cpp", "aten/src/ATen/cuda/detail/CUDAHooks.h", "aten/src/ATen/native/cuda/BatchLinearAlgebra.cpp", "aten/src/ATen/native/cuda/Copy.cu", "aten/src/ATen/native/cuda/Resize.cpp", "aten/src/THC/CMakeLists.txt", "aten/src/THC/THCGeneral.cpp", "aten/src/THC/THCGeneral.h", "aten/src/THC/THCGeneral.hpp", "aten/src/THC/THCTensorMathMagma.cpp"], "labels": ["open source", "Merged", "cla signed", "ciflow/default", "ciflow/cuda"]}, "6a7296be9c": {"title": "[PyTorch] Use castRaw in InterfaceType (#66728)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66728\n\nTwo extra refcount bumps.\nghstack-source-id: 140760872\n\nTest Plan: CI\n\nReviewed By: suo\n\nDifferential Revision: D31698577\n\nfbshipit-source-id: 1f50195a99f98f857abc9b03b4254519c316fefe", "pr_number": "66728", "files_changed": ["aten/src/ATen/core/jit_type.h", "aten/src/ATen/core/type.cpp"], "labels": ["Merged", "cla signed", "ciflow/default"]}, "44fd312604": {"title": "[PyTorch] Use intrusive_ptr to save space in KernelFunction (#65618)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65618\n\nThis saves 8 bytes per KernelFunction, which should help in resource-constrained environments.\nghstack-source-id: 140731069\n\nTest Plan: CI\n\nReviewed By: ezyang\n\nDifferential Revision: D25405736\n\nfbshipit-source-id: 757c0f1387da9147e46ac69af2aa9fffd2998e35", "pr_number": "65618", "files_changed": ["aten/src/ATen/core/boxing/KernelFunction.h", "aten/src/ATen/core/boxing/KernelFunction_impl.h", "aten/src/ATen/core/boxing/impl/make_boxed_from_unboxed_functor.h", "c10/util/intrusive_ptr.h"], "labels": ["Merged", "cla signed", "ciflow/default"]}, "d0a63c978b": {"title": "[PyTorch][easy] Don't copy string in TensorType::repr_str unnecessarily (#66699)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66699\n\nstd::string::operator+ will copy the string an extra time even if the argument is `\"\"`. See https://godbolt.org/z/3sM5h1qTo\nghstack-source-id: 140743822\n\nTest Plan: CI\n\nReviewed By: suo\n\nDifferential Revision: D31693522\n\nfbshipit-source-id: 6a8033c90366904b9aff44214b600cfb255a0809", "pr_number": "66699", "files_changed": ["aten/src/ATen/core/jit_type.h"], "labels": ["Merged", "cla signed", "ciflow/default"]}, "c9c447f4be": {"title": "[PyTorch] Fix missing moves in ListType (#66701)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66701\n\nWe own the argument vector.\nghstack-source-id: 140760983\n\nTest Plan: CI\n\nReviewed By: suo\n\nDifferential Revision: D31693645\n\nfbshipit-source-id: 02829bc3c728f6d1d07be08b0d977eee1efee38f", "pr_number": "66701", "files_changed": ["aten/src/ATen/core/jit_type.h"], "labels": ["Merged", "cla signed", "ciflow/default"]}, "a17a4e93ce": {"title": "[PyTorch][easy] Fix missing move in UnionType::createWithContained (#66691)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66691\n\nDoes what it says on the tin.\nghstack-source-id: 140736047\n\nTest Plan: CI\n\nReviewed By: suo\n\nDifferential Revision: D31691627\n\nfbshipit-source-id: 21a5d0248bf3412f5af36260597a5f663ab34361", "pr_number": "66691", "files_changed": ["aten/src/ATen/core/jit_type.h"], "labels": ["cla signed", "ciflow/default"]}, "d05c1ec007": {"title": "Add lazy Node base and associated infra (#66601)", "body": "Summary:\n- Adds Node base class and unit tests\n- Also adds metadata utils to enable source code annotation and scope tracking\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66601\n\nTest Plan: Add new unit tests\n\nReviewed By: desertfire\n\nDifferential Revision: D31634044\n\nfbshipit-source-id: a042d54f06fbc480acfc63c18d43cb6fceb6fea5", "pr_number": "66601", "files_changed": ["test/cpp/lazy/CMakeLists.txt", "test/cpp/lazy/test_ir.cpp", "tools/build_variables.bzl", "torch/csrc/lazy/core/config.cpp", "torch/csrc/lazy/core/config.h", "torch/csrc/lazy/core/hash.cpp", "torch/csrc/lazy/core/hash.h", "torch/csrc/lazy/core/ir.cpp", "torch/csrc/lazy/core/ir.h", "torch/csrc/lazy/core/ir_metadata.cpp", "torch/csrc/lazy/core/ir_metadata.h"], "labels": ["fb-exported", "Merged", "cla signed", "ciflow/default", "ciflow/all"]}, "08a464a9f3": {"title": "[PyTorch] Pass c10::optional<bool> to Stride ctor by value (#66698)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66698\n\nthis type should fit in a register; no need to pass by reference.\nghstack-source-id: 140742830\n\nTest Plan: CI\n\nReviewed By: suo\n\nDifferential Revision: D31693291\n\nfbshipit-source-id: 299fb3d1830a059b59268487c22e030446c3496e", "pr_number": "66698", "files_changed": ["aten/src/ATen/core/jit_type.h"], "labels": ["cla signed", "ciflow/default"]}, "8f09292c5e": {"title": "add `OpInfo` for `torch.nn.functional.pairwise_distance` (#65460)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65460\n\ncc albanD mruberry jbschlosser walterddr\n\nTest Plan: Imported from OSS\n\nReviewed By: gchanan\n\nDifferential Revision: D31111701\n\nPulled By: zou3519\n\nfbshipit-source-id: a4034418cf8d14f584134a16d822181703858f99", "pr_number": "65460", "files_changed": ["torch/testing/_internal/common_methods_invocations.py"], "labels": ["module: nn", "module: tests", "open source", "cla signed", "ciflow/default"]}, "1164118fc2": {"title": "add `OpInfo` for `torch.nn.pixel_shuffle` (#65467)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/65467\n\nTest Plan: Imported from OSS\n\nReviewed By: mrshenli\n\nDifferential Revision: D31111697\n\nPulled By: zou3519\n\nfbshipit-source-id: 618e6b2cc927814f85500374a2838d98c9c45d6e", "pr_number": "65467", "files_changed": ["torch/testing/_internal/common_methods_invocations.py"], "labels": ["module: nn", "module: tests", "open source", "cla signed", "ciflow/default"]}, "9f782f8b35": {"title": "add `OpInfo` for `torch.nn.pixel_unshuffle` (#65468)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/65468\n\nTest Plan: Imported from OSS\n\nReviewed By: mrshenli\n\nDifferential Revision: D31111699\n\nPulled By: zou3519\n\nfbshipit-source-id: a92c2f1f4986a54abab82360e97ea2ce22fb9397", "pr_number": "65468", "files_changed": ["torch/testing/_internal/common_methods_invocations.py"], "labels": ["module: nn", "module: tests", "open source", "cla signed", "ciflow/slow-gradcheck", "ciflow/default", "ciflow/all"]}, "05b6dc9d75": {"title": "Fix BatchMatMul test and shape inference (#66733)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66733\n\nFix the test for BatchMatMul to compare glow/caffe2 outputs and fix its shape inference function since it made simplifying assumptions for broadcasting and failed on some of the shapes in the test. The previous inference was failing for any cases where the first n - 2 output dimensions of A x B was not simply that of whichever one of A or B had higher rank (ex. A: [2, 2, 2, 3, 4], B: [3, 1, 2, 2, 4, 5] we expect output dimensions [3, 2, 2, 2, 3, 5] rather than [3, 1, 2, 2, 3, 5].\n\nTest Plan:\n```\nbuck test glow/fb/test/numerics:test_operator_onnxifinnpi -- -r .*test_batch_matmul_manydims.* --env USE_INF_API=1\n```\n\nReviewed By: khabinov\n\nDifferential Revision: D31701184\n\nfbshipit-source-id: 31d0fb17409a399b90fb8042385e000ed81c3581", "pr_number": "66733", "files_changed": ["caffe2/operators/batch_matmul_op.cc"], "labels": ["fb-exported", "Merged", "cla signed", "ciflow/default"]}, "299a6a65b2": {"title": "[skip ci] Set test owners for autograd tests (#66834)", "body": "Summary:\nAction following https://github.com/pytorch/pytorch/issues/66232\n\ncc ezyang albanD zou3519 gqchen pearu nikitaved soulitzer Lezcano Varal7\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66834\n\nReviewed By: albanD\n\nDifferential Revision: D31761778\n\nPulled By: janeyx99\n\nfbshipit-source-id: 355edfb1b940154e84fbba6f7b096605e75ae459", "pr_number": "66834", "files_changed": ["test/autograd/test_complex.py", "test/test_autograd.py", "test/test_public_bindings.py"], "labels": ["module: autograd", "cla signed", "ciflow/default"]}, "c806bb1022": {"title": "[skip ci] Set test owner for test_complex.py (#66835)", "body": "Summary:\nAction following https://github.com/pytorch/pytorch/issues/66232\n\ncc ezyang anjali411 dylanbespalko mruberry Lezcano nikitaved\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66835\n\nReviewed By: anjali411\n\nDifferential Revision: D31761723\n\nPulled By: janeyx99\n\nfbshipit-source-id: ca672f5a1be9dc27284fade725a8238cbfd877a3", "pr_number": "66835", "files_changed": ["test/test_complex.py"], "labels": ["module: complex", "cla signed", "ciflow/default"]}, "fd608cd313": {"title": "[skip ci] Set test owners for optim tests (#66861)", "body": "Summary:\nAction following https://github.com/pytorch/pytorch/issues/66232\n\ncc vincentqb jbschlosser albanD\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66861\n\nReviewed By: albanD\n\nDifferential Revision: D31761369\n\nPulled By: janeyx99\n\nfbshipit-source-id: 57829e1f1509fc2af321530a4b55c9d33b7fb150", "pr_number": "66861", "files_changed": ["test/test_optim.py"], "labels": ["module: optimizer", "cla signed", "ciflow/default"]}, "17f07c310b": {"title": "Fix type checking errors in torch/ao/quantization/quantize_fx.py (#66804)", "body": "Summary:\n- [x] Fix the Pyre type checking errors in `torch/ao/quantization/quantize_fx.py`\n```\ntorch/quantization/quantize_fx.py:41:8 Incompatible variable type [9]: fuse_custom_config_dict is declared to have type `Dict[str, typing.Any]` but is used as type `None`.\ntorch/quantization/quantize_fx.py:143:16 Incompatible variable type [9]: prepare_custom_config_dict is declared to have type `Dict[str, typing.Any]` but is used as type `None`.\ntorch/quantization/quantize_fx.py:144:16 Incompatible variable type [9]: equalization_qconfig_dict is declared to have type `Dict[str, typing.Any]` but is used as type `None`.\ntorch/quantization/quantize_fx.py:206:8 Incompatible variable type [9]: prepare_custom_config_dict is declared to have type `Dict[str, typing.Any]` but is used as type `None`.\ntorch/quantization/quantize_fx.py:230:12 Incompatible variable type [9]: fuse_custom_config_dict is declared to have type `Dict[str, typing.Any]` but is used as type `None`.\ntorch/quantization/quantize_fx.py:268:8 Incompatible variable type [9]: prepare_custom_config_dict is declared to have type `Dict[str, typing.Any]` but is used as type `None`.\ntorch/quantization/quantize_fx.py:269:8 Incompatible variable type [9]: equalization_qconfig_dict is declared to have type `Dict[str, typing.Any]` but is used as type `None`.\ntorch/quantization/quantize_fx.py:427:8 Incompatible variable type [9]: prepare_custom_config_dict is declared to have type `Dict[str, typing.Any]` but is used as type `None`.\ntorch/quantization/quantize_fx.py:464:8 Incompatible variable type [9]: convert_custom_config_dict is declared to have type `Dict[str, typing.Any]` but is used as type `None`.\ntorch/quantization/quantize_fx.py:486:8 Incompatible variable type [9]: convert_custom_config_dict is declared to have type `Dict[str, typing.Any]` but is used as type `None`.\ntorch/quantization/quantize_fx.py:547:8 Incompatible variable type [9]: convert_custom_config_dict is declared to have type `Dict[str, typing.Any]` but is used as type `None`.\n```\nFixes the issue: [MLH-Fellowship/pyre-check/issues/76](https://github.com/MLH-Fellowship/pyre-check/issues/76)\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66804\n\nReviewed By: onionymous\n\nDifferential Revision: D31738171\n\nPulled By: 0xedward\n\nfbshipit-source-id: 00d4c5749c469aff39a1531365461ced747e52fc", "pr_number": "66804", "files_changed": ["torch/ao/quantization/quantize_fx.py"], "labels": ["triaged", "open source", "cla signed", "ciflow/default"]}, "94afbd158c": {"title": "[skip ci] Set test owner for test_numpy_interop.py (#66851)", "body": "Summary:\nAction following https://github.com/pytorch/pytorch/issues/66232\n\ncc mruberry rgommers\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66851\n\nReviewed By: gchanan\n\nDifferential Revision: D31761703\n\nPulled By: janeyx99\n\nfbshipit-source-id: 4dec507dff0ce25d2780b6020f0d9790ab1cb499", "pr_number": "66851", "files_changed": ["test/test_numpy_interop.py"], "labels": ["module: numpy", "Merged", "cla signed", "ciflow/default"]}, "c9d9244166": {"title": "[skip ci] Set test owner for test_spectral_ops.py (#66843)", "body": "Summary:\nAction following https://github.com/pytorch/pytorch/issues/66232\n\ncc mruberry peterbell10\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66843\n\nReviewed By: gchanan\n\nDifferential Revision: D31761715\n\nPulled By: janeyx99\n\nfbshipit-source-id: 1173a200478b87568768fafcfee117c09c1cffbd", "pr_number": "66843", "files_changed": ["test/test_spectral_ops.py"], "labels": ["module: fft", "Merged", "cla signed", "ciflow/default"]}, "50f5689d60": {"title": "Set test owner for distributions tests (#66842)", "body": "Summary:\nAction following https://github.com/pytorch/pytorch/issues/66232\n\ncc fritzo neerajprad alicanb nikitaved\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66842\n\nReviewed By: neerajprad\n\nDifferential Revision: D31761720\n\nPulled By: janeyx99\n\nfbshipit-source-id: 9d9e88d93e2efb90c971f165b4040880e9d90c56", "pr_number": "66842", "files_changed": ["test/distributions/test_constraints.py", "test/distributions/test_distributions.py", "test/distributions/test_transforms.py", "test/distributions/test_utils.py"], "labels": ["module: distributions", "Merged", "cla signed", "ciflow/default"]}, "7e81a89e13": {"title": "[PyTorch] Fix performance-no-automatic-move clang tidy warnings in matchTypeVariables (#66720)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66720\n\nSee the documentation for the warning. https://clang.llvm.org/extra/clang-tidy/checks/performance-no-automatic-move.html\nghstack-source-id: 140922952\n\nTest Plan: CI\n\nReviewed By: suo\n\nDifferential Revision: D31697506\n\nfbshipit-source-id: 26ce6c47d0f3b0c4e48ecc882f6792f1b5a45bac", "pr_number": "66720", "files_changed": ["aten/src/ATen/core/type.cpp"], "labels": ["Merged", "cla signed", "ciflow/default"]}, "552af8bdef": {"title": "[PyTorch] Fix missing move in OptionalType::createWithContained (#66697)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66697\n\nWe own this vector, so we can move from it.\nghstack-source-id: 140742640\n\nTest Plan: CI\n\nReviewed By: suo\n\nDifferential Revision: D31693230\n\nfbshipit-source-id: 3f33ca6e47e29b0e3d6c8fad59c234c55e1e159f", "pr_number": "66697", "files_changed": ["aten/src/ATen/core/jit_type.h"], "labels": ["cla signed", "ciflow/default"]}, "690c2a7076": {"title": "masked_scatter: fuse mask count check into one kernel (#66871)", "body": "Summary:\nThis saves 1 kernel launch, 7 dispatcher calls, 3 `TensorImpl` allocations and 1 CUDA memory allocation.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66871\n\nReviewed By: gchanan\n\nDifferential Revision: D31763713\n\nPulled By: ngimel\n\nfbshipit-source-id: b0d2f9415b7fd013fb4e7d68ade6e38a58f5b153", "pr_number": "66871", "files_changed": ["aten/src/ATen/native/cuda/IndexKernel.cu"], "labels": ["open source", "cla signed", "ciflow/default"]}, "3488a85a76": {"title": "Sparse CSR CUDA: fix input checks for `addmm` and `mm` (#66485)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66485\n\nThe errors for incorrectly sized inputs should match the dense variants\nof functions.\nMoved addmm_out_sparse_csr_dense_cuda from SparseCsrTensorMath.cu and\nremoved unnecessary device check.\n\ncc nikitaved pearu cpuhrsch IvanYashchuk\n\nTest Plan: Imported from OSS\n\nReviewed By: jbschlosser\n\nDifferential Revision: D31764036\n\nPulled By: cpuhrsch\n\nfbshipit-source-id: 76900fe9e4a49474695a01f34bad41cb3422321c", "pr_number": "66485", "files_changed": ["aten/src/ATen/native/sparse/cuda/SparseBlas.cpp", "aten/src/ATen/native/sparse/cuda/SparseCsrTensorMath.cu", "test/test_sparse_csr.py"], "labels": ["module: sparse", "open source", "cla signed", "ciflow/default", "ciflow/cuda"]}, "57c596eb9e": {"title": "add interactive_embedded_interpreter.cpp to the OSS build (#66352)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66352\n\nAdd cmake rules for interactive_embedded_interpreter.cpp .\n\nThe builtin_registry.cpp has already been handled in https://github.com/pytorch/pytorch/pull/66347 . I'll remove the change in this PR once that one is merged.\n\nTest Plan: Imported from OSS\n\nReviewed By: suo\n\nDifferential Revision: D31521249\n\nPulled By: shunting314\n\nfbshipit-source-id: bb9d340e5a6aad7d76078ca03a82b5ae7494a124", "pr_number": "66352", "files_changed": ["torch/csrc/deploy/CMakeLists.txt", "torch/csrc/deploy/interactive_embedded_interpreter.cpp"], "labels": ["cla signed", "ciflow/default"]}, "e70b5d64f4": {"title": "Change README getting started link to explicit instructions (#66828)", "body": "Summary:\nThis changes the link for installing binaries to the page on pytorch.org that is entirely the download command selector (which isn't visible on a normal aspect ratio screen when the main website page first loads anymore).\n\nThis also includes some other random fixes:\n* Update HUD link\n* Clean ups\n\nFixes #{issue number}\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66828\n\nReviewed By: malfet\n\nDifferential Revision: D31750654\n\nPulled By: driazati\n\nfbshipit-source-id: aef9ceba71418f6f7648eab9a8c8a78d6c60518b", "pr_number": "66828", "files_changed": ["README.md"], "labels": ["cla signed", "ciflow/default"]}, "bd4d5cb14c": {"title": "Sparse CSR: Add torch.empty (#63509)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/63509\n\nThe primary use of `torch.empty` is to reserve memory for tensor and set the type, device, size information. The same is done here for SparseCSR.\n`crow_indices` is initialized as an empty tensor of size `num_rows + 1`. `col_indices` and `values` are initialized as empty tensors of size 0.\n\ncc nikitaved pearu cpuhrsch IvanYashchuk\n\nTest Plan: Imported from OSS\n\nReviewed By: anjali411\n\nDifferential Revision: D31770359\n\nPulled By: cpuhrsch\n\nfbshipit-source-id: c83f2a2e0d7514ba24780add1086e1bccf541dd9", "pr_number": "63509", "files_changed": ["aten/src/ATen/native/native_functions.yaml", "aten/src/ATen/native/sparse/SparseCsrTensor.cpp", "test/test_sparse_csr.py"], "labels": ["module: sparse", "open source", "Merged", "cla signed", "ciflow/default"]}, "b3bb234e16": {"title": "Remove THCGeneral.cpp (#66766)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/66766\n\nTest Plan: Imported from OSS\n\nReviewed By: zou3519\n\nDifferential Revision: D31721647\n\nPulled By: ngimel\n\nfbshipit-source-id: 5033a2800871c8745a1a92e379c9f97c98af212e", "pr_number": "66766", "files_changed": ["BUILD.bazel", "aten/src/ATen/cuda/detail/CUDAHooks.cpp", "aten/src/ATen/native/cuda/Dropout.cu", "aten/src/ATen/native/cuda/TensorFactories.cu", "aten/src/ATen/native/cuda/Unique.cu", "aten/src/ATen/native/cudnn/Conv_v7.cpp", "aten/src/THC/CMakeLists.txt", "aten/src/THC/THCGeneral.cpp", "aten/src/THC/THCGeneral.h", "aten/src/THC/THCStorage.cpp", "torch/csrc/cuda/Stream.cpp", "torch/csrc/generic/StorageSharing.cpp", "torch/csrc/generic/serialization.cpp"], "labels": ["open source", "Merged", "cla signed", "ciflow/default"]}, "c69e33bb11": {"title": "Fix doc string for torch.acosh (#66814)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66814\nShift equation above note as per issue 65905 on github\n\nTest Plan:\nImported from OSS\n\nIn preview docs built from PR\n\nhttps://docs-preview.pytorch.org/66814/generated/torch.acosh.html#torch.acosh equation is now above note\n\n{F671441651}\n\nReviewed By: gchanan\n\nDifferential Revision: D31742677\n\nPulled By: mikaylagawarecki\n\nfbshipit-source-id: 9fa5390ad2a01ca001418c0bd624f2145f861bf4", "pr_number": "66814", "files_changed": ["torch/_torch_docs.py"], "labels": ["Merged", "cla signed", "ciflow/default"]}, "867ccc9987": {"title": "Strided masked reduction: amin. (#66385)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66385\n\ncc nikitaved pearu cpuhrsch IvanYashchuk\n\nTest Plan: Imported from OSS\n\nReviewed By: jbschlosser\n\nDifferential Revision: D31779530\n\nPulled By: cpuhrsch\n\nfbshipit-source-id: de753c2d191f7980a48831b892d3a1e8a7a547cd", "pr_number": "66385", "files_changed": ["torch/_masked/__init__.py", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["module: sparse", "open source", "module: reductions", "cla signed", "ciflow/default"]}, "f95fef7897": {"title": "Add prim::TensorExprDynamicGuard to bc allowlist (#66939)", "body": "Summary:\nFixes #{issue number}\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66939\n\nReviewed By: ejguan\n\nDifferential Revision: D31797160\n\nPulled By: soulitzer\n\nfbshipit-source-id: 630b7a0ab99671192397f927391361622f7e9c2e", "pr_number": "66939", "files_changed": ["test/backward_compatibility/check_backward_compatibility.py"], "labels": ["Merged", "cla signed", "ciflow/default"]}, "8a65047acc": {"title": "[skip ci] Set test owners for everything considered with module: tests (#66865)", "body": "Summary:\nAction following https://github.com/pytorch/pytorch/issues/66232\n\ncc mruberry\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66865\n\nReviewed By: anjali411\n\nDifferential Revision: D31771147\n\nPulled By: janeyx99\n\nfbshipit-source-id: 8bebe5ac2098364ef1ee93b590abb5f4455b0f89", "pr_number": "66865", "files_changed": ["test/test_binary_ufuncs.py", "test/test_indexing.py", "test/test_reductions.py", "test/test_shape_ops.py", "test/test_sort_and_select.py", "test/test_testing.py", "test/test_torch.py", "test/test_unary_ufuncs.py", "test/test_view_ops.py"], "labels": ["module: tests", "cla signed", "ciflow/default"]}, "452b359c3f": {"title": "[skip ci] Set test owners for tensor creation tests (#66864)", "body": "Summary:\nAction following https://github.com/pytorch/pytorch/issues/66232\n\ncc gchanan mruberry\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66864\n\nReviewed By: anjali411\n\nDifferential Revision: D31771139\n\nPulled By: janeyx99\n\nfbshipit-source-id: 74adeae7de355fa6c63de22290fa324911230368", "pr_number": "66864", "files_changed": ["test/test_tensor_creation_ops.py"], "labels": ["module: tensor creation", "cla signed", "ciflow/default"]}, "409364e597": {"title": "[skip ci] Set test owners for test_typing.py (#66869)", "body": "Summary:\nAction following https://github.com/pytorch/pytorch/issues/66232\n\ncc ezyang malfet rgommers xuzhao9 gramster\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66869\n\nReviewed By: anjali411\n\nDifferential Revision: D31766850\n\nPulled By: janeyx99\n\nfbshipit-source-id: e9772f5378be07162d4f4d06925165e396d7d6c6", "pr_number": "66869", "files_changed": ["test/test_type_hints.py", "test/test_type_info.py", "test/test_typing.py"], "labels": ["module: typing", "cla signed", "ciflow/default"]}, "822277f302": {"title": "[skip ci] Set test owners for test_type_promotion.py (#66866)", "body": "Summary:\nAction following https://github.com/pytorch/pytorch/issues/66232\n\ncc nairbv mruberry\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66866\n\nReviewed By: anjali411\n\nDifferential Revision: D31771149\n\nPulled By: janeyx99\n\nfbshipit-source-id: 87c04ed4a75ada06a553a11064d44ac65fc4c6ea", "pr_number": "66866", "files_changed": ["test/test_type_promotion.py"], "labels": ["module: type promotion", "cla signed", "ciflow/default"]}, "a015964cf8": {"title": "Strided masked reduction: prod. (#66386)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66386\n\ncc nikitaved pearu cpuhrsch\n\nTest Plan: Imported from OSS\n\nReviewed By: anjali411\n\nDifferential Revision: D31779598\n\nPulled By: cpuhrsch\n\nfbshipit-source-id: 304a3d6abc794a49de5b044aade6cfd727758495", "pr_number": "66386", "files_changed": ["torch/_masked/__init__.py", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["module: sparse", "open source", "module: reductions", "cla signed", "ciflow/default"]}, "793f366e34": {"title": "[skip ci] Set test owners for sparse tests (#66863)", "body": "Summary:\nAction following https://github.com/pytorch/pytorch/issues/66232\n\ncc nikitaved pearu cpuhrsch IvanYashchuk\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66863\n\nReviewed By: anjali411\n\nDifferential Revision: D31771126\n\nPulled By: janeyx99\n\nfbshipit-source-id: 6cb5ca0557e8555f6a09b3e607ff8888e505486e", "pr_number": "66863", "files_changed": ["test/test_sparse.py", "test/test_sparse_csr.py"], "labels": ["module: sparse", "cla signed", "ciflow/default"]}, "5569d5824c": {"title": "Fix documentation of arguments for torch.nn.functional.Linear (#66884)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66884\n\nAddressing docs fix mentioned in issue 64978 on Github\nghstack-source-id: 141093449\n\nTest Plan: https://pxl.cl/1Rxkz\n\nReviewed By: anjali411\n\nDifferential Revision: D31767303\n\nfbshipit-source-id: f1ca10fed5bb768749bce3ddc240bbce1dfb3f84", "pr_number": "66884", "files_changed": ["torch/nn/functional.py"], "labels": ["cla signed", "ciflow/default"]}, "6e67150f57": {"title": "[skip ci] Set test owner for test_mkldnn.py (#66845)", "body": "Summary:\nAction following https://github.com/pytorch/pytorch/issues/66232\n\ncc gujinghui PenghuiCheng XiaobingSuper jianyuh VitalyFedyunin\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66845\n\nReviewed By: anjali411\n\nDifferential Revision: D31803377\n\nPulled By: janeyx99\n\nfbshipit-source-id: 4fcf77d3e4bf976449a0b1ab4d750619db3493a1", "pr_number": "66845", "files_changed": ["test/test_mkldnn.py"], "labels": ["module: mkldnn", "Merged", "cla signed", "ciflow/default"]}, "450221c534": {"title": "Sparse CSR: Add tensor.resize_ and tensor.copy_ (#63510)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/63510\n\nSparse CSR matrix resizing behavior:\nIf we _increase the number of rows_ the number of specified elements in the matrix remains the same -> the size of col_indices, values doesn't change, the size of crow_indices becomes `rows+1`.\nIf we _decrease the number of rows_ the number of specified elements will be `min(nnz, rows*cols)` -> need to resize `crow_indices` to `rows+1` and set the last element to `min(nnz, rows*cols)`; decrease the size of col_indices and values to `min(nnz, rows*cols)`.\nIf we _increase the number of columns_ the number of specified elements in the matrix remains the same, the number of rows remains the same -> no need to resize anything, just set new sizes.\nWe _cannot decrease the number of columns_ because it would require recomputing `crow_indices`.\n\ncc nikitaved pearu cpuhrsch IvanYashchuk\n\nTest Plan: Imported from OSS\n\nReviewed By: anjali411\n\nDifferential Revision: D31796680\n\nPulled By: cpuhrsch\n\nfbshipit-source-id: 7d8a9701ce06d30a1841f94bba0a057cacea9401", "pr_number": "63510", "files_changed": ["aten/src/ATen/SparseCsrTensorImpl.cpp", "aten/src/ATen/SparseCsrTensorImpl.h", "aten/src/ATen/native/native_functions.yaml", "aten/src/ATen/native/sparse/SparseCsrTensor.cpp", "test/test_sparse_csr.py"], "labels": ["module: sparse", "open source", "cla signed", "ciflow/default"]}, "257239972c": {"title": "Fix attr_to_scope's key in `torch/utils/tensorboard/_pytorch_graph.py` (#65692)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/65652\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65692\n\nReviewed By: Reubend\n\nDifferential Revision: D31678606\n\nPulled By: edward-io\n\nfbshipit-source-id: 7c0bf740ee4f8c21bd01ced3ae70df23c9efadfb", "pr_number": "65692", "files_changed": ["test/expect/TestTensorBoard.test_nested_nn_squential.expect", "test/test_tensorboard.py", "torch/utils/tensorboard/_pytorch_graph.py"], "labels": ["triaged", "open source", "Merged", "cla signed", "ciflow/default"]}, "18bbc4c2b7": {"title": "[Static Runtime] Fix a bug in aten::index (#66940)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66940\n\n`aten::index`'s schema is as follows:\n\n```\n\"aten::index.Tensor(Tensor self, Tensor?[] indices) -> Tensor\n```\n\nThe current implementation assumes `indices`' elements are all tensors by doing `elem.toTensor`, which is incorrectly. This change creates an empty optional value if an element from `indices` is not a tensor.\n\nTest Plan: Fixed `StaticRuntime, IndividualOps_Index` to correctly test `aten::index` with `indices` that contains `None`.\n\nReviewed By: hlu1\n\nDifferential Revision: D31712145\n\nfbshipit-source-id: be1c29674bcd55b67b0dcc2a988bc37fd43745f3", "pr_number": "66940", "files_changed": ["aten/src/ATen/native/IndexingUtils.h", "benchmarks/static_runtime/test_scripts.h", "benchmarks/static_runtime/test_static_runtime.cc"], "labels": ["fb-exported", "cla signed", "ciflow/default"]}, "e046386be8": {"title": "Avoid inlining error reporting in checked_convert (#66721)", "body": "Summary:\n**Summary:** Move the error reporting part to the cpp file to avoid callers inlining it, which inflates the generated code size. See https://github.com/pytorch/pytorch/issues/65830.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66721\n\nTest Plan:\nCompiling the simple program below now generates ~150 lines of assembly, compared to 700+ lines before.\n\n```\n#include <c10/core/Scalar.h>\n\nvoid g(float) {}\n\nvoid f(const c10::Scalar& scalar) {\n    auto x = scalar.to<float>();\n    g(x);\n}\n```\n\n**Reviewers:** Brian Hirsh\n\n**Subscribers:** Brian Hirsh, Edward Yang, Yining Lu\n\n**Tasks:** T103384490\n\n**Tags:** pytorch\n\nFixes https://github.com/pytorch/pytorch/issues/65830\n\nReviewed By: zou3519, bdhirsh\n\nDifferential Revision: D31737607\n\nPulled By: andrewor14\n\nfbshipit-source-id: 3d493c4d8e51d8f8a19d00f59b8ea28176c8a9e3", "pr_number": "66721", "files_changed": ["c10/util/TypeCast.cpp", "c10/util/TypeCast.h"], "labels": ["cla signed", "ciflow/default"]}, "db4165892b": {"title": "[SmartCompose][OnDevice]fix function name bug in mobile export & Script to convert mobile model (#66915)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66915\n\nPull Request resolved: https://github.com/pytorch/pytorch-canary/pull/3\n\nfix function name bug in mobile export\n\nTest Plan: buck run pytext/fb/assistant/smart_compose:mobile_converter -- --model_input=pytext_training/tree/teams/assistant/smart_compose/300555761/model.ts --model_output=pytext_training/tree/teams/assistant/smart_compose/300555761/mobile_model_test.ts\n\nReviewed By: JacobSzwejbka\n\nDifferential Revision: D31782983\n\nfbshipit-source-id: 7288bb65adc7346d218980a535d68a12d8ef2033", "pr_number": "66915", "files_changed": ["torch/utils/bundled_inputs.py"], "labels": ["fb-exported", "cla signed", "ciflow/default"]}, "8beabffac3": {"title": "[PyTorchEdge] Make aten function common to aten and torch_common (#66663)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66663\n\nfb: TensorCompare.cpp is in per-app, a target higher than torch_mobile\n\nPlease read this doc to know about [Per-app ATen/native and Template Selective Build](\nhttps://docs.google.com/document/d/1O5--mOAi_gGh2GkE-REo3qJRRQ_Lks69IfgszcB8ThI/edit)\n\nCreate a filed called \"prim_native_functions.cpp\" in ATen, add it to aten_cpu, and cut-paste native::is_nonzero() to prim_native_functions.cpp.\nBy doing this we move the function to lower layer which is more visible to all targets depending on it.\n\nInstruction count comparison new vs old\nhttps://www.internalfb.com/phabricator/paste/view/P463272302?view=diff\n\nTest Plan:\nfb:\n```\n(base) [pavithran@devvm1803.vll0 /data/users/pavithran/fbsource] buck build //xplat/caffe2:aten_cpu\nBuilding: finished in 0.4 sec (100%) 1/202 jobs, 0/202 updated\n  Total time: 0.4 sec\nMore details at https://www.internalfb.com/intern/buck/build/ea35300b-55be-4b9f-bc74-80cdd869c16a\nBUILD SUCCEEDED\n(base) [pavithran@devvm1803.vll0 /data/users/pavithran/fbsource] buck build //xplat/caffe2:aten_native_cpu\nBuilding: finished in 0.7 sec (100%) 1/1 jobs, 0/1 updated\n  Total time: 0.8 sec\nMore details at https://www.internalfb.com/intern/buck/build/ccd97d43-c59d-4f29-9418-485cd24575e2\nBUILD SUCCEEDED\n```\n\nReviewed By: iseeyuan\n\nDifferential Revision: D31669536\n\nfbshipit-source-id: d35f069f975db6dce0b678c5b5ddd74bd690f599", "pr_number": "66663", "files_changed": ["aten/src/ATen/native/TensorCompare.cpp", "aten/src/ATen/native/prim_native_functions.cpp", "tools/build_variables.bzl"], "labels": ["fb-exported", "cla signed", "ciflow/default"]}, "32e790997b": {"title": "[Rocm]Reduce severity of detected possible memory leak from assertion to warning (#65973)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/62533.\nIn very rare cases, the decorator for detecting memory leak is throwing assertion, even when the test is passing, and the memory is being freed with a tiny delay. The issue is not being reproduced in internal testing, but shows up sometimes in CI environment.\n\nReducing the severity of such detection to warning, so as not to fail the CI tests, as the actual test is not failing, rather only the check inside the decorator is failing.\n\nLimiting the change to ROCM only for now.\n\ncc jeffdaily sunway513 jithunnair-amd ROCmSupport\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65973\n\nReviewed By: anjali411\n\nDifferential Revision: D31776154\n\nPulled By: malfet\n\nfbshipit-source-id: 432199fca17669648463c4177c62adb553cacefd", "pr_number": "65973", "files_changed": ["test/test_cuda.py", "torch/testing/_internal/common_utils.py"], "labels": ["module: rocm", "triaged", "open source", "Merged", "cla signed"]}, "f5c5ab2868": {"title": "[skip ci] Set test owner for cpp-extensions tests (#66837)", "body": "Summary:\nAction following https://github.com/pytorch/pytorch/issues/66232\n\ncc yf225 glaringlee zou3519\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66837\n\nReviewed By: anjali411\n\nDifferential Revision: D31828401\n\nPulled By: janeyx99\n\nfbshipit-source-id: 35ac27f3e1c0eb70ccb38c07c42ba61bd0c848fe", "pr_number": "66837", "files_changed": ["test/test_cpp_extensions_aot.py", "test/test_cpp_extensions_jit.py"], "labels": ["module: cpp-extensions", "cla signed", "ciflow/default"]}, "960e3216a4": {"title": "[skip ci] Set test owner for named tensor tests (#66849)", "body": "Summary:\nAction following https://github.com/pytorch/pytorch/issues/66232\n\ncc zou3519\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66849\n\nReviewed By: zou3519\n\nDifferential Revision: D31828903\n\nPulled By: janeyx99\n\nfbshipit-source-id: 30810bcec750ba8e1d5a342c31a5996bf57acd69", "pr_number": "66849", "files_changed": ["test/test_namedtensor.py"], "labels": ["module: named tensor", "cla signed", "ciflow/default"]}, "13b8599831": {"title": "[skip ci] Set test owner for test_dispatch.py (#66840)", "body": "Summary:\nAction following https://github.com/pytorch/pytorch/issues/66232\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66840\n\nReviewed By: saketh-are\n\nDifferential Revision: D31829224\n\nPulled By: janeyx99\n\nfbshipit-source-id: 66aceacd4f976c36ed48ca5be59616d245ba2a82", "pr_number": "66840", "files_changed": ["test/test_dispatch.py", "test/test_gen_backend_stubs.py"], "labels": ["module: dispatch", "cla signed", "ciflow/default"]}, "23321ba7a3": {"title": "Fix bug [#66780]: wrong input to torch.is_floating_point (#66783)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/66783\n\nTest Plan: Imported from OSS\n\nReviewed By: jbschlosser\n\nDifferential Revision: D31802971\n\nPulled By: cpuhrsch\n\nfbshipit-source-id: 6a7d8b83dad219fd683504f9084b77358800507c", "pr_number": "66783", "files_changed": ["test/test_reductions.py"], "labels": ["open source", "cla signed", "ciflow/default"]}, "40e5d31a52": {"title": "Add OpInfo for torch.bincount (#65796)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/65796\n\nReviewed By: bdhirsh\n\nDifferential Revision: D31386560\n\nPulled By: saketh-are\n\nfbshipit-source-id: acb6ed3f743ddcccd0ff7ce1ab21f77c2078da37", "pr_number": "65796", "files_changed": ["torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "cla signed", "ciflow/default"]}, "94f4e9a995": {"title": "Enable warning tests for nondeterministic backward functions (#66736)", "body": "Summary:\nFollowup from https://github.com/pytorch/pytorch/issues/66233\n\nSince https://github.com/pytorch/pytorch/issues/50209 was fixed, we can enable these warning tests now\n\ncc mruberry kurtamohler\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66736\n\nReviewed By: zou3519\n\nDifferential Revision: D31723385\n\nPulled By: mruberry\n\nfbshipit-source-id: dc1922a6d0c45cc80020db85710e755a89113861", "pr_number": "66736", "files_changed": ["test/test_torch.py", "torch/testing/_internal/common_device_type.py"], "labels": ["module: determinism", "open source", "Merged", "cla signed", "ciflow/default"]}, "78f970568c": {"title": "Add dummy op to use instead of searchsorted (#66964)", "body": "Summary:\nWould help unblock https://github.com/pytorch/pytorch/issues/66818 if this actually works\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66964\n\nReviewed By: mruberry\n\nDifferential Revision: D31817942\n\nPulled By: janeyx99\n\nfbshipit-source-id: 9e9a2bcb0c0479ec7000ab8760a2e64bf0e85e95", "pr_number": "66964", "files_changed": ["aten/src/ATen/native/cuda/Bucketization.cu", "aten/src/ATen/native/native_functions.yaml", "caffe2/CMakeLists.txt", "torch/utils/cpp_extension.py"], "labels": ["Merged", "cla signed", "ciflow/default", "ciflow/all"]}, "00a871c5c9": {"title": "[skip ci] Set test owner for multiprocessing tests (#66848)", "body": "Summary:\nAction following https://github.com/pytorch/pytorch/issues/66232\n\ncc VitalyFedyunin\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66848\n\nReviewed By: VitalyFedyunin\n\nDifferential Revision: D31828908\n\nPulled By: janeyx99\n\nfbshipit-source-id: 45d6901648f5564c1bf07ad8d01d69ef486ae104", "pr_number": "66848", "files_changed": ["test/test_multiprocessing.py", "test/test_multiprocessing_spawn.py"], "labels": ["module: multiprocessing", "Merged", "cla signed", "ciflow/default"]}, "6f1ba16d6d": {"title": "[skip ci] Set test owners for cpp test (#66836)", "body": "Summary:\nAction following https://github.com/pytorch/pytorch/issues/66232\n\ncc yf225 glaringlee\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66836\n\nReviewed By: saketh-are\n\nDifferential Revision: D31828641\n\nPulled By: janeyx99\n\nfbshipit-source-id: 076d41686746fecebc07452df8212eef15a7824c", "pr_number": "66836", "files_changed": ["test/test_cpp_api_parity.py"], "labels": ["module: cpp", "Merged", "cla signed", "ciflow/default"]}, "b07371f19c": {"title": "[skip ci] Set test owners for serialization tests (#66862)", "body": "Summary:\nAction following https://github.com/pytorch/pytorch/issues/66232\n\ncc mruberry\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66862\n\nReviewed By: saketh-are\n\nDifferential Revision: D31828615\n\nPulled By: janeyx99\n\nfbshipit-source-id: 8d28970eead9d6f26e9ea64b823295d9c9e1469d", "pr_number": "66862", "files_changed": ["test/test_serialization.py"], "labels": ["module: serialization", "Merged", "cla signed", "ciflow/default"]}, "062ae8df0e": {"title": "Automated submodule update: tensorpipe (#65353)", "body": "Summary:\nThis is an automated pull request to update the first-party submodule for [pytorch/tensorpipe](https://github.com/pytorch/tensorpipe).\n\nNew submodule commit: https://github.com/pytorch/tensorpipe/commit/183172ba8c323a1a325c61d67445874141c88c12\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65353\n\nTest Plan: Ensure that CI jobs succeed on GitHub before landing.\n\nReviewed By: lw\n\nDifferential Revision: D31059779\n\nfbshipit-source-id: 7bddff5139f8168750e22e1cc8c0d49931db542e", "pr_number": "65353", "files_changed": ["third_party/tensorpipe"], "labels": ["triaged", "open source", "Merged", "cla signed", "ciflow/default"]}, "892ac08a02": {"title": "Do not generate not_implemented error for forward AD when input with tangent passed to non-differentiable function (#66926)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/61926\n\n1. update the `if` to just use requires_derivative since that should reflect when function is not differentiable\n2. if `requires_derivative=True` but no outputs have forward derivatives, we should error as usual\n3. ~In the future we may also want to handle the case~ when `len(fw_derivatives) > 0 and len(fw_derivatives) < num_diff_outputs` we should add assert in codegen that this does not happen.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66926\n\nReviewed By: anjali411\n\nDifferential Revision: D31810736\n\nPulled By: soulitzer\n\nfbshipit-source-id: 11a14477cc7554f576cff2ed1711a448a8c6a66a", "pr_number": "66926", "files_changed": ["test/test_autograd.py", "tools/autograd/gen_variable_type.py"], "labels": ["Merged", "cla signed", "ciflow/default"]}, "462f333c01": {"title": "[numpy] add torch.argwhere (#64257)", "body": "Summary:\nAdds `torch.argwhere` as an alias to `torch.nonzero`\n\nCurrently, `torch.nonzero` is actually provides equivalent functionality to `np.argwhere`.\n\nFrom NumPy docs,\n> np.argwhere(a) is almost the same as np.transpose(np.nonzero(a)), but produces a result of the correct shape for a 0D array.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/64257\n\nReviewed By: dagitses\n\nDifferential Revision: D31474901\n\nPulled By: saketh-are\n\nfbshipit-source-id: 335327a4986fa327da74e1fb8624cc1e56959c70", "pr_number": "64257", "files_changed": ["aten/src/ATen/core/aten_interned_strings.h", "aten/src/ATen/core/interned_strings.h", "aten/src/ATen/native/TensorAdvancedIndexing.cpp", "aten/src/ATen/native/native_functions.yaml", "docs/source/tensors.rst", "docs/source/torch.rst", "torch/_tensor_docs.py", "torch/_torch_docs.py", "torch/overrides.py", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["oncall: jit", "open source", "Merged", "cla signed", "ciflow/default"]}, "fcfa06586d": {"title": "Wextra fix for NamedTensor.cpp (#66897)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66897\n\nFixes:\n```\nstderr: caffe2/aten/src/ATen/native/NamedTensor.cpp:226:19: error: comparison of integers of different signs: 'const unsigned long' and 'int64_t' (aka 'long') [-Werror,-Wsign-compare]\n    if (order_idx >= ellipsis_idx) {\n        ~~~~~~~~~ ^  ~~~~~~~~~~~~\nstderr: caffe2/aten/src/ATen/native/NamedTensor.cpp:226:19: error: comparison of integers of different signs: 'const unsigned long' and 'int64_t' (aka 'long') [-Werror,-Wsign-compare]\n    if (order_idx >= ellipsis_idx) {\n        ~~~~~~~~~ ^  ~~~~~~~~~~~~\n```\n\nTest Plan: Sandcastle\n\nReviewed By: malfet\n\nDifferential Revision: D31774623\n\nfbshipit-source-id: b6e5b76695e512084ac5c9cb4215de7e9b763cf8", "pr_number": "66897", "files_changed": ["aten/src/ATen/native/NamedTensor.cpp"], "labels": ["fb-exported", "cla signed", "ciflow/default"]}, "f29e5220a6": {"title": "Revert D31474901: [pytorch][PR] [numpy] add torch.argwhere", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD31474901\n\nOriginal commit changeset: 335327a4986f\n\nfbshipit-source-id: 534093e459762ff7a888c58d76e49e362015f2ba", "pr_number": null, "files_changed": ["aten/src/ATen/core/aten_interned_strings.h", "aten/src/ATen/core/interned_strings.h", "aten/src/ATen/native/TensorAdvancedIndexing.cpp", "aten/src/ATen/native/native_functions.yaml", "docs/source/tensors.rst", "docs/source/torch.rst", "torch/_tensor_docs.py", "torch/_torch_docs.py", "torch/overrides.py", "torch/testing/_internal/common_methods_invocations.py"], "labels": []}, "b696d64ef4": {"title": "Binaries without AVX512 kernels shouldn't report CPU Capability as AVX512 on machines with AVX512 support (#66703)", "body": "Summary:\n### BUG\nIf a PyTorch binary is built with a compiler that doesn't support all the AVX512 intrinsics in the codebase, then it won't have ATen AVX512 kernels, but at runtime, CPU capability would still be incorrectly returned as AVX512 on a machine that supports AVX512. It seems that PyTorch Linux releases are done on CentOS with `gcc 7.3`, so this bug would manifest in the 1.10 release, unless a fix such as this one is added. gcc versions below 9.0 don't support all the AVX512 intrinsics in the codebase, such as `_mm512_set_epi16`.\n\n### FIX\nCPU Capability would be returned as AVX512 at runtime only if the binary was built with a compiler that supports all the AVX512 intrinsics in the codebase, and if the hardware the binary is being run on supports all the required AVX512 instruction sets.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66703\n\nReviewed By: gchanan\n\nDifferential Revision: D31732625\n\nPulled By: malfet\n\nfbshipit-source-id: e52d06b87fbe2af9b303a2e9c264189c8512d5ec", "pr_number": "66703", "files_changed": ["aten/src/ATen/native/DispatchStub.cpp"], "labels": ["triaged", "open source", "Merged", "cla signed", "ciflow/default"]}, "2578de4851": {"title": "[skip ci] Set test owner for test_cuda* tests (#66838)", "body": "Summary:\nAction following https://github.com/pytorch/pytorch/issues/66232\n\ncc ngimel\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66838\n\nReviewed By: saketh-are\n\nDifferential Revision: D31841411\n\nPulled By: janeyx99\n\nfbshipit-source-id: 5cdffdef4a92f9adcef1143ae4598b052c5acc6b", "pr_number": "66838", "files_changed": ["test/test_cuda.py", "test/test_cuda_primary_ctx.py"], "labels": ["module: cuda", "Merged", "cla signed", "ciflow/default"]}, "20f08d23a0": {"title": "Revert D31838513: Strided masked reduction: mean.", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD31838513\n\nOriginal commit changeset: 54b99ccf9821\n\nfbshipit-source-id: 5480e8482c8770b41579ee085e158572b659c1f5", "pr_number": null, "files_changed": ["torch/_masked/__init__.py", "torch/testing/_internal/common_methods_invocations.py"], "labels": []}, "35965869cf": {"title": "Enroll bowangbj@ to PyTorch distributed package (#67062)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/67062\n\nFor cc and potential reviews\n\nTest Plan: Imported from OSS\n\nReviewed By: mrshenli\n\nDifferential Revision: D31849050\n\nfbshipit-source-id: d3899c2ca857b8f22bdc88b4e83cdd20bbf0b1d6", "pr_number": "67062", "files_changed": ["CODEOWNERS"], "labels": ["cla signed", "ciflow/default"]}, "28fac23409": {"title": "Fixes CUDA vs CPU consistency for index_put_ when accumulating (#66790)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/39227\nFixes https://github.com/pytorch/pytorch/issues/66495 (duplicate of 39227)\n\nDescription:\n- Expands values for CUDA implementation\n- Improved shapes checking for CUDA\n- Improved error message for CUDA\n- Added tests\n\ncc zou3519\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66790\n\nReviewed By: mruberry\n\nDifferential Revision: D31843566\n\nPulled By: ngimel\n\nfbshipit-source-id: c9e5d12a33e1067619c210174ba6e3cd66d5718b", "pr_number": "66790", "files_changed": ["aten/src/ATen/native/cuda/Indexing.cu", "test/test_indexing.py"], "labels": ["triaged", "open source", "cla signed", "ciflow/default"]}, "4fe8055b9f": {"title": "made functorch not decompose by default (#66945)", "body": "Summary:\nBasically reverting this: https://github.com/pytorch/pytorch/pull/63616\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66945\n\nReviewed By: zou3519\n\nDifferential Revision: D31802176\n\nPulled By: Chillee\n\nfbshipit-source-id: b1cabd7af66aef26411801516c87336eaea4fccb", "pr_number": "66945", "files_changed": ["c10/core/DispatchKeySet.cpp"], "labels": ["cla signed", "ciflow/default"]}, "01ced45217": {"title": "[iOS] Bump up iOS CocoaPods version to 1.10.0 (#67058)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/67058\n\nTest Plan: Imported from OSS\n\nReviewed By: xta0\n\nDifferential Revision: D31846445\n\nPulled By: hanton\n\nfbshipit-source-id: 7510a6c15fdeecc996fcce5c48db32e148ba7def", "pr_number": "67058", "files_changed": ["ios/LibTorch-Lite.podspec", "ios/LibTorch.podspec"], "labels": ["cla signed", "ciflow/default"]}, "d1a5612a3e": {"title": "remove accscalar from i0 and i0e (#67048)", "body": "Summary:\nRemoves some of the half math ops to make https://github.com/pytorch/pytorch/issues/64023 possible.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/67048\n\nReviewed By: mruberry\n\nDifferential Revision: D31847249\n\nPulled By: ngimel\n\nfbshipit-source-id: 8385aacd846bb990e368ff336eb346d847af70b9", "pr_number": "67048", "files_changed": ["aten/src/ATen/native/cuda/Math.cuh", "aten/src/ATen/native/cuda/UnarySpecialOpsKernel.cu"], "labels": ["cla signed", "ciflow/default"]}, "e8742f15cf": {"title": "[quant][graphmode][fx] Add observation_type.py (#67063)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/67063\n\nAdding ObservationType Enum for `backend_config_dict`\n\nTest Plan: Imported from OSS\n\nReviewed By: vkuzo\n\nDifferential Revision: D31849078\n\nfbshipit-source-id: e9e7225d564b51fa9454f7f087dd134152c069a0", "pr_number": "67063", "files_changed": ["torch/ao/quantization/fx/backend_config_dict/observation_type.py"], "labels": ["cla signed", "ciflow/default", "fx"]}, "7e5aa0d35a": {"title": "fixed unique arguments documentation (#66132)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66132\n\nDifferential Revisi\n<img width=\"875\" alt=\"Screen Shot 2021-10-05 at 12 10 39 PM\" src=\"https://user-images.githubusercontent.com/17888388/136276286-3df20681-7b7a-4a91-97d6-4f1ac3722121.png\">\non: [D31397746](https://our.intern.facebook.com/intern/diff/D31397746/)\n\nTest Plan: Imported from OSS\n\nReviewed By: gchanan\n\nDifferential Revision: D31734476\n\nPulled By: samdow\n\nfbshipit-source-id: 8999443c7f9b24394d7543652b8350261c1f8b3a", "pr_number": "66132", "files_changed": ["torch/functional.py"], "labels": ["cla signed", "ciflow/default"]}, "7b0408684b": {"title": "Fix linter (#67122)", "body": "Summary:\nFixes regression introduced by https://github.com/pytorch/pytorch/commit/7e5aa0d35a540807d524966e118d1ead4d3b2eae\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/67122\n\nReviewed By: seemethere\n\nDifferential Revision: D31872569\n\nPulled By: malfet\n\nfbshipit-source-id: ada0137db9a46cbec573489c9c37a94f3a7576ae", "pr_number": "67122", "files_changed": ["torch/functional.py"], "labels": ["cla signed", "ciflow/default"]}, "af1a2df825": {"title": "enable better depthwise conv perf on cudnn 8.2+ (#58749)", "body": "Summary:\nThere are multiple improvement of depthwise convolution speed in cudnn between 7.6 and 8.2, since https://github.com/pytorch/pytorch/pull/22302.\nThis PR aim to harvest all the new improvement by enable more cudnn kernel. The workload checking logic can also be simplified now.\nTo keep the change simple, I kept things before cudnn 8.2 unchanged.\n\nSimilar to https://github.com/pytorch/pytorch/pull/22302, I used a script [here](https://gist.github.com/FDecaYed/e8ba98a95cd33697df2ace86fdb44897) to benchmark. Both run are using cudnn 8.2\nOne enhancement I did to the script is switch to event based timing. With warmup kernels to fill the launch queue ahead, this should give us accurate kernel timing even in CPU launch bound cases.\n\nHere is A100 and V100 result sorted by speedup.\n[Book1.xlsx](https://github.com/pytorch/pytorch/files/6530371/Book1.xlsx)\n\nResult highlights:\nNewly turned on 5x5 cudnn kernel show up to 6x speedup.\nClose to half of test sizes show >10% speedup.\nFixed some corner cases that previously caused 15-20x slowdown.\nOnly slowdown a handful of cases(~10 out of >1000)\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/58749\n\nReviewed By: bdhirsh\n\nDifferential Revision: D31613199\n\nPulled By: ngimel\n\nfbshipit-source-id: 883b58facad67ccd51dc9ab539368b4738d40398", "pr_number": "58749", "files_changed": ["aten/src/ATen/native/Convolution.cpp"], "labels": ["triaged", "open source", "cla signed", "ciflow/default"]}, "83f70db95c": {"title": "Fix common device computation for comparison ops. (#66245)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66245\n\nFixes #66053\n\nThis PR splits `declare_static_dtype_and_device` into two new methods for\n`TensorIteratorBase`: `declare_static_dtype` and `declare_static_device`.\n\nTest Plan: Imported from OSS\n\nReviewed By: ejguan\n\nDifferential Revision: D31503849\n\nPulled By: ngimel\n\nfbshipit-source-id: 4b131b691d29ceb5f3709f5d6503997ea0875c54", "pr_number": "66245", "files_changed": ["aten/src/ATen/TensorIterator.cpp", "aten/src/ATen/TensorIterator.h", "aten/src/ATen/native/BinaryOps.cpp", "test/test_binary_ufuncs.py"], "labels": ["open source", "cla signed", "module: structured kernels", "ciflow/default"]}, "64c68edaf3": {"title": "[pt] Add Half precision support for bucketize and searchsorted op (#67077)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/67077\n\nTest Plan: CI\n\nReviewed By: yinghai\n\nDifferential Revision: D31852556\n\nfbshipit-source-id: 1e4212146ee67edc6b6568d25db79de525782788", "pr_number": "67077", "files_changed": ["aten/src/ATen/native/Bucketization.cpp", "aten/src/ATen/native/cuda/Bucketization.cu", "test/test_reductions.py", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["fb-exported", "Merged", "cla signed", "ciflow/default"]}, "f1b5f1898b": {"title": "Automated submodule update: kineto (#67133)", "body": "Summary:\nThis is an automated pull request to update the first-party submodule for [pytorch/kineto](https://github.com/pytorch/kineto).\n\nNew submodule commit: https://github.com/pytorch/kineto/commit/879a203d9bf554e95541679ddad6e0326f272dc1\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/67133\n\nTest Plan: Ensure that CI jobs succeed on GitHub before landing.\n\nReviewed By: mrshenli\n\nDifferential Revision: D31877172\n\nfbshipit-source-id: 224a499607d1f3bf7c00d8d8dd1fdac47cd33a3b", "pr_number": "67133", "files_changed": ["third_party/kineto"], "labels": ["open source", "cla signed", "ciflow/default"]}, "acb340de75": {"title": "[Pytorch][Bootcamp] Add fixes and vanilla testing for Adagrad non-vectorized and vectorized optimizers to handle complex numbers (#66671)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66671\n\nMade changes in the step function of the vectorized and non-vectorized adagrad optimizers to handle complex numbers as two real numbers as per 65711 on github\nghstack-source-id: 141442350\n\nTest Plan:\nbuck test mode/dev caffe2/test:optim -- 'test_adagrad_complex'\nhttps://pxl.cl/1Rd44\n\nReviewed By: albanD\n\nDifferential Revision: D31673503\n\nfbshipit-source-id: 90a0d0c69b556716e2d17c59ce80f09c750fc464", "pr_number": "66671", "files_changed": ["test/test_optim.py", "torch/optim/_functional.py", "torch/optim/_multi_tensor/_functional.py", "torch/optim/_multi_tensor/adagrad.py", "torch/optim/adagrad.py"], "labels": ["module: optimizer", "module: complex", "cla signed", "ciflow/default"]}, "d68bb50ef3": {"title": "Disable SVE when cross-compiling for M1 (#67114)", "body": "Summary:\nFollowup after https://github.com/pytorch/pytorch/issues/58653\nIt does not matter whether one compiles locally or cross-compiles -\nattempts to use SVE on M1 results in compiler crash as SVE ABI is not\ndefined on MacOS\n\nFixes #{issue number}\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/67114\n\nReviewed By: VitalyFedyunin\n\nDifferential Revision: D31869356\n\nPulled By: malfet\n\nfbshipit-source-id: 184e26ae40edc7ef7b703200b53ea7a15da74818", "pr_number": "67114", "files_changed": ["aten/src/ATen/CMakeLists.txt"], "labels": ["cla signed", "ciflow/default"]}, "fa7fb7b4d9": {"title": "[skip ci] Set test owner for test_profiler.py (#66831)", "body": "Summary:\nFollowup action to https://github.com/pytorch/pytorch/issues/66232\n\ncc ilia-cher robieta chaekit gdankel bitfort ngimel orionr nbcsm guotuofeng guyang3532 gaoteng-git\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66831\n\nReviewed By: gdankel\n\nDifferential Revision: D31909245\n\nPulled By: janeyx99\n\nfbshipit-source-id: 4156a5cffa215c29022fc4dab6ee5b442a509db4", "pr_number": "66831", "files_changed": ["test/test_profiler.py"], "labels": ["oncall: profiler", "cla signed", "ciflow/default"]}, "1f55dd83ac": {"title": "[WIP] wrap XLATensors into Python XLA wrapper class (#65841)", "body": "Summary:\n**Improbably** fixes https://github.com/pytorch/pytorch/issues/65130\n\nezyang I'm super n00b in Python extensions, is this what we want to do?\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65841\n\nReviewed By: navahgar\n\nDifferential Revision: D31889790\n\nPulled By: Krovatkin\n\nfbshipit-source-id: c7f077b89f6f02df1962ab83d9e13fcc348a227d", "pr_number": "65841", "files_changed": ["torch/_C/_autograd.pyi", "torch/autograd/__init__.py", "torch/autograd/variable.py", "torch/csrc/autograd/init.cpp", "torch/csrc/autograd/python_variable.cpp", "torch/csrc/autograd/python_variable.h"], "labels": ["cla signed", "ciflow/default"]}, "ad5731cacc": {"title": "[PyTorch] Add flop count for bmm and baddbmm (#66636)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66636\n\nAdd FLOP count for bmm and baddbmm, which is `2*b*m*n*k`.\n\nReviewed By: ngimel\n\nDifferential Revision: D31622061\n\nfbshipit-source-id: f3e1e1e34c45228693117b81647fb4a623c4085b", "pr_number": "66636", "files_changed": ["test/cpp/jit/test_misc.cpp", "torch/csrc/autograd/profiler_utils.cpp"], "labels": ["oncall: jit", "fb-exported", "cla signed", "ciflow/default"]}, "49251d05ec": {"title": "[skip ci] Set test owners for NNC tests (#66833)", "body": "Summary:\nAction following https://github.com/pytorch/pytorch/issues/66232\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66833\n\nReviewed By: albanD\n\nDifferential Revision: D31907812\n\nPulled By: janeyx99\n\nfbshipit-source-id: 5e5013b4276fd208ac68d61cf787679799695602", "pr_number": "66833", "files_changed": ["test/test_jit_fuser_te.py", "test/test_tensorexpr.py", "test/test_tensorexpr_pybind.py"], "labels": ["cla signed", "NNC", "ciflow/default"]}, "129e99fbce": {"title": "__getitem__: Ensure Tensor subclasses are not treated as tuples (#67202)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/67027\n\n`torch.Tensor` is considered a Mapping, but not a Sequence in Python\nbecause it uses `tp_as_mapping` instead of defining `__getitem__` in\nPython. However, If you try to overwrite `__getitem__` from Python\nit is considered a `Sequence` and so the tensor is treated like a\ntuple for indexing purposes.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/67202\n\nReviewed By: VitalyFedyunin\n\nDifferential Revision: D31908515\n\nPulled By: albanD\n\nfbshipit-source-id: 0ca55a36be3421f96428a8eacf5d195646252b38", "pr_number": "67202", "files_changed": ["torch/csrc/autograd/python_variable_indexing.cpp"], "labels": ["module: advanced indexing", "open source", "cla signed", "ciflow/default"]}, "a72a6365c9": {"title": "disallow requires_grad=True in make_tensor for integral inputs (#67149)", "body": "Summary:\nper title\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/67149\n\nReviewed By: albanD\n\nDifferential Revision: D31928613\n\nPulled By: ngimel\n\nfbshipit-source-id: 4491954c4fcd4a4e3121155d4451cc7370c27a0b", "pr_number": "67149", "files_changed": ["test/test_testing.py", "torch/testing/_creation.py", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["cla signed", "ciflow/default"]}, "dfa7225a38": {"title": "[Pytorch][Bootcamp] Add fix and testing for non-vectorized Adadelta optimizer to handle complex numbers (#66587)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66587\n\nMade some changes in the step function of the non-vectorized Adadelta optimizer to handle complex numbers as two real numbers as per 65711 on github\nghstack-source-id: 141484731\n\nTest Plan:\nbuck test mode/dev caffe2/test:optim -- 'test_adadelta_complex'\n\nhttps://pxl.cl/1R7kJ\n\nReviewed By: albanD\n\nDifferential Revision: D31630069\n\nfbshipit-source-id: 2741177b837960538ce39772897af36bbce7b7d8", "pr_number": "66587", "files_changed": ["test/test_optim.py", "torch/optim/_functional.py"], "labels": ["cla signed", "ciflow/default"]}, "d691bc1207": {"title": "Revert D31937065: [pytorch][PR] fix binding to the wrong python module", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD31937065 (https://github.com/pytorch/pytorch/commit/7ac8ed741d0f55d84bad0bcda49b4437f0b0c94d)\n\nOriginal commit changeset: 5c10b2870bcc\n\nfbshipit-source-id: 9b21ffea8054b8a3a0b96e1b78e933f8654e7f2f", "pr_number": null, "files_changed": ["torch/autograd/__init__.py", "torch/csrc/autograd/init.cpp"], "labels": []}, "f2f7b02b4c": {"title": "Add support for vmap+fwdAD for basic out-of-place op (#66291)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66291\n\nIn this PR:\n - Trivial batching rules for `make_dual` and `is_same_size` that enable forward ad + vmap functionality\n - Adds a check in gradcheck that is performed when both `check_batched_grad` and `check_forward_ad` are `True` (an OpInfo using this is added later in the stack).\n - Tests for the gradcheck functionality\n - Tests that basic out-of-place op works\n\nTest Plan: Imported from OSS\n\nReviewed By: albanD, saketh-are\n\nDifferential Revision: D31842018\n\nPulled By: soulitzer\n\nfbshipit-source-id: 84b18d9a77eeb19897757e37555581f2a9dc43d8", "pr_number": "66291", "files_changed": ["aten/src/ATen/BatchingRegistrations.cpp", "test/test_autograd.py", "torch/autograd/gradcheck.py"], "labels": ["Merged", "cla signed", "ciflow/default"]}, "3a1aa31a2f": {"title": "Add dummy bfloat16 VSX implementation (#67331)", "body": "Summary:\nJust a copy of DEFAULT bfloat16 implementation and revert restriction\nintroduced by https://github.com/pytorch/pytorch/pull/61630\n\nFixes https://github.com/pytorch/pytorch/issues/66867 and https://github.com/pytorch/pytorch/issues/62016\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/67331\n\nReviewed By: ngimel\n\nDifferential Revision: D31959916\n\nPulled By: malfet\n\nfbshipit-source-id: 8ca5e65ca041fef67ee18ddbb215cff01fd1e004", "pr_number": "67331", "files_changed": ["aten/src/ATen/cpu/vec/functional.h", "aten/src/ATen/cpu/vec/vec256/vsx/vec256_bfloat16_vsx.h", "aten/src/ATen/cpu/vec/vec256/vsx/vec256_common_vsx.h"], "labels": ["module: POWER", "Merged", "cla signed", "ciflow/default"]}, "9900310133": {"title": "Fix sign warnings in CUDA kernels (#66753)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66753\n\nFixes these Wextra compilation errors:\n```\nstderr: caffe2/aten/src/ATen/native/cuda/UnarySignKernels.cu: In lambda function:\ncaffe2/aten/src/ATen/native/cuda/UnarySignKernels.cu:49:72: error: comparison is always false due to limited range of data type [-Werror=type-limits]\n   49 |   AT_DISPATCH_ALL_TYPES_AND2 (https://github.com/pytorch/pytorch/commit/44fd312604fff5244e6d4fa1b3e440dd9b9e959f)(kBFloat16, ScalarType::Half, iter.input_dtype(), \"signbit_cuda\", [&]() {\n      |                                                                      ~~^~~\nstderr: caffe2/aten/src/ATen/native/cuda/BinaryMulDivKernel.cu: In lambda function:\ncaffe2/aten/src/ATen/native/cuda/BinaryMulDivKernel.cu:99:86: error: comparison is always false due to limited range of data type [-Werror=type-limits]\n   99 |     AT_DISPATCH_INTEGRAL_TYPES(dtype, \"div_floor_cuda\", [&]() {\n      |                                                                                      ^\ncaffe2/aten/src/ATen/native/cuda/BinaryMulDivKernel.cu:99:97: error: comparison is always false due to limited range of data type [-Werror=type-limits]\n   99 |     AT_DISPATCH_INTEGRAL_TYPES(dtype, \"div_floor_cuda\", [&]() {\n      |                                                                                                 ^\nstderr: caffe2/aten/src/ATen/native/cuda/BinaryMulDivKernel.cu: In lambda function:\ncaffe2/aten/src/ATen/native/cuda/BinaryMulDivKernel.cu:99:86: error: comparison is always false due to limited range of data type [-Werror=type-limits]\n   99 |     AT_DISPATCH_INTEGRAL_TYPES(dtype, \"div_floor_cuda\", [&]() {\n      |                                                                                      ^\n```\nAnd also these warnings:\n```\ncaffe2/c10/util/Half.h(461): warning: pointless comparison of unsigned integer with zero\n          detected during instantiation of \"std::enable_if<<expression>, __nv_bool>::type c10::overflows<To,From>(From) [with To=size_t, From=unsigned long]\"\ncaffe2/aten/src/ATen/native/Resize.h(45): here\ncaffe2/c10/util/Half.h(459): warning: pointless comparison of unsigned integer with zero\n          detected during instantiation of \"std::enable_if<<expression>, __nv_bool>::type c10::overflows<To,From>(From) [with To=size_t, From=unsigned long]\"\ncaffe2/aten/src/ATen/native/Resize.h(45): here\n```\nI thought I'd fixed this previously using `std::is_unsigned` in D25256251 (https://github.com/pytorch/pytorch/commit/cff1ff7fb6744dd6370ccccc5c90230ca3721140), but apparently that was insufficient.\n\nTest Plan: Sandcastle\n\nReviewed By: malfet, ngimel\n\nDifferential Revision: D31708173\n\nfbshipit-source-id: 7714f6bbf109d2f2164630d3fc46bad18046c06c", "pr_number": "66753", "files_changed": ["aten/src/ATen/native/cuda/BinaryMulDivKernel.cu", "aten/src/ATen/native/cuda/BinaryRemainderKernel.cu", "aten/src/ATen/native/cuda/UnarySignKernels.cu", "c10/util/Half.h", "c10/util/TypeSafeSignMath.h"], "labels": ["fb-exported", "Merged", "cla signed", "ciflow/default"]}, "708f7b1209": {"title": "Update extending doc to cover forward mode AD (#66962)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/66962\n\nReviewed By: VitalyFedyunin\n\nDifferential Revision: D31897782\n\nPulled By: albanD\n\nfbshipit-source-id: 64164783a14a7ed4cedc17da28f1181d9807a499", "pr_number": "66962", "files_changed": ["docs/source/autograd.rst", "docs/source/notes/extending.rst", "torch/autograd/function.py"], "labels": ["Merged", "cla signed", "ciflow/default"]}, "2267a984eb": {"title": "[ROCm] Add sparse mappings for CUDA->HIP translation (#67323)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/67323\n\nApplied patch proposed by Jeff https://github.com/pytorch/pytorch/pull/63948#issuecomment-952166982.\nIn PyTorch, we map cuBLAS->rocBLAS and cuSPARSE->hipSPARSE. Note the prefix, roc versus hip.\nThe 'hip' APIs offer a more direct CUDA-friendly mapping, but calling rocBLAS directly has better performance.\nUnfortunately, the `roc*` types and `hip*` types differ, i.e., `rocblas_float_complex` versus `hipComplex`.\nIn the case of SPARSE, we must use the hip types for complex instead of the roc types,\nbut the pytorch mappings assume roc. Therefore, we create a new SPARSE mapping that has a higher priority.\nIts mappings will trigger first, and only when a miss occurs will the lower-priority pytorch mapping take place.\nWhen a file contains \"sparse\" in the filename, a mapping marked with API_SPARSE is preferred over other choices.\n\ncc jeffdaily sunway513 jithunnair-amd ROCmSupport KyleCZH\n\nTest Plan: Imported from OSS\n\nReviewed By: ngimel\n\nDifferential Revision: D31969246\n\nPulled By: cpuhrsch\n\nfbshipit-source-id: 4ce1b35eaf9ef0d146a0955ce70c354ddd8f4669", "pr_number": "67323", "files_changed": ["torch/utils/hipify/cuda_to_hip_mappings.py", "torch/utils/hipify/hipify_python.py"], "labels": ["module: rocm", "open source", "cla signed", "ciflow/default"]}, "dea8b27433": {"title": "[Pytorch Edge] Make some torchbind classes selective (#67340)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/67340\n\nCurrently Torchbind classes arent selective. This makes is a rough granularity pass that will remove entire classes if they arent selected. If we need finer granularity in the future we can make individual methods within classes Selective though instrumenting that will be significantly more involved I think. On a linux build only __torch__.torch.classes._nnapi.Compilation remains unselective. I cant find where its registered :P (theres a couple Android only ones and presumably some metal only ones as well)\n\nMany of the classes registered in functions returned a reference to the class that was created. I talked with dreiss about it and we decided that this seemingly didnt serve any purpose, and leaving it like that would make the return value difficult (but possible) to create with selectivity. Since it seems useless anyway I just changed them to return an int so that they can still be called from a global scope, but not have any issues with the return type.\nghstack-source-id: 141690776\n\nTest Plan: CI, model unit tests, test models in prod apps\n\nReviewed By: dhruvbird\n\nDifferential Revision: D31092564\n\nfbshipit-source-id: 657f7eb83490292436c15cf134ceca9b72fb9e1a", "pr_number": "67340", "files_changed": ["aten/src/ATen/native/xnnpack/RegisterOpContextClass.cpp", "torch/custom_class.h", "torch/library.h"], "labels": ["cla signed", "ciflow/default"]}, "9e175400ac": {"title": "Moving python binding to _C and its decl to the right pyi file (#67365)", "body": "Summary:\nFixes #{issue number}\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/67365\n\nReviewed By: malfet, albanD\n\nDifferential Revision: D31972163\n\nPulled By: Krovatkin\n\nfbshipit-source-id: e5313c2c8cb810b57b7fe16af8ba26edbe486488", "pr_number": "67365", "files_changed": ["torch/_C/__init__.pyi.in", "torch/_C/_autograd.pyi", "torch/autograd/__init__.py", "torch/csrc/autograd/init.cpp"], "labels": ["Merged", "cla signed", "ciflow/default"]}, "c8dd90c858": {"title": "[PyTorch] Fix extra refcount bumps in ClassAttribute (#66723)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66723\n\nMissing move in constructor and forced copy in getter.\nghstack-source-id: 141594742\n\nTest Plan: CI\n\nReviewed By: suo\n\nDifferential Revision: D31697702\n\nfbshipit-source-id: c2018531e7ec4a4853cd003ea3753273a5fae7fb", "pr_number": "66723", "files_changed": ["aten/src/ATen/core/jit_type.h"], "labels": ["cla signed", "ciflow/default"]}, "fae1c0a434": {"title": "[PyTorch] Reduce refcount bumps in ClassType (#66724)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66724\n\nForwarding fix from previous diff through the ClassType getters & moving Types in where possible.\n\nghstack-source-id: 141594741\n\nTest Plan: CI\n\nReviewed By: suo\n\nDifferential Revision: D31697995\n\nfbshipit-source-id: 05d6af7c23e3b7a94db75b20d06338bc9ade3e20", "pr_number": "66724", "files_changed": ["aten/src/ATen/core/jit_type.h", "aten/src/ATen/core/type.cpp"], "labels": ["cla signed", "ciflow/default"]}, "6293e0ad61": {"title": "update coverage ignore to not skip whole modules (#67395)", "body": "Summary:\nThis reduces the chance of a newly added functions to be ignored by mistake.\n\nThe only test that this impacts is the coverage test that runs as part of the python doc build. So if that one works, it means that the update to the list here is correct.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/67395\n\nReviewed By: jbschlosser\n\nDifferential Revision: D31991936\n\nPulled By: albanD\n\nfbshipit-source-id: 5b4ce7764336720827501641311cc36f52d2e516", "pr_number": "67395", "files_changed": ["docs/source/conf.py"], "labels": ["Merged", "cla signed", "ciflow/default"]}, "2366948085": {"title": "[LT] Add ir_util for ComputePostOrder (#67282)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/67282\n\nTest Plan: `build/bin/test_lazy`\n\nReviewed By: wconstab, ngimel\n\nDifferential Revision: D31961754\n\nPulled By: desertfire\n\nfbshipit-source-id: 28466588ece8057640a7202b8c79cc1a4357d373", "pr_number": "67282", "files_changed": ["test/cpp/lazy/CMakeLists.txt", "test/cpp/lazy/test_ir_util.cpp", "tools/build_variables.bzl", "torch/csrc/lazy/core/ir_util.cpp", "torch/csrc/lazy/core/ir_util.h"], "labels": ["Merged", "cla signed", "ciflow/default"]}, "6900aacf54": {"title": "[fbcode] Fix operator_benchmark with jit mode (#67382)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/67382\n\ntwo simple updates:\n\n* fix running benchmark with --use_jit. Previously will fail with error\n\n  torch.jit.frontend.UnsupportedNodeError: import statements aren't supported:\n  File \"/proc/self/fd/3/bmm_test.py\", line 9\n  def __invoke_main():\n    import ctypes\n    ~~~~~~ <--- HERE\n    import ctypes.util\n    import errno\n\n* add matmul to bmm benchmark as D31837588\n\nTest Plan:\nbuck run mode/opt caffe2/benchmarks/operator_benchmark/pt:bmm_test --  --forward_only=True --mkl_num_threads=1 --omp_num_threads=1\n --use_jit=True\n\nReviewed By: ShijunK\n\nDifferential Revision: D31960528\n\nfbshipit-source-id: 84b892934149784d1b8a0f90b0233cc2f1cf1f5f", "pr_number": "67382", "files_changed": ["benchmarks/operator_benchmark/pt/bmm_test.py"], "labels": ["fb-exported", "cla signed", "ciflow/default"]}, "d3f03af496": {"title": "Fix indentation in forward_grad.h (#67359)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/67359\n\nTest Plan: Imported from OSS\n\nReviewed By: ngimel\n\nDifferential Revision: D31973039\n\nPulled By: albanD\n\nfbshipit-source-id: 80ca7870ea35977560334aa65aa344da6847c039", "pr_number": "67359", "files_changed": ["torch/csrc/autograd/forward_grad.h"], "labels": ["Merged", "cla signed", "ciflow/default"]}, "b27b1ff809": {"title": "Fix deadlock when forward and backward AD are used at the same time (#67360)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/67360\n\nTest Plan: Imported from OSS\n\nReviewed By: soulitzer\n\nDifferential Revision: D31973040\n\nPulled By: albanD\n\nfbshipit-source-id: f9c75c6497b622c86e8653027bce45461304eff5", "pr_number": "67360", "files_changed": ["test/test_autograd.py", "torch/csrc/autograd/forward_grad.h", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "cla signed", "ciflow/default"]}, "6ed68f3f84": {"title": "Document `torch.jit.is_tracing()` (#67326)", "body": "Summary:\nThis PR adds `torch.jit.is_tracing()` to the JIT API reference.\nThis function is widely used but left undocumented: https://github.com/search?q=torch.jit.is_tracing&type=code\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/67326\n\nReviewed By: tugsbayasgalan\n\nDifferential Revision: D31985251\n\nPulled By: Krovatkin\n\nfbshipit-source-id: 852b432b08d63df8bd7a7a02c9555e61f5f37978", "pr_number": "67326", "files_changed": ["docs/source/jit_language_reference.rst", "docs/source/jit_language_reference_v2.rst"], "labels": ["triaged", "open source", "Merged", "cla signed", "ciflow/default"]}, "b0a8ca2cb5": {"title": "add tags for inplace view ops in native_functions.yaml (#65412)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/65412\n\nTest Plan: Imported from OSS\n\nReviewed By: albanD\n\nDifferential Revision: D31942094\n\nPulled By: bdhirsh\n\nfbshipit-source-id: 1f7f6ea7df13e9f91b81ed64088e35e471800aa8", "pr_number": "65412", "files_changed": ["aten/src/ATen/native/native_functions.yaml", "tools/codegen/gen.py", "tools/codegen/model.py"], "labels": ["Merged", "cla signed", "ciflow/default"]}, "cee4e8f35d": {"title": "Add FlexiBLAS build support per #64752 (#64815)", "body": "Summary:\nTo enable building torch+dependencies, set WITH_BLAS=flexi BLAS=FlexiBLAS\n\nFixes https://github.com/pytorch/pytorch/issues/64752\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/64815\n\nReviewed By: jbschlosser\n\nDifferential Revision: D31997745\n\nPulled By: albanD\n\nfbshipit-source-id: db208d59002f5896608a03132616400f09d972aa", "pr_number": "64815", "files_changed": ["cmake/Dependencies.cmake", "cmake/Modules/FindBLAS.cmake", "cmake/Modules/FindFlexiBLAS.cmake", "cmake/Modules/FindLAPACK.cmake", "setup.py"], "labels": ["open source", "cla signed", "ciflow/default"]}, "f3aae62942": {"title": "Port `tril` and `triu` to structured kernels (#67055)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/67055\n\nThis PR ports `tril` and `triu` operations to structured kernels.\nghstack-source-id: 141797608\n\nTest Plan: Extended the existing unit tests.\n\nReviewed By: wanchaol\n\nDifferential Revision: D31844638\n\nfbshipit-source-id: 03ea4aa2410b042cafc3c5397e777a9ca5351b39", "pr_number": "67055", "files_changed": ["aten/src/ATen/native/TriangularOps.cpp", "aten/src/ATen/native/cuda/TriangularOps.cu", "aten/src/ATen/native/native_functions.yaml"], "labels": ["cla signed", "module: structured kernels", "ciflow/default"]}, "5b8b2382d1": {"title": "Mark mv as CompositeExplicitAutograd (#67373)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/67373\n\nFrom the implementation of mv, it's decomposed into addmv. So it should\nbe a CompositeExplicitAutograd op.\n\nTest Plan: It shouldn't change any behaviors. So, CI.\n\nReviewed By: bdhirsh\n\nDifferential Revision: D31973265\n\nPulled By: alanwaketan\n\nfbshipit-source-id: 3b6850f08e6f671b908a177f148cc6194baa8a13", "pr_number": "67373", "files_changed": ["aten/src/ATen/native/native_functions.yaml"], "labels": ["cla signed", "ciflow/default"]}, "bd5e6fe5ac": {"title": "Skip complex128 dtype for test_addmm_sizes_all_sparse_csr Windows test (#67453)", "body": "Summary:\nWindows CUDA 11.1 periodic CI is failing. See https://github.com/pytorch/pytorch/pull/63511#issuecomment-953940183.\nI don't understand though why periodic-win-vs2019-cuda11.1-py3 was triggered on the PR, but no test from `test_sparse_csr.py` were run https://github.com/pytorch/pytorch/runs/3975200820?check_suite_focus=true.\n\ncc nikitaved pearu cpuhrsch IvanYashchuk mruberry\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/67453\n\nReviewed By: malfet, seemethere, janeyx99\n\nDifferential Revision: D31997574\n\nPulled By: cpuhrsch\n\nfbshipit-source-id: ae8bfb6da865014f39e6ad5675eb17e5a4d39744", "pr_number": "67453", "files_changed": ["test/test_sparse_csr.py"], "labels": ["module: sparse", "module: tests", "open source", "cla signed", "ciflow/win"]}, "b8f07689f2": {"title": "[ROCm] Enable frexp support for ROCm builds (#67226)", "body": "Summary:\nThe frexp function has been enabled in ROCm code.  Updating PyTorch\nto enable this functionality.\n\ncc jeffdaily sunway513 jithunnair-amd ROCmSupport KyleCZH\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/67226\n\nReviewed By: jbschlosser\n\nDifferential Revision: D31984606\n\nPulled By: ngimel\n\nfbshipit-source-id: b58eb7f226f6eb3e17d8b1e2517a4ea7297dc1d5", "pr_number": "67226", "files_changed": ["aten/src/ATen/native/cuda/UnaryOpsKernel.cu", "torch/testing/_internal/common_methods_invocations.py", "torch/utils/hipify/cuda_to_hip_mappings.py"], "labels": ["module: rocm", "open source", "cla signed", "ciflow/default"]}, "6696c59af4": {"title": "Adding `optimizer` attribute to SequentialLR (#67406)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/67318 :)\n\ncc albanD, datumbox\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/67406\n\nReviewed By: jbschlosser\n\nDifferential Revision: D31997873\n\nPulled By: albanD\n\nfbshipit-source-id: f579fb886d049a545673fd92ef5892fcf501bcc6", "pr_number": "67406", "files_changed": ["torch/optim/lr_scheduler.py"], "labels": ["cla signed", "ciflow/default"]}, "18807273cb": {"title": "Fix Ads build broken due to comparison type mismatch (#67526)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/67526\n\nBuild error P465285570 due to D31942093 (https://github.com/pytorch/pytorch/commit/0032fa772588cd20f2b85324bdeb31ac497bbf29)\n\n(Note: this ignores all push blocking failures!)\n\nTest Plan: build passes after fix\n\nReviewed By: jbschlosser\n\nDifferential Revision: D32013247\n\nfbshipit-source-id: b60a65afd7a5a2d3249150fbc2ac52374d62a591", "pr_number": "67526", "files_changed": ["aten/src/ATen/core/VariableFallbackKernel.cpp"], "labels": ["fb-exported", "cla signed", "ciflow/default"]}, "99282126dc": {"title": "pytorch quantization: document the custom module APIs (#67449)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/67449\n\nAdds a description of what the current custom module API does\nand API examples for Eager mode and FX graph mode to the main\nPyTorch quantization documentation page.\n\nTest Plan:\n```\ncd docs\nmake html\npython -m http.server\n// check the docs page, it renders correctly\n```\n\nReviewed By: jbschlosser\n\nDifferential Revision: D31994641\n\nPulled By: vkuzo\n\nfbshipit-source-id: d35a62947dd06e71276eb6a0e37950d3cc5abfc1", "pr_number": "67449", "files_changed": ["docs/source/quantization.rst"], "labels": ["cla signed", "ciflow/default"]}, "2e156f649e": {"title": "Sort output of *NativeFunctions.h (#67046)", "body": "Summary:\nThis ensures deterministic output, allowing systems like ccache to be\nmore effective.\n\ncc ezyang bhosmer bdhirsh\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/67046\n\nReviewed By: VitalyFedyunin\n\nDifferential Revision: D31896114\n\nPulled By: bdhirsh\n\nfbshipit-source-id: d29ef0cf6c7e3408b104c5239b620eaa24327088", "pr_number": "67046", "files_changed": ["tools/codegen/gen_backend_stubs.py"], "labels": ["triaged", "open source", "module: codegen", "cla signed", "ciflow/default"]}, "ddc9bd335b": {"title": "Adds reference vs. noncontiguous OpInfo test (#67434)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/63341.\n\nThis PR adds a new test, `test_noncontigous_samples`, that runs ops forward and backward and compares their outputs and grads between \"normal\" contiguous SampleInputs and noncontiguous SampleInputs. This test should preclude the need for noncontiguous SampleInputs going forward.\n\nThe test was added by generalizing the `.numpy()` transform on SampleInputs to support a new `.noncontiguous()` transform and copying forward/backward patterns from other tests in test_ops.py. It also discovered that many SampleInputs were incorrectly reusing tensors, so those have been revised. SampleInputs creating noncontiguous tensors for testing have also been altered to no longer do so.\n\nIn addition, this test discovered the following high priority silent correctness issues:\n\n- https://github.com/pytorch/pytorch/issues/67432\n- https://github.com/pytorch/pytorch/issues/67517\n- https://github.com/pytorch/pytorch/issues/67513\n- https://github.com/pytorch/pytorch/issues/67512\n- https://github.com/pytorch/pytorch/issues/67470\n\nIt also identified the following issues:\n- https://github.com/pytorch/pytorch/issues/67539\n\nThe pow OpInfo also incorrectly specified that pow supported the bool datatype, and this has been fixed. Its SampleInputs were written in a way that made requests for boolean SampleInputs return type promoting inputs that never actually tried to compute pow in bool.\n\nThis PR suggests we should add the following guidance for writing SampleInputs:\n\n- ensure that all SampleInputs are independent of each other (don't reuse tensors)\n- ensure that all SampleInput tensors have no grad or backward functions (no autograd history) -- they should be leaves\n- prefer keeping sample inputs simple where possible, a good set of handwritten samples that test interesting cases may be better than an exhaustive but hard to read and maintain programmatic enumeration\n- keep code readable by using functools.partial and writing simple inline helpers; break up large statements into a more readable series of smaller statements; especially don't write complicated generator expressions with a `for` at the end!\n\nfyi kshitij12345 krshrimali pmeier anjali411 saketh-are zou3519 dagitses\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/67434\n\nReviewed By: ngimel\n\nDifferential Revision: D32014557\n\nPulled By: mruberry\n\nfbshipit-source-id: b17e19adc1d41e24441f0765af13d381fef5e3c1", "pr_number": "67434", "files_changed": ["test/test_ops.py", "torch/testing/_internal/common_methods_invocations.py", "torch/testing/_internal/common_utils.py"], "labels": ["Merged", "cla signed", "ciflow/slow-gradcheck", "ciflow/all"]}, "fcba8018c2": {"title": "Update codeowners for sphinx conf (#67548)", "body": "Summary:\nAdd a codeowner for the conf file to ensure allowlist modification is monitored.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/67548\n\nReviewed By: jbschlosser\n\nDifferential Revision: D32023929\n\nPulled By: albanD\n\nfbshipit-source-id: 63f18cdd725cc60993a6c0a9e3529ed95845e0bb", "pr_number": "67548", "files_changed": ["CODEOWNERS"], "labels": ["Merged", "cla signed", "ciflow/default"]}, "c19cda5782": {"title": "[skip ci] Add test owners for a special hi-pri class of tests (#67553)", "body": "Summary:\nAction following https://github.com/pytorch/pytorch/issues/66232\n\nThis change does require some context: there were several suggestions regarding what to do about this group of tests: tests that are core and crucial to all of PyTorch and are too broad to be owned by one team.\n1. Let's add a \"module: core\" and put people behind it! This idea sounds appealing unless you are one of the people backing the label. From talking to albanD among others, this idea of putting all these core tests on the shoulder of a few people or one team isn't super fair and I have not yet found anyone willing to take on this job.\n2. Taking advantage of the fact that we already have a triaging oncall that takes turns triaging issues, we can leave these tests essentially unlabeled and allow the oncall to triage these tests. Since these tests are crucial to PyTorch, we'll add the \"high priority\" label to mark them different from other unowned tests (see https://github.com/pytorch/pytorch/issues/67552).\n3. I _could_ still create an unbacked label \"module: core\" and attribute these tests there, but I don't like the idea of creating a facade that the tests are \"triaged\" to a label when no one is actually taking a look.\n\nNow we could potentially break these tests down into smaller files so that each piece _could_ be owned by a team, but 1. I don't know if this is currently feasible and 2. This approach does not prevent that from happening in the future.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/67553\n\nReviewed By: albanD\n\nDifferential Revision: D32025004\n\nPulled By: janeyx99\n\nfbshipit-source-id: 1fb1aa4c27e305695ab6e80ae3d02f90519939c0", "pr_number": "67553", "files_changed": ["test/test_foreach.py", "test/test_functional_autograd_benchmark.py", "test/test_module_init.py", "test/test_modules.py", "test/test_ops.py", "test/test_overrides.py", "test/test_python_dispatch.py", "test/test_pytree.py", "test/test_stateless.py", "test/test_utils.py"], "labels": ["Merged", "cla signed", "ciflow/default"]}, "5c77ccefe0": {"title": "Resolves #67227 documentation issue (#67379)", "body": "Summary:\nChanged \"Chi2\" in the docstring to a more intuitive \"Chi-squared\"\n\nFixes https://github.com/pytorch/pytorch/issues/67227\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/67379\n\nReviewed By: jbschlosser\n\nDifferential Revision: D32023761\n\nPulled By: ngimel\n\nfbshipit-source-id: b514b49726f616914871a9a831aa10e12e4be90b", "pr_number": "67379", "files_changed": ["torch/distributions/chi2.py"], "labels": ["open source", "cla signed", "ciflow/default"]}, "c00806beda": {"title": "Add skipXLA and expectedFailureXLA decorator (#66857)", "body": "Summary:\nAdd skipXLA and expectedFailureXLA decorator and relevant test.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66857\n\nReviewed By: ngimel\n\nDifferential Revision: D32039856\n\nPulled By: mruberry\n\nfbshipit-source-id: 3c99d5e06c1c7684d1f798c11c783bd6ebea9899", "pr_number": "66857", "files_changed": ["test/test_torch.py", "torch/testing/_internal/common_device_type.py"], "labels": ["triaged", "open source", "cla signed", "ciflow/default"]}, "4a2bbc619d": {"title": "move functionalize fallback out of aten/core (#67564)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/67564\n\nmoves the functionalize fallback out of aten/core and into aten, which should fix the issue described at https://fb.workplace.com/groups/163556484490704/permalink/1029416141238063/. I'm still not clear on why this didn't fail anything in CI / sandcastle on the initial diff: D31942093 (https://github.com/pytorch/pytorch/commit/0032fa772588cd20f2b85324bdeb31ac497bbf29)\nghstack-source-id: 141959891\n\nTest Plan: Locally, running `buck build mode/opt //sigrid/feed/prediction_replayer:fully_remote_replayer_main`\n\nReviewed By: zou3519\n\nDifferential Revision: D32027585\n\nfbshipit-source-id: 2d86c4a6b3a73b00ee0ccee2f89a54704ed83e00", "pr_number": "67564", "files_changed": ["aten/src/ATen/FunctionalizeFallbackKernel.cpp", "aten/src/ATen/core/VariableFallbackKernel.cpp", "tools/build_variables.bzl"], "labels": ["Merged", "cla signed", "ciflow/default"]}, "aa16de517d": {"title": "Revert D31984694: [pytorch][PR] make `TORCH_(CUDABLAS|CUSOLVER)_CHECK` usable in custom extensions", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD31984694 (https://github.com/pytorch/pytorch/commit/d4493b27ee513100aab94381a2fc86ba6188f1a1)\n\nOriginal commit changeset: 0035ecd13980\n\nfbshipit-source-id: c85689007719c9e4a930b0a8a32d481a501d3c14", "pr_number": null, "files_changed": ["aten/src/ATen/cuda/CUDABlas.cpp", "aten/src/ATen/cuda/CUDASolver.cpp", "aten/src/ATen/cuda/Exceptions.h", "test/cpp_extensions/cublas_extension.cpp", "test/cpp_extensions/cusolver_extension.cpp", "test/cpp_extensions/setup.py", "test/test_cpp_extensions_aot.py"], "labels": []}, "510e3026a9": {"title": "[numpy] add torch.argwhere (#64257)", "body": "Summary:\nAdds `torch.argwhere` as an alias to `torch.nonzero`\n\nCurrently, `torch.nonzero` is actually provides equivalent functionality to `np.argwhere`.\n\nFrom NumPy docs,\n> np.argwhere(a) is almost the same as np.transpose(np.nonzero(a)), but produces a result of the correct shape for a 0D array.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/64257\n\nReviewed By: qihqi\n\nDifferential Revision: D32049884\n\nPulled By: saketh-are\n\nfbshipit-source-id: 016e49884698daa53b83e384435c3f8f6b5bf6bb", "pr_number": "64257", "files_changed": ["aten/src/ATen/core/aten_interned_strings.h", "aten/src/ATen/core/interned_strings.h", "aten/src/ATen/native/TensorAdvancedIndexing.cpp", "aten/src/ATen/native/native_functions.yaml", "docs/source/tensors.rst", "docs/source/torch.rst", "torch/_tensor_docs.py", "torch/_torch_docs.py", "torch/overrides.py", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["oncall: jit", "open source", "Merged", "cla signed", "ciflow/default"]}, "9cdd1d7e48": {"title": "Docs module check (#67440)", "body": "Summary:\nAdd check to make sure we do not add new submodules without documenting them in an rst file.\nThis is especially important because our doc coverage only runs for modules that are properly listed.\n\ntemporarily removed \"torch\" from the list to make sure the failure in CI looks as expected. EDIT: fixed now\n\nThis is what a CI failure looks like for the top level torch module as an example:\n![image](https://user-images.githubusercontent.com/6359743/139264690-01af48b3-cb2f-4cfc-a50f-975fca0a8140.png)\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/67440\n\nReviewed By: jbschlosser\n\nDifferential Revision: D32005310\n\nPulled By: albanD\n\nfbshipit-source-id: 05cb2abc2472ea4f71f7dc5c55d021db32146928", "pr_number": "67440", "files_changed": ["docs/source/conf.py"], "labels": ["cla signed", "ciflow/default"]}, "00da7b9a3b": {"title": "Set test owner for vmap (#67582)", "body": "Summary:\nMore leftover actions from https://github.com/pytorch/pytorch/issues/66232\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/67582\n\nReviewed By: zou3519\n\nDifferential Revision: D32045160\n\nPulled By: janeyx99\n\nfbshipit-source-id: 92ae9a533285b05b44bd04bb6127061c6fddd689", "pr_number": "67582", "files_changed": ["test/test_vmap.py"], "labels": ["Merged", "cla signed", "ciflow/default"]}, "da29655797": {"title": "Disable miopen test for convolution on mobile (#66564)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66564\n\nMobile thinks that we are segfaulting in _convolution, and this\nis the most recent substantive change to this function.  I think\nit's pretty unlikely to have caused the crash, but if we don't have\nany better ideas we should try this.\nghstack-source-id: 141972758\n\nTest Plan: ship it and see if it resolves the error report\n\nReviewed By: kimishpatel\n\nDifferential Revision: D31598633\n\nfbshipit-source-id: c34f4b0b7b8529e21fd019c886ad8d68ffe286b0", "pr_number": "66564", "files_changed": ["aten/src/ATen/native/Convolution.cpp"], "labels": ["cla signed", "ciflow/default"]}, "a122ba776a": {"title": "Fix less_than_lowest warnings (#67422)", "body": "Summary:\nFixes useless comparison against zero warnings for Half.h\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/67422\n\nTest Plan: Sandcastle\n\nReviewed By: ngimel\n\nDifferential Revision: D31951939\n\nfbshipit-source-id: 3e9940adda2d57b4d9b122f3862706c673f9ef4b", "pr_number": "67422", "files_changed": ["c10/util/Half.h", "c10/util/TypeSafeSignMath.h"], "labels": ["fb-exported", "cla signed", "ciflow/default"]}, "4d99bc839b": {"title": "Remove TH/THC Storage functions for unused dtypes (#67480)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/67466\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/67480\n\nReviewed By: mruberry\n\nDifferential Revision: D32023494\n\nPulled By: ngimel\n\nfbshipit-source-id: 8827e1d6e765fee7219b5ee9888a1a3e3c5fbe89", "pr_number": "67480", "files_changed": ["aten/src/TH/THStorageFunctions.cpp", "aten/src/TH/THStorageFunctions.h", "aten/src/TH/generic/THStorageCopy.cpp", "aten/src/TH/generic/THStorageCopy.h", "aten/src/TH/generic/THTensor.cpp", "aten/src/THC/THCStorage.cpp", "aten/src/THC/THCStorage.cu", "aten/src/THC/THCStorage.h", "aten/src/THC/THCStorageCopy.cpp", "aten/src/THC/THCStorageCopy.cu", "aten/src/THC/THCStorageCopy.h", "aten/src/THC/generic/THCStorageCopy.cpp", "aten/src/THC/generic/THCStorageCopy.cu", "aten/src/THC/generic/THCStorageCopy.h", "torch/csrc/Storage.cpp", "torch/csrc/Storage.h", "torch/csrc/cuda/Storage.cpp", "torch/csrc/cuda/Storage.h", "torch/csrc/cuda/serialization.cpp", "torch/csrc/cuda/serialization.h", "torch/csrc/cuda/utils.h", "torch/csrc/generic/Storage.cpp", "torch/csrc/serialization.cpp", "torch/csrc/serialization.h", "torch/csrc/utils.cpp", "torch/csrc/utils.h"], "labels": ["triaged", "open source", "cla signed", "ciflow/default"]}, "76f57cd442": {"title": "[CODEOWNERS] Remove @neginraoof (#67631)", "body": "Summary:\nShe no longer works on the ONNX exporter\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/67631\n\nReviewed By: malfet\n\nDifferential Revision: D32070435\n\nPulled By: msaroufim\n\nfbshipit-source-id: d741a15bd7a916745aa7f2f3d9bb1dc699553900", "pr_number": "67631", "files_changed": ["CODEOWNERS"], "labels": ["open source", "cla signed", "ciflow/default"]}, "ba369ea053": {"title": "check to ensure profiler_edge is only added when use_kineto is on (#67494)", "body": "Summary:\nFixes #{issue number}\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/67494\n\nReviewed By: jbschlosser\n\nDifferential Revision: D32031142\n\nPulled By: mcr229\n\nfbshipit-source-id: 8267f0e02c5bed0fbc4956af6935a551bedb27ef", "pr_number": "67494", "files_changed": ["test/cpp/jit/CMakeLists.txt"], "labels": ["oncall: jit", "cla signed", "ciflow/default"]}, "152f665dee": {"title": "Inserted check for PyObject_IsInstance in THPVariableCheck (#67588)", "body": "Summary:\nInserted check for the return of PyObject_IsInstance to capture the case in which it raises an exception and return -1. When this happen THPVariable_Check now throws a python_error to signal the exception.\n\nFixes https://github.com/pytorch/pytorch/issues/65084\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/67588\n\nReviewed By: mruberry\n\nDifferential Revision: D32064776\n\nPulled By: albanD\n\nfbshipit-source-id: 895c7682e0991ca257e27f9638a7462d83707320", "pr_number": "67588", "files_changed": ["torch/csrc/autograd/python_variable.h"], "labels": ["open source", "Merged", "cla signed", "ciflow/default"]}, "7deb1726ea": {"title": "Remove native_functions.yaml dependency from ScanKernels.cu (#66620)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66620\n\nThis splits the Tensor-dependant code out into a cpp file.\n\nA slight complicating factor is `scan_dim` using `copy_` to handle\nnon-contiguous out arguments. So, I've moved that code into the\ncaller which does introduce some duplication. Though it's only ~10\nlines extra in total.\n\nTest Plan: Imported from OSS\n\nReviewed By: VitalyFedyunin\n\nDifferential Revision: D31856106\n\nPulled By: dagitses\n\nfbshipit-source-id: 91bb4ce5e7c6487e3ea0d5ec4d9f7a625d8ef978", "pr_number": "66620", "files_changed": ["aten/src/ATen/native/cuda/ScanKernels.cpp", "aten/src/ATen/native/cuda/ScanKernels.cu", "aten/src/ATen/native/cuda/ScanKernels.h", "caffe2/CMakeLists.txt"], "labels": ["open source", "Merged", "cla signed", "ciflow/default"]}, "16ee6409ee": {"title": "Changed value constraint of exponential dist (#67184)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/67183.\n\ncc fritzo neerajprad alicanb nikitaved\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/67184\n\nReviewed By: ejguan\n\nDifferential Revision: D32114661\n\nPulled By: neerajprad\n\nfbshipit-source-id: ea23e59f38a23a7b0bab4fbbd98ae3feba468b9c", "pr_number": "67184", "files_changed": ["torch/distributions/exponential.py"], "labels": ["module: distributions", "triaged", "open source", "Merged", "cla signed", "ciflow/default"]}, "05d1dcc14c": {"title": "Split channels_last test cases for tensor conversion OpInfos (#67368)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/67368\n\nThis PR adds an addition test variant for the tensor conversion\nfunctions (bfloat16, char, long, ...) that tests channels_last. This is\nbecause some backends (mostly just functorch right now) don't have\nchannels last handling and may want to test that separately from the\nmore general case of these operations.\n\nTest Plan: - wait for tests\n\nReviewed By: mruberry\n\nDifferential Revision: D31972959\n\nPulled By: zou3519\n\nfbshipit-source-id: 68fea46908b2cdfeb0607908898bb8f9ef25b264", "pr_number": "67368", "files_changed": ["test/test_jit_fuser_te.py", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "cla signed", "ciflow/slow-gradcheck", "ciflow/default", "ciflow/all"]}, "ea4d983885": {"title": "Modify \"gemm\" code to enable access to \"sbgemm_\" routine in OpenBLAS (#58831)", "body": "Summary:\nOpenBLAS recently added support for bfloat16 GEMM, so this change has PyTorch call out to OpenBLAS for that, like it does for single and double precision\n\nOur goal is to try to enable PyTorch to make calls to \"sbgemm\" in OpenBLAS.\n\nWe are prepared (if it is your preference) to add fences to the code to limit this change to the Power architecture,\nbut our first instinct is that anyone on any architecture that enables access to sbgemm in their OpenBLAS library\nshould be able to use this code.  (but again, we respect that as we are just starting to modify PyTorch, we respect\nyour guidance!)\n\n(there is no issue number related to this)\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/58831\n\nReviewed By: albanD\n\nDifferential Revision: D29951900\n\nPulled By: malfet\n\nfbshipit-source-id: 3d0a4a638ac95b2ff2e9f6d08827772e28d397c3", "pr_number": "58831", "files_changed": ["aten/src/ATen/native/CPUBlas.cpp", "aten/src/ATen/native/CPUBlas.h", "cmake/Modules/FindBLAS.cmake", "cmake/Summary.cmake"], "labels": ["triaged", "open source", "Merged", "cla signed", "ciflow/default"]}, "201f7d330a": {"title": "Remove duplicate check in distributions arg validation (#67741)", "body": "Summary:\nPartial fix for https://github.com/pytorch/pytorch/issues/66800. (Duplicate of https://github.com/pytorch/pytorch/issues/67725 against pytorch/pytorch so as to trigger TorchBench)\n\nhttps://github.com/pytorch/pytorch/issues/61056 added a more verbose error message for distributions failing argument validation. However, it did not replace the earlier error check as was originally intended and was flagged by xuzhao9 as being the potential cause of a perf regression in `test_eval[soft_actor_critic-cuda-eager]`.\n\nxuzhao9: Is there a way for me to check if this resolves the perf issue you mentioned?\n\ncc VitalyFedyunin ngimel\n\nNote that existing tests already check for the error message and should verify that the removed lines are redundant.\n\nRUN_TORCHBENCH: soft_actor_critic\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/67741\n\nReviewed By: neerajprad\n\nDifferential Revision: D32135675\n\nPulled By: xuzhao9\n\nfbshipit-source-id: 37dfd3ff53b95017c763371979ab3a2c302a72b9", "pr_number": "67741", "files_changed": ["torch/distributions/distribution.py"], "labels": ["Merged", "cla signed", "ciflow/default"]}, "6df0d7d502": {"title": "[lint] add basic lintrunner compatibility (#67110)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/67110\n\nAdds support for using lintrunner with:\n- clang-format\n- clang-tidy\n- flake8\n- mypy\n\nTest Plan: Imported from OSS\n\nReviewed By: driazati\n\nDifferential Revision: D32145555\n\nPulled By: suo\n\nfbshipit-source-id: 2150348e26fba4ae738cd0b9684b2889ce0f1133", "pr_number": "67110", "files_changed": [".lintrunner.toml", "tools/linter/adapters/README.md", "tools/linter/adapters/clangformat_linter.py", "tools/linter/adapters/clangtidy_init.py", "tools/linter/adapters/clangtidy_linter.py", "tools/linter/adapters/flake8_linter.py", "tools/linter/adapters/grep_linter.py", "tools/linter/adapters/mypy_linter.py", "tools/linter/install/download_bin.py"], "labels": ["Merged", "cla signed", "ciflow/default"]}, "4d601a1c36": {"title": "codegen: Split up source, header and Declarations.yaml generation (#67497)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/67497\n\nThis allows more of the code-generation to happen in parallel, whereas\npreviously all codegen was serialized.\n\nTest Plan: Imported from OSS\n\nReviewed By: dagitses, mruberry\n\nDifferential Revision: D32027250\n\nPulled By: albanD\n\nfbshipit-source-id: 6407c4c3e25ad15d542aa73da6ded6a309c8eb6a", "pr_number": "67497", "files_changed": ["aten/src/ATen/CMakeLists.txt", "caffe2/CMakeLists.txt", "cmake/Codegen.cmake", "tools/codegen/gen.py", "torch/CMakeLists.txt"], "labels": ["open source", "Merged", "cla signed", "ciflow/default"]}, "89b02fc70b": {"title": "[StaticRuntime][Easy] Correct typos in test_static_runtime (#67739)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/67739\n\nTest Plan:\n```\nbuck test //caffe2/benchmarks/static_runtime:static_runtime_cpptest\n```\n\nReviewed By: mikeiovine\n\nDifferential Revision: D32125879\n\nfbshipit-source-id: bd989e5088edff87624b858bd9045dfe9da3fbe7", "pr_number": "67739", "files_changed": ["benchmarks/static_runtime/test_static_runtime.cc"], "labels": ["fb-exported", "Merged", "cla signed", "ciflow/default"]}, "5fd93fb5f8": {"title": "broaden retries on TestHub (#67779)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/67779\n\nNot all flaky failures from this test are URLErrors; I think we should\nerr on the side of being expansive with retries here.\n\nTest Plan: Imported from OSS\n\nReviewed By: jamesr66a\n\nDifferential Revision: D32145434\n\nPulled By: suo\n\nfbshipit-source-id: 3c3274b2080681fcafb3ea6132e420605f65c429", "pr_number": "67779", "files_changed": ["test/test_utils.py"], "labels": ["Merged", "cla signed", "ciflow/default"]}, "fddfb81dd0": {"title": "Add BF16 type to _autocast_to_full_precision (#67707)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/67707\n\nhttps://github.com/pytorch/pytorch/pull/63939/files has added FP16 support to torchscript.\n\nThis is to add BF16 device type when doing full conversion.\n\nTest Plan: Unit test. Also tested BF16 locally on A100 using MLP model.\n\nReviewed By: idning\n\nDifferential Revision: D32027152\n\nfbshipit-source-id: b2a5ff2b22ea1e02306b0399f2b39b8493be4f45", "pr_number": "67707", "files_changed": ["aten/src/ATen/native/TensorConversions.cpp", "test/test_jit_autocast.py"], "labels": ["fb-exported", "Merged", "cla signed", "ciflow/default"]}, "fd77fff0b1": {"title": "[FSDP] customizable backend in test (#67135)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/67135\n\nAdd ability to use env var backend for quicker testing (and gloo2 in\nthe future)\nghstack-source-id: 142274304\n\nTest Plan: CI\n\nReviewed By: mrshenli\n\nDifferential Revision: D31878285\n\nfbshipit-source-id: 80ae7107cd631a1a15ebc23262b27d8192cfe4b6", "pr_number": "67135", "files_changed": ["torch/testing/_internal/common_fsdp.py"], "labels": ["Merged", "cla signed", "ciflow/default"]}, "99c7a9f09d": {"title": "fix bfloat16 autocast skip (#67822)", "body": "Summary:\nPer title\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/67822\n\nReviewed By: mruberry\n\nDifferential Revision: D32162605\n\nPulled By: ngimel\n\nfbshipit-source-id: eb5ccf6c441231e572ec93ac8c2638d028abecad", "pr_number": "67822", "files_changed": ["test/test_jit_autocast.py"], "labels": ["Merged", "cla signed", "ciflow/default"]}, "b8d365ca3a": {"title": "ci fix (#67826)", "body": "Summary:\nFixes #{issue number}\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/67826\n\nReviewed By: Chillee\n\nDifferential Revision: D32164770\n\nPulled By: mruberry\n\nfbshipit-source-id: c1de7e6db6d0cb1761388f1ea0178dbff3fe6dc8", "pr_number": "67826", "files_changed": ["test/test_jit_autocast.py"], "labels": ["Merged", "cla signed", "ciflow/default"]}, "04fe4382ec": {"title": "Automated submodule update: tensorpipe (#67769)", "body": "Summary:\nThis is an automated pull request to update the first-party submodule for [pytorch/tensorpipe](https://github.com/pytorch/tensorpipe).\n\nNew submodule commit: https://github.com/pytorch/tensorpipe/commit/caa2ccb39425bbb26797fed205565291c4c02c5f\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/67769\n\nTest Plan: Ensure that CI jobs succeed on GitHub before landing.\n\nReviewed By: lw\n\nDifferential Revision: D32138256\n\nfbshipit-source-id: dfe4c73ae25c8f362f2917dd7594bdcd418c2a0d", "pr_number": "67769", "files_changed": ["third_party/tensorpipe"], "labels": ["open source", "Merged", "cla signed", "ciflow/default"]}, "61ed9285dd": {"title": "Automated submodule update: tensorpipe (#67845)", "body": "Summary:\nThis is an automated pull request to update the first-party submodule for [pytorch/tensorpipe](https://github.com/pytorch/tensorpipe).\n\nNew submodule commit: https://github.com/pytorch/tensorpipe/commit/d2aa3485e8229c98891dfd604b514a39d45a5c99\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/67845\n\nTest Plan: Ensure that CI jobs succeed on GitHub before landing.\n\nReviewed By: lw\n\nDifferential Revision: D32170821\n\nfbshipit-source-id: 1958e824a9f02c5178fa5d4a73a171dedefc540c", "pr_number": "67845", "files_changed": ["third_party/tensorpipe"], "labels": ["open source", "Merged", "cla signed", "ciflow/default"]}, "927da4d32f": {"title": "Remove native_functions.yaml dependency from Sort.cu (#66793)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/66793\n\nTest Plan: Imported from OSS\n\nReviewed By: ejguan\n\nDifferential Revision: D31856100\n\nPulled By: dagitses\n\nfbshipit-source-id: 1469ce1deb4124f2a9e160a8e3298d56ac3f6561", "pr_number": "66793", "files_changed": ["aten/src/ATen/cuda/detail/KernelUtils.h", "aten/src/ATen/native/cuda/Sort.cpp", "aten/src/ATen/native/cuda/Sort.cu", "aten/src/ATen/native/cuda/Sort.h", "aten/src/ATen/native/cuda/SortUtils.cuh", "caffe2/CMakeLists.txt"], "labels": ["open source", "Merged", "cla signed", "ciflow/default"]}, "4262c8913c": {"title": "Remove native_functions.yaml dependency from TensorTopK.cu (#66794)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/66794\n\nTest Plan: Imported from OSS\n\nReviewed By: bdhirsh\n\nDifferential Revision: D31856104\n\nPulled By: dagitses\n\nfbshipit-source-id: 2b9c0e1072455c5019c6f681faa3de848b3dae46", "pr_number": "66794", "files_changed": ["aten/src/ATen/native/cuda/TensorTopK.cpp", "aten/src/ATen/native/cuda/TensorTopK.cu", "aten/src/ATen/native/cuda/TensorTopK.h", "caffe2/CMakeLists.txt"], "labels": ["open source", "Merged", "cla signed", "ciflow/default"]}, "af1bd88fc4": {"title": "Allow scalars for aliased binary ops {`multiply`, `subtract`, `divide`} (#65937)", "body": "Summary:\nhttps://github.com/pytorch/pytorch/issues/65868 pointed out that the \"long-form\" versions of some binary ops like `mul`, `sub`, and `div` don't match their alias's behavior when it comes to handling scalar inputs. This PR adds the missing registration in `python_arg_parser.cpp` to resolve this.\n\nCC ptrblck ngimel\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65937\n\nReviewed By: malfet\n\nDifferential Revision: D32156580\n\nPulled By: ngimel\n\nfbshipit-source-id: b143cf7119a8bb51609e1b8734204edb750f0210", "pr_number": "65937", "files_changed": ["torch/csrc/utils/python_arg_parser.cpp", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["triaged", "open source", "Merged", "cla signed"]}, "7c739e1ab9": {"title": "Resubmit #67161 (#67735)", "body": "Summary:\nSkip building extensions if windows following https://github.com/pytorch/pytorch/pull/67161#issuecomment-958062611\n\nRelated issue: https://github.com/pytorch/pytorch/issues/67073\n\ncc ngimel xwang233 ptrblck\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/67735\n\nReviewed By: bdhirsh\n\nDifferential Revision: D32141250\n\nPulled By: ngimel\n\nfbshipit-source-id: 9bfdb7cf694c99f6fc8cbe9033a12429b6e4b6fe", "pr_number": "67735", "files_changed": ["aten/src/ATen/cuda/CUDABlas.cpp", "aten/src/ATen/cuda/CUDASolver.cpp", "aten/src/ATen/cuda/Exceptions.h", "test/cpp_extensions/cublas_extension.cpp", "test/cpp_extensions/cusolver_extension.cpp", "test/cpp_extensions/setup.py", "test/test_cpp_extensions_aot.py"], "labels": ["open source", "Merged", "cla signed", "ciflow/default", "ciflow/win"]}, "90d311b268": {"title": "[RPC] Add exception logging to constValue() (#67802)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/67802\n\nIn RPC C++ code, we might sometimes call constValue() when the future actually has an exception, and in unittests we want to assert on the exception. What happens is that we get a message basically saying \"!eptr_\" which indicates there is some exception but we don't know what it is.\n\nThis diff simply adds logging for the exception and mentions that `value` over `constValue` should be used when the future can have an exception. The contract of `constValue` to throw when `eptr_` is set is still held, it is just enhanced with additional logging.\nghstack-source-id: 142375391\n\nTest Plan: Added UT\n\nReviewed By: mrshenli\n\nDifferential Revision: D32156552\n\nfbshipit-source-id: 4dd5e73b92173209074c104a4b75c2021e20de4b", "pr_number": "67802", "files_changed": ["aten/src/ATen/core/ivalue_inl.h", "test/cpp/jit/test_misc.cpp"], "labels": ["oncall: jit", "Merged", "cla signed", "ciflow/default"]}, "279af1a668": {"title": "Revert D32154787: Formatted with Black", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD32154787 (https://github.com/pytorch/pytorch/commit/08d630b9a6c0fd4e07149feb335a8c50be903290)\n\nOriginal commit changeset: 6a95691c4ad9\n\nfbshipit-source-id: 2dbcf2395071433731683f685a0351fa8604d620", "pr_number": null, "files_changed": ["test/jit/test_freezing.py"], "labels": []}, "938bab0bfd": {"title": "[PyTorch] Add int version of vectorized PrefixSum to Benchmark (#67865)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/67865\n\n- Add int version of vectorized PrefixSum\n- Use unaligned load/store instructions\n- Add exclusive scan version. \"exclusive\" means that the i-th input element is not included in the i-th sum. For details see https://en.cppreference.com/w/cpp/algorithm/exclusive_scan\n\nTest Plan:\n```\nbuck build mode/opt-clang //caffe2/benchmarks/cpp/tensorexpr:tensorexpr_bench\nOMP_NUM_THREADS=1 numactl -m 0 -C 5 \\\n./buck-out/opt/gen/caffe2/benchmarks/cpp/tensorexpr/tensorexpr_bench --benchmark_filter=PrefixSumBench\n```\n\nFor full benchmark results, see P465274613\n\n```\nPrefixSumBench/LocalInt/64                            57 ns         56 ns   12414048 GB/s=9.06239G/s\nPrefixSumBench/LocalInt/256                          221 ns        221 ns    3160853 GB/s=9.28635G/s\nPrefixSumBench/LocalInt/1024                         818 ns        817 ns     857922 GB/s=10.0235G/s\nPrefixSumBench/LocalInt/4096                        3211 ns       3210 ns     217614 GB/s=10.2093G/s\nPrefixSumBench/LocalInt/16384                      12806 ns      12804 ns      54805 GB/s=10.2364G/s\nPrefixSumBench/LocalInt/65536                      51115 ns      51079 ns      13741 GB/s=10.2643G/s\nPrefixSumBench/LocalInt/262144                    205974 ns     205912 ns       3401 GB/s=10.1847G/s\nPrefixSumBench/LocalInt/1048576                   829523 ns     828859 ns        845 GB/s=10.1207G/s\nPrefixSumBench/LocalIntAVX2/64                        45 ns         45 ns   15568113 GB/s=11.3549G/s\nPrefixSumBench/LocalIntAVX2/256                      208 ns        208 ns    3371174 GB/s=9.86913G/s\nPrefixSumBench/LocalIntAVX2/1024                     893 ns        892 ns     783154 GB/s=9.18629G/s\nPrefixSumBench/LocalIntAVX2/4096                    3618 ns       3613 ns     193834 GB/s=9.06838G/s\nPrefixSumBench/LocalIntAVX2/16384                  14416 ns      14411 ns      48564 GB/s=9.09543G/s\nPrefixSumBench/LocalIntAVX2/65536                  57650 ns      57617 ns      12156 GB/s=9.09952G/s\nPrefixSumBench/LocalIntAVX2/262144                230855 ns     230612 ns       3035 GB/s=9.09386G/s\nPrefixSumBench/LocalIntAVX2/1048576               924265 ns     923777 ns        758 GB/s=9.08077G/s\nPrefixSumBench/LocalIntAVX512/64                      23 ns         23 ns   24876551 GB/s=22.0697G/s\nPrefixSumBench/LocalIntAVX512/256                     95 ns         95 ns    7387386 GB/s=21.556G/s\nPrefixSumBench/LocalIntAVX512/1024                   435 ns        435 ns    1609682 GB/s=18.8425G/s\nPrefixSumBench/LocalIntAVX512/4096                  1815 ns       1815 ns     385462 GB/s=18.0561G/s\nPrefixSumBench/LocalIntAVX512/16384                 7479 ns       7476 ns      93660 GB/s=17.5335G/s\nPrefixSumBench/LocalIntAVX512/65536                30171 ns      29879 ns      23430 GB/s=17.5468G/s\nPrefixSumBench/LocalIntAVX512/262144              125805 ns     125631 ns       5570 GB/s=16.6929G/s\nPrefixSumBench/LocalIntAVX512/1048576             504216 ns     503983 ns       1384 GB/s=16.6446G/s\nPrefixSumBench/ExclusiveScanIntAVX512/64              23 ns         23 ns   30058295\nPrefixSumBench/ExclusiveScanIntAVX512/256            101 ns        101 ns    7398498\nPrefixSumBench/ExclusiveScanIntAVX512/1024           435 ns        434 ns    1403877\nPrefixSumBench/ExclusiveScanIntAVX512/4096          1979 ns       1978 ns     354016\nPrefixSumBench/ExclusiveScanIntAVX512/16384         7828 ns       7819 ns      89551\nPrefixSumBench/ExclusiveScanIntAVX512/65536        31206 ns      31192 ns      22408\nPrefixSumBench/ExclusiveScanIntAVX512/262144      130106 ns     130023 ns       5388\nPrefixSumBench/ExclusiveScanIntAVX512/1048576     525515 ns     524976 ns       1244\n```\n\nReviewed By: navahgar, swolchok\n\nDifferential Revision: D32011740\n\nfbshipit-source-id: 7962de710bd588291dd6bf0c719f579c55f7c063", "pr_number": "67865", "files_changed": ["benchmarks/cpp/tensorexpr/bench_prefix_sum.cpp"], "labels": ["fb-exported", "Merged", "cla signed", "ciflow/default"]}, "8bed46ef38": {"title": "[WIP][LTC] Upstream class Shape (#67672)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/67672\n\nThis commit Upstreams class Shape from lazy_tensor_staging branch.\n\nTest Plan: WIP.\n\nReviewed By: malfet\n\nDifferential Revision: D32095478\n\nPulled By: alanwaketan\n\nfbshipit-source-id: 61611b12fc079b195833b5b22a6cf73c0935b8b9", "pr_number": "67672", "files_changed": ["test/cpp/lazy/CMakeLists.txt", "test/cpp/lazy/test_shape.cpp", "tools/build_variables.bzl", "torch/csrc/lazy/core/shape.cpp", "torch/csrc/lazy/core/shape.h"], "labels": ["Merged", "cla signed", "ciflow/default"]}, "c541c69e89": {"title": "Fix minor typo in contributing.md (#67855)", "body": "Summary:\nFixes #{issue number}\nNo issue number, minor change\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/67855\n\nReviewed By: malfet\n\nDifferential Revision: D32186689\n\nPulled By: driazati\n\nfbshipit-source-id: 7cda19f66ff1312296d8310922bb0d221df81e46", "pr_number": "67855", "files_changed": ["CONTRIBUTING.md"], "labels": ["open source", "Merged", "cla signed", "ciflow/default"]}, "56dda833ff": {"title": "Small updates to RELEASE.md (#65489)", "body": "Summary:\nCombine `xla` and `builder` branch pinning steps and link them to a PR that does it correctly\nUpdate example PR for version bump, as few files have changed\nDeleted FaceHub step as it is no longer necessary after recent update\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65489\n\nReviewed By: zhouzhuojie, seemethere\n\nDifferential Revision: D31120498\n\nPulled By: malfet\n\nfbshipit-source-id: e1a9db2b03243c8d28eeed9888c3653e4460ad07", "pr_number": "65489", "files_changed": ["RELEASE.md"], "labels": ["Merged", "cla signed", "ciflow/default"]}, "c2ceba8ada": {"title": "[PyTorchEdge] Move all serialize/deserialize files to a separate target (#66805)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66805\n\n{F672465642}\n\nDGW:\n```\nbuck query 'allpaths(//xplat/caffe2:torch_mobile_core, //xplat/caffe2:torch_mobile_interpreter)' --output-format dot_compact | pastry\nbunnylol dgw paste_id\n\n```\n\nTest Plan:\nbuck builds to pass\n\n```\nbuck build fbsource//fbandroid/mode/opt @//fbandroid/mode/messenger //fbandroid/apps/messenger:messenger_staticdi_dextr_splitarsc_dlstr_xzs_for_perftest_redex_optimizedtestableresources_postprocessed_resign //fbandroid/apps/messenger:messenger_staticdi_dextr_splitarsc_dlstr_xzs_for_perftest#unstripped_native_libraries\n\nbuck build //xplat/caffe2:torch_mobile_coreAndroid#android-armv7,shared\n\nbuck build //xplat/caffe2:torch_commonAndroid#android-armv7,shared\n\n```\n\nDGW:\n```\nbuck query 'allpaths(//xplat/caffe2/fb/runtime:only_flatbuffer_test, //xplat/caffe2:miniz)' --output-format dot_compact | pastry\nP464671429: https://www.internalfb.com/intern/paste/P464671429/\n\nbunnylol dgw P464671429\n```\n\nloader is decoupled from miniz\n\n```\nbuck query 'allpaths(//xplat/caffe2/fb/runtime:flatbuffer_loader, //xplat/caffe2:miniz)' --output-format dot_compactdigraph result_graph {\n}\n```\n\nReviewed By: iseeyuan\n\nDifferential Revision: D31532862\n\nfbshipit-source-id: 51e6880e78e1cafe20c8d90e98037edc3c1b6b11", "pr_number": "66805", "files_changed": ["tools/build_variables.bzl"], "labels": ["fb-exported", "Merged", "cla signed", "ciflow/default"]}, "f5daa9f76b": {"title": "[iOS] Enable ARC for CMake build (#67884)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/67884\n\nTest Plan: Imported from OSS\n\nReviewed By: husthyc\n\nDifferential Revision: D32191532\n\nPulled By: xta0\n\nfbshipit-source-id: a295004f8e7f1b0f5a4ab12ffd9b37c36b80226b", "pr_number": "67884", "files_changed": ["scripts/build_ios.sh"], "labels": ["Merged", "cla signed", "ciflow/default"]}, "4b084bc832": {"title": "Benchmarks for various fusers (#67622)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/67622\n\nTest Plan: Imported from OSS\n\nReviewed By: eellison\n\nDifferential Revision: D32171063\n\nPulled By: bertmaher\n\nfbshipit-source-id: 40d3a7adcc52aba3b051e382ec5ec4ee7e43d81b", "pr_number": "67622", "files_changed": ["benchmarks/fuser/plot_speedups.py", "benchmarks/fuser/run_benchmarks.py"], "labels": ["Merged", "cla signed", "ciflow/default"]}, "b1ac7f51a1": {"title": "Revert D32175957: Adding custom testing based on opinfos input for ops with custom rules.", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD32175957 (https://github.com/pytorch/pytorch/commit/b8e165e8417892a3cc1cd439603c3854ef2ee2de)\n\nOriginal commit changeset: 1cb51a7b6cbb\n\nfbshipit-source-id: 29fd0750d9981758436c55eea2de40cdaddfb9be", "pr_number": null, "files_changed": ["test/jit/test_dtype_analysis.py"], "labels": []}, "b098264f22": {"title": "Revert D32063662: [pytorch][PR] TST Adds device transfer into module info tests", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD32063662 (https://github.com/pytorch/pytorch/commit/da59bd1d131589268227ae2064ad90c43bb3b17d)\n\nOriginal commit changeset: 0868235a0ae7\n\nfbshipit-source-id: a4f775874faa88be0eb5272dedf3bbc8194ebde6", "pr_number": null, "files_changed": ["test/test_modules.py"], "labels": []}, "53ebccbe78": {"title": "Fix warnings produced when running test_optim.py (#67756)", "body": "Summary:\nFixes part of https://github.com/pytorch/pytorch/issues/67696 by adding calls to `optimizer.step()` in various places.\n\n## Notes for reviewers:\n- It is not entirely clear which is the right optimizer to step in each case. I have favoured the more explicit approach of creating a set of optimizers and calling step on each of them.\n- At the time of writing, the only Scheduler without an `optimizer` instance variable is `ChainedScheduler` which I need to deal with once. I use `hasattr` to do this check. Let me know if this ought to be changed.\n- I am opening this PR for review when it only solve part of the issue, as I'd rather get feedback sooner. I think it is fine to fix the issue in several PRs too.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/67756\n\nReviewed By: jbschlosser\n\nDifferential Revision: D32187864\n\nPulled By: albanD\n\nfbshipit-source-id: fd0d133bcaa3a24588e5a997ad198fdf5879ff5a", "pr_number": "67756", "files_changed": ["test/test_optim.py"], "labels": ["open source", "Merged", "cla signed", "ciflow/default"]}, "07a08fb95f": {"title": "Fix typo in LinearLR docs (#67840)", "body": "Summary:\nThe final learning rate should be 0.05 like the lr used as the argument for the optimizer and not 0.005.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/67840\n\nReviewed By: jbschlosser\n\nDifferential Revision: D32187091\n\nPulled By: albanD\n\nfbshipit-source-id: 8aff691bba3896a847d7b9d9d669a65f67a6f066", "pr_number": "67840", "files_changed": ["torch/optim/lr_scheduler.py"], "labels": ["open source", "Merged", "cla signed", "ciflow/default"]}, "823ae3a4ff": {"title": "[forward ad] Also check layout of grad matches that of self for inplace over view (#67816)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/67800\n\nCurrently when the grad is the same layout as base, we try to assign the same tensor to the forward grad of both the base and the view. However, when the layout of the grad is different from the layout of the view, this triggers a copy to be created, and the tangent of the view (after the inplace) will not have a view relationship with the view of the base.\n\nThis PR just changes it so that we only do the above optimization when the layout also matches the layout of self\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/67816\n\nReviewed By: malfet\n\nDifferential Revision: D32190021\n\nPulled By: soulitzer\n\nfbshipit-source-id: b1b2c9b332e83f4df5695ee9686ea76447f9305b", "pr_number": "67816", "files_changed": ["test/test_autograd.py", "torch/csrc/autograd/autograd_meta.cpp"], "labels": ["Merged", "cla signed", "ciflow/default"]}, "9dafb6434b": {"title": "remove use of THGenerateAllTypes, clean up (#67867)", "body": "Summary:\nPer title\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/67867\n\nReviewed By: mruberry\n\nDifferential Revision: D32191053\n\nPulled By: ngimel\n\nfbshipit-source-id: 84eb6c2989495fca5f7b055c4984efe5de94e812", "pr_number": "67867", "files_changed": ["aten/src/TH/CMakeLists.txt", "aten/src/TH/THGeneral.cpp", "aten/src/TH/THGeneral.h.in", "aten/src/TH/THTensor.cpp", "aten/src/TH/THTensor.h", "aten/src/TH/THTensor.hpp", "aten/src/TH/generic/THStorage.cpp", "aten/src/TH/generic/THStorage.h", "aten/src/TH/generic/THTensor.cpp", "aten/src/TH/generic/THTensor.h", "aten/src/TH/generic/THTensor.hpp", "torch/csrc/generic/StorageMethods.cpp", "torch/csrc/utils.h"], "labels": ["Merged", "cla signed", "ciflow/default"]}, "cdd5d16489": {"title": "[Foreach] Implement L1&L2 norm (#62646)", "body": "Summary:\nImplement L1 & L2 norm in fast path with the reference of [nvidia/apex](https://github.com/NVIDIA/apex/blob/master/csrc/multi_tensor_l2norm_kernel.cu).\nWhen `ord` is neither 1 nor 2, then slow path is chosen.\n\nRelated: https://github.com/pytorch/pytorch/issues/58833\n\ncc ptrblck mcarilli ngimel\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/62646\n\nReviewed By: malfet\n\nDifferential Revision: D32173421\n\nPulled By: ngimel\n\nfbshipit-source-id: 14b7544601658a979b83509df351e1848ded7675", "pr_number": "62646", "files_changed": ["aten/src/ATen/native/ForeachOpsKernels.cpp", "aten/src/ATen/native/cuda/ForeachReduceOp.cu", "aten/src/ATen/native/cuda/MultiTensorApply.cuh", "aten/src/ATen/native/cuda/block_reduce.cuh", "aten/src/ATen/native/native_functions.yaml", "test/test_foreach.py", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["triaged", "open source", "Merged", "cla signed", "ciflow/default"]}, "240e8d5cc5": {"title": "Updated searchsorted functionality (#66818)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/60492\n\nUpdates searchsorted API to be more consistent with numpy and adds an OpInfo for searchsorted\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66818\n\nReviewed By: mruberry\n\nDifferential Revision: D31745142\n\nPulled By: samdow\n\nfbshipit-source-id: 0b9600afc3cb0720afb5811212404ee96d2a7d93", "pr_number": "66818", "files_changed": ["aten/src/ATen/autocast_mode.cpp", "aten/src/ATen/native/Bucketization.cpp", "aten/src/ATen/native/BucketizationUtils.h", "aten/src/ATen/native/cuda/Bucketization.cu", "aten/src/ATen/native/native_functions.yaml", "test/test_reductions.py", "torch/_torch_docs.py", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "cla signed", "ciflow/default", "ciflow/all"]}, "9e8016d8c4": {"title": "Revert D31932215: [pytorch][PR] Don't #define NUM_THREADS", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD31932215 (https://github.com/pytorch/pytorch/commit/f70e8064f4e6266ab945bc24345c622dc7933d0e)\n\nOriginal commit changeset: ccdf11e249fb\n\nfbshipit-source-id: 4c330aebe9cfb483f02ceb1fdaf5c3b0f8fa6fa1", "pr_number": null, "files_changed": ["aten/src/ATen/native/cuda/CUDALoops.cuh", "aten/src/ATen/native/cuda/CrossKernel.cu", "aten/src/ATen/native/cuda/DistributionTemplates.h", "aten/src/ATen/native/cuda/FunctionOfAMatrixUtilsKernel.cu", "aten/src/ATen/native/cuda/LinearAlgebra.cu", "aten/src/ATen/native/cuda/Loops.cuh", "aten/src/ATen/native/cuda/MemoryAccess.cuh", "aten/src/ATen/native/cuda/ROCmLoops.cuh", "aten/src/ATen/native/cuda/ScatterGatherKernel.cu", "aten/src/ATen/native/cuda/UnfoldBackwardKernel.cu", "aten/src/ATen/test/cuda_vectorized_test.cu"], "labels": []}, "efdb17b984": {"title": "Add meta support to tensor range factories (#67032)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/67032\n\nThis PR adds meta backend support to the `range`, `arange`, `linspace`, and `logspace` operators.\n\nNote that the original PR (#66630) was reverted due to two failing unit tests in the Bionic CI. This revision includes a fix for those tests; otherwise its content is identical to the previous PR.\n\nOriginal commit changeset: 2f9d8d1acbb0\nghstack-source-id: 142487306\n\nTest Plan: Extended the existing tensor creation tests to assert meta backend support.\n\nReviewed By: zhaojuanmao\n\nDifferential Revision: D31834403\n\nfbshipit-source-id: a489858a2a8a38a03234b14408e14d2b208a8d34", "pr_number": "67032", "files_changed": ["aten/src/ATen/native/Histogram.cpp", "aten/src/ATen/native/RangeFactories.cpp", "aten/src/ATen/native/native_functions.yaml", "test/test_indexing.py", "test/test_tensor_creation_ops.py", "torch/testing/_core.py"], "labels": ["Merged", "cla signed", "ciflow/default"]}, "9cacf2b718": {"title": "Add custom zipper script to zip python modules for torch.deploy (#67006)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/67006\n\nTest Plan: nervouslaugh_\n\nReviewed By: shunting314\n\nDifferential Revision: D31822429\n\nfbshipit-source-id: c2efeab1446fbeb70b98d4ee766fbc670cf091b0", "pr_number": "67006", "files_changed": ["torch/utils/_zip.py"], "labels": ["fb-exported", "Merged", "cla signed", "ciflow/default"]}, "0dc99dcf59": {"title": "Update __init__.py (#67900)", "body": "Summary:\nfix bugs https://github.com/pytorch/pytorch/issues/67896\nfix a syntax error in pytorch/torch/cuda/__init__.py\nFixes https://github.com/pytorch/pytorch/issues/67896\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/67900\n\nReviewed By: mruberry\n\nDifferential Revision: D32211978\n\nPulled By: soulitzer\n\nfbshipit-source-id: a313a5e23b4d79e5b7bb909eaf82c9ee6cab10c9", "pr_number": "67900", "files_changed": ["torch/cuda/__init__.py"], "labels": ["triaged", "open source", "Merged", "cla signed", "ciflow/default"]}, "b3770766c4": {"title": "Fixes deprecation warnings in `test_optim.py` (#67954)", "body": "Summary:\nCatches deprecation warnings when we call `scheduler.step(epoch)`\nin tests.\n\nRemoves duplicate parameters to optimizers unless we are specifically\ntesting for that\n\nFixes https://github.com/pytorch/pytorch/issues/67696\n\nThere is one warning remaining when I run this locally -- however that is due to the implementation of the `SequentialLR` Scheduler. I will open a new issue relating to that.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/67954\n\nReviewed By: H-Huang\n\nDifferential Revision: D32244056\n\nPulled By: albanD\n\nfbshipit-source-id: 2ab3086a58e10c8d29809ccbaab80606a1ec61d8", "pr_number": "67954", "files_changed": ["test/test_optim.py"], "labels": ["triaged", "open source", "Merged", "cla signed", "ciflow/default"]}, "5bb5bfccf7": {"title": "[lint] add lintrunner support for circleci_linter (#67872)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/67872\n\nAs title. This demonstrates some of the nice features of lintrunner:\n- Uniform error reporting means you get a nice diff of the changes for\nfree\n- Can run with -a to just accept the changes (don't need to tell people\nto run a special regenerate command since the linter adaper already knows how.)\n\nDifferential Revision:\nD32187386\nD32187386\n\nTest Plan: Imported from OSS\n\nReviewed By: driazati\n\nPulled By: suo\n\nfbshipit-source-id: 71de6b042730be80ff6794652039e9bc655a72b1", "pr_number": "67872", "files_changed": [".lintrunner.toml", "tools/linter/adapters/circleci_linter.py", "tools/linter/adapters/clangformat_linter.py", "tools/linter/adapters/clangtidy_linter.py", "tools/linter/adapters/flake8_linter.py", "tools/linter/adapters/grep_linter.py", "tools/linter/adapters/mypy_linter.py"], "labels": ["Merged", "cla signed", "ciflow/default"]}, "4b021280ad": {"title": "[lint] add nativefunctions to lintrunner (#67890)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/67890\n\nAdding another linter. I also added a generic initializer that installs\nthe right pip packages (you can invoke it by running `lintrunner init`).\n\nDifferential Revision:\nD32197366\nD32197366\n\nTest Plan: Imported from OSS\n\nReviewed By: driazati\n\nPulled By: suo\n\nfbshipit-source-id: 82844e78f1ee3047220d8444874eab41d7cc0e9e", "pr_number": "67890", "files_changed": [".lintrunner.toml", "requirements-flake8.txt", "tools/linter/adapters/nativefunctions_linter.py", "tools/linter/adapters/pip_init.py"], "labels": ["Merged", "cla signed", "ciflow/default"]}, "419c58ea9c": {"title": "[lint] add newlines linter to lintrunner (#67894)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/67894\n\nAs title. Confirmed that the code base passes by running:\n\n```\nlintrunner --paths-cmd='git grep -Il \"\"' --take NEWLINE\n```\n\nand seeing that it pases\n\nTest Plan: Imported from OSS\n\nReviewed By: H-Huang\n\nDifferential Revision: D32250604\n\nPulled By: suo\n\nfbshipit-source-id: de9bcba635d21f8832bb25147b19b7b2e8802247", "pr_number": "67894", "files_changed": [".lintrunner.toml", "tools/linter/adapters/newlines_linter.py"], "labels": ["Merged", "cla signed", "ciflow/default"]}, "53f118c800": {"title": "[lint] improve mypy lintrunner config (#67936)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/67936\n\n- Add the strict config\n- Make the patterns exactly match the current CI\n- Add init_args\n\nTest Plan: Imported from OSS\n\nReviewed By: H-Huang\n\nDifferential Revision: D32250605\n\nPulled By: suo\n\nfbshipit-source-id: a71d434bf6024db4462260a460a1bc2d9ac66a32", "pr_number": "67936", "files_changed": [".lintrunner.toml", "tools/linter/adapters/mypy_linter.py"], "labels": ["Merged", "cla signed", "ciflow/default"]}, "d201102d36": {"title": "[lint] Add the rest of the grep linters (#67932)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/67932\n\nAlso various improvements to grep_linter.py, including the ability to\nspecify a replacement pattern.\n\nTest Plan: Imported from OSS\n\nReviewed By: H-Huang\n\nDifferential Revision: D32250603\n\nPulled By: suo\n\nfbshipit-source-id: e07eb182e9473a268e2b805a68a859b91228bfbb", "pr_number": "67932", "files_changed": [".lintrunner.toml", "tools/linter/adapters/grep_linter.py"], "labels": ["Merged", "cla signed", "ciflow/default"]}, "8e2528132b": {"title": "[lint] small updates to .lintrunner.toml (#67942)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/67942\n\n- Change \"name\" to \"code\" for consistency with linttool and LintMessage\nformat.\n- Change \"args\" and \"init_args\" to \"command\" and \"init_command\" for\nconsistency with internal representation.\n\nTest Plan: Imported from OSS\n\nReviewed By: H-Huang\n\nDifferential Revision: D32250606\n\nPulled By: suo\n\nfbshipit-source-id: 557fef731bab9adca7ab1e7cc41b996956076b05", "pr_number": "67942", "files_changed": [".lintrunner.toml", "tools/linter/adapters/circleci_linter.py", "tools/linter/adapters/clangformat_linter.py", "tools/linter/adapters/clangtidy_linter.py", "tools/linter/adapters/flake8_linter.py", "tools/linter/adapters/grep_linter.py", "tools/linter/adapters/mypy_linter.py", "tools/linter/adapters/nativefunctions_linter.py", "tools/linter/adapters/newlines_linter.py"], "labels": ["Merged", "cla signed", "ciflow/default"]}, "db456d16ee": {"title": "`torch.lobpcg.backward`: do not save non-Variable types with `ctx.save_for_backward`. (#67994)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/67827\n\ncc ezyang albanD zou3519 gqchen pearu nikitaved soulitzer Lezcano Varal7\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/67994\n\nReviewed By: H-Huang\n\nDifferential Revision: D32244818\n\nPulled By: albanD\n\nfbshipit-source-id: 702a3a1d1f4c160bef7ec1f764a2ab5d01ca7901", "pr_number": "67994", "files_changed": ["test/test_autograd.py", "torch/_lobpcg.py"], "labels": ["module: autograd", "open source", "Merged", "cla signed", "ciflow/slow-gradcheck", "ciflow/slow", "ciflow/default"]}, "fd198a2fea": {"title": "[fx2trt] fix import in oss tests (#68016)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/68016\n\nWe would want to use oss test utils.\n\nAlso refactor both test utils so that the internal one is an enhancement over the oss test utils.\n\nTest Plan: CI\n\nReviewed By: wushirong\n\nDifferential Revision: D32250266\n\nfbshipit-source-id: 968b8f215ca2d294f7d0bd13cf9563be567954dd", "pr_number": "68016", "files_changed": ["test/fx2trt/converters/acc_op/test_adaptive_avgpool.py", "test/fx2trt/converters/acc_op/test_avgpool.py", "test/fx2trt/converters/acc_op/test_batchnorm.py", "test/fx2trt/converters/acc_op/test_binary_ops.py", "test/fx2trt/converters/acc_op/test_cat.py", "test/fx2trt/converters/acc_op/test_chunk.py", "test/fx2trt/converters/acc_op/test_clamp.py", "test/fx2trt/converters/acc_op/test_convolution.py", "test/fx2trt/converters/acc_op/test_dequantize.py", "test/fx2trt/converters/acc_op/test_flatten.py", "test/fx2trt/converters/acc_op/test_gelu.py", "test/fx2trt/converters/acc_op/test_getitem.py", "test/fx2trt/converters/acc_op/test_layer_norm.py", "test/fx2trt/converters/acc_op/test_layernorm.py", "test/fx2trt/converters/acc_op/test_linear.py", "test/fx2trt/converters/acc_op/test_matmul.py", "test/fx2trt/converters/acc_op/test_max.py", "test/fx2trt/converters/acc_op/test_maximum.py", "test/fx2trt/converters/acc_op/test_maxpool.py", "test/fx2trt/converters/acc_op/test_min.py", "test/fx2trt/converters/acc_op/test_minimum.py", "test/fx2trt/converters/acc_op/test_narrow.py", "test/fx2trt/converters/acc_op/test_pad.py", "test/fx2trt/converters/acc_op/test_permute.py", "test/fx2trt/converters/acc_op/test_quantize_per_tensor.py", "test/fx2trt/converters/acc_op/test_relu.py", "test/fx2trt/converters/acc_op/test_reshape.py", "test/fx2trt/converters/acc_op/test_sigmoid.py", "test/fx2trt/converters/acc_op/test_size.py", "test/fx2trt/converters/acc_op/test_softmax.py", "test/fx2trt/converters/acc_op/test_split.py", "test/fx2trt/converters/acc_op/test_squeeze.py", "test/fx2trt/converters/acc_op/test_sum.py", "test/fx2trt/converters/acc_op/test_tanh.py", "test/fx2trt/converters/acc_op/test_tile.py", "test/fx2trt/converters/acc_op/test_topk.py", "test/fx2trt/converters/acc_op/test_unary_ops.py", "test/fx2trt/converters/acc_op/test_unsqueeze.py", "test/fx2trt/converters/vanilla/test_add.py", "test/fx2trt/converters/vanilla/test_convolution.py", "test/fx2trt/core/test_trt_module.py", "torch/testing/_internal/common_fx2trt.py"], "labels": ["fb-exported", "Merged", "cla signed", "ciflow/default", "fx"]}, "a2ab06514b": {"title": "Fixes CUDA vs CPU consistency for index_put_ when accumulating (part 2) (#67189)", "body": "Summary:\nDescription:\n- Follow up PR to https://github.com/pytorch/pytorch/issues/66790 to fix the tests on functorch, https://github.com/pytorch/functorch/issues/195\n\nIn functorch, a null tensor is added to the list of indices for the batch dimension in C++, but I can not find an equivalent of that in python without using `torch.jit.script`. If any other better solutions could be suggested, I'd be happy to replace the current way of testing.\n\ncc ngimel zou3519\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/67189\n\nReviewed By: suo\n\nDifferential Revision: D31966686\n\nPulled By: ngimel\n\nfbshipit-source-id: a14b9e5d77d9f43cd728d474e2976d84a87a6ff4", "pr_number": "67189", "files_changed": ["aten/src/ATen/native/cuda/Indexing.cu", "test/test_indexing.py"], "labels": ["open source", "Merged", "cla signed", "ciflow/default"]}, "4b1d044498": {"title": "[WIP][resubmit] Don't #define NUM_THREADS (#68008)", "body": "Summary:\nThis reverts commit 9e8016d8c48e9c99addad93112f99d3375a0fbc7.\n\ncc jeffdaily sunway513 jithunnair-amd ROCmSupport KyleCZH\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/68008\n\nReviewed By: H-Huang\n\nDifferential Revision: D32254779\n\nPulled By: ngimel\n\nfbshipit-source-id: 38ec415199f62a1e58000abe3e34ac91898a94ae", "pr_number": "68008", "files_changed": ["aten/src/ATen/native/cuda/CUDALoops.cuh", "aten/src/ATen/native/cuda/CrossKernel.cu", "aten/src/ATen/native/cuda/DistributionTemplates.h", "aten/src/ATen/native/cuda/FunctionOfAMatrixUtilsKernel.cu", "aten/src/ATen/native/cuda/LinearAlgebra.cu", "aten/src/ATen/native/cuda/Loops.cuh", "aten/src/ATen/native/cuda/MemoryAccess.cuh", "aten/src/ATen/native/cuda/ROCmLoops.cuh", "aten/src/ATen/native/cuda/ScatterGatherKernel.cu", "aten/src/ATen/native/cuda/UnfoldBackwardKernel.cu", "aten/src/ATen/test/cuda_vectorized_test.cu"], "labels": ["module: rocm", "open source", "Merged", "cla signed", "ciflow/default", "ciflow/cuda"]}, "417dc7f86c": {"title": "Revert D32007691: [pytorch][PR] Op info for activation functions 2 (softsign, tanh, tanhshrink, threshold, celu, sigmoid, mish, hardsigmoid)", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD32007691 (https://github.com/pytorch/pytorch/commit/ea60e7d5597e4bb4df0db8ab97cf7eca75969d56)\n\nOriginal commit changeset: 6cb14dc56e29\n\nfbshipit-source-id: 9ef599ef07302fb521b1f413b989786adfa3576c", "pr_number": null, "files_changed": ["test/test_jit_fuser_te.py", "torch/testing/_internal/common_methods_invocations.py"], "labels": []}, "c5e5264be2": {"title": "Disable TF32 in `pinv_jvp` and `pinv_backward` (#67948)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/67947\n\ncc ptrblck xwang233 zasdfgbnm\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/67948\n\nReviewed By: H-Huang\n\nDifferential Revision: D32251934\n\nPulled By: ngimel\n\nfbshipit-source-id: a2b1a118337b38db61350c9e49f1ba19030d70ec", "pr_number": "67948", "files_changed": ["torch/csrc/autograd/FunctionsManual.cpp"], "labels": ["open source", "Merged", "cla signed", "ciflow/default"]}, "f9422e1c6b": {"title": "Fix deadlock for multi-output forward AD (#67995)", "body": "Summary:\nWill hide some of the issues from https://github.com/pytorch/pytorch/issues/67367\nThis will at least allow us to run gradcheck for now until the above issue is fixed.\n\nFor more context, the deadlock happens when we (wrongfully) set a forward grad that also has a forward grad of the same level.\nIn particular, when exiting the level from https://github.com/pytorch/pytorch/blob/191b48b12f33e1e9525882da0c62b68686d69e42/torch/csrc/autograd/forward_grad.cpp#L23\nWe are taking the `all_forward_levels_mutex_` lock and proceed to delete the level at https://github.com/pytorch/pytorch/blob/191b48b12f33e1e9525882da0c62b68686d69e42/torch/csrc/autograd/forward_grad.cpp#L29 (nothing else usually references this object, so it gets deleted as soon as it gets removed from the vector). Note that, at this point, we still have the lock!\n\nIn the level destructor in https://github.com/pytorch/pytorch/blob/191b48b12f33e1e9525882da0c62b68686d69e42/torch/csrc/autograd/forward_grad.cpp#L55 we are deleting the forward grad. Which triggers the deletion the grad Tensor and everything it holds (assuming nothing else references it).\nBut in the (bad) case where this Tensor also has a forward grad for this level, the autograd meta clears the fw grads: https://github.com/pytorch/pytorch/blob/191b48b12f33e1e9525882da0c62b68686d69e42/torch/csrc/autograd/forward_grad.h#L124\nWhile clearing, we access the level (to de-register this forward grad) via https://github.com/pytorch/pytorch/blob/191b48b12f33e1e9525882da0c62b68686d69e42/torch/csrc/autograd/forward_grad.h#L139\nBut this tries to access the level again in https://github.com/pytorch/pytorch/blob/191b48b12f33e1e9525882da0c62b68686d69e42/torch/csrc/autograd/forward_grad.cpp#L39 and deadlocks.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/67995\n\nReviewed By: soulitzer\n\nDifferential Revision: D32250996\n\nPulled By: albanD\n\nfbshipit-source-id: f6118117effd3114fa90dc8fe22865339445f70c", "pr_number": "67995", "files_changed": ["torch/csrc/autograd/forward_grad.cpp", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "cla signed", "ciflow/default"]}, "577a4d34a7": {"title": "making import_module private and deprecating public method (#67990)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/67990\n\nDuplicate of the following PR which was merged by mistake without ghimport\nhttps://github.com/pytorch/pytorch/pull/67914\n\ncc albanD NicolasHug\n\nTest Plan: Imported from OSS\n\nReviewed By: H-Huang\n\nDifferential Revision: D32247560\n\nPulled By: jdsgomes\n\nfbshipit-source-id: 8ba5ba7d17fc3d0d2c377da467ea805822e21ec1", "pr_number": "67990", "files_changed": ["torch/hub.py"], "labels": ["Merged", "cla signed", "ciflow/default"]}, "114ef8c5ea": {"title": "Add SiLU backward Aten symbol (#67665)", "body": "Summary:\nThis is related to https://github.com/pytorch/xla/issues/3192. bdhirsh\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/67665\n\nReviewed By: desertfire\n\nDifferential Revision: D32245736\n\nPulled By: bdhirsh\n\nfbshipit-source-id: c5a2b24214fa37a181246cbbfcbee131473cf807", "pr_number": "67665", "files_changed": ["aten/src/ATen/core/aten_interned_strings.h"], "labels": ["triaged", "open source", "Merged", "cla signed", "ciflow/default"]}, "96b4f2296e": {"title": "CppSignature: Compare types by their mangled names (#67987)", "body": "Summary:\n`.name()` has to call `__cxa_demangle` and allocate a new string, both of which can be avoided by just comparing the mangled names directly.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/67987\n\nReviewed By: mruberry\n\nDifferential Revision: D32264560\n\nPulled By: H-Huang\n\nfbshipit-source-id: 9dd4388ba4e2648c92e4062dafe6d8dc3ea6484e", "pr_number": "67987", "files_changed": ["aten/src/ATen/core/dispatch/CppSignature.h"], "labels": ["open source", "Merged", "cla signed", "ci/master", "ciflow/default"]}, "acb035f513": {"title": "Revert D31609714: Fix Dispatching not considering List[Optional[Tensor]] for dispatch", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD31609714 (https://github.com/pytorch/pytorch/commit/c581f56c74d3ce369885ee6221c486c39e7cb77f)\n\nOriginal commit changeset: bb91cafd32fb\n\nfbshipit-source-id: a04055e7af4bf8491b44bbc3e3bddc7831ab205e", "pr_number": null, "files_changed": ["aten/src/ATen/core/dispatch/DispatchKeyExtractor.h", "test/test_python_dispatch.py"], "labels": []}, "a473417076": {"title": "[LT] Merge permutation_util into master (#67766)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/67766\n\nTest Plan: `build/bin/test_lazy`\n\nReviewed By: wconstab\n\nDifferential Revision: D32147676\n\nPulled By: desertfire\n\nfbshipit-source-id: 528b48c9cf789abc171235091c7146b2ab7a9c76", "pr_number": "67766", "files_changed": ["test/cpp/lazy/CMakeLists.txt", "test/cpp/lazy/test_permutation_util.cpp", "tools/build_variables.bzl", "torch/csrc/lazy/core/permutation_util.cpp", "torch/csrc/lazy/core/permutation_util.h"], "labels": ["Merged", "cla signed", "ciflow/default"]}, "a027551358": {"title": "[LT] Merge cache.h (#67929)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/67929\n\n1. Write a node-hash based unit test for Cache\n2. Replace CHECK with TORCH_CHECK in IrUtil\n\nTest Plan: Imported from OSS\n\nReviewed By: H-Huang\n\nDifferential Revision: D32246134\n\nPulled By: desertfire\n\nfbshipit-source-id: c464bc300126d47e9ad4af3b3e8484a389757dc0", "pr_number": "67929", "files_changed": ["test/cpp/lazy/CMakeLists.txt", "test/cpp/lazy/test_cache.cpp", "test/cpp/lazy/test_ir.cpp", "test/cpp/lazy/test_ir_util.cpp", "torch/csrc/lazy/core/cache.h", "torch/csrc/lazy/core/ir_util.cpp"], "labels": ["Merged", "cla signed", "ciflow/default"]}, "726e2ed715": {"title": "[lint] add more lints to lintrunner (#68069)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/68069\n\n- executable bit\n- cub include\n- raw CUDA API usage\n\nTest Plan: Imported from OSS\n\nReviewed By: janeyx99\n\nDifferential Revision: D32286559\n\nPulled By: suo\n\nfbshipit-source-id: 21d58e259c951424f9c6cbf1dac6d79fe7236aa4", "pr_number": "68069", "files_changed": [".lintrunner.toml", "tools/linter/adapters/exec_linter.py"], "labels": ["Merged", "cla signed", "ciflow/default"]}, "273f7ae9b3": {"title": "fx: Update fx.rst (#68043)", "body": "Summary:\nWhen I run this part of the code on the document with PyTorch version 1.10.0, I found some differences between the output and the document, as follows:\n\n```python\nimport torch\nimport torch.fx as fx\n\nclass M(torch.nn.Module):\n    def forward(self, x, y):\n        return x + y\n\n# Create an instance of `M`\nm = M()\n\ntraced = fx.symbolic_trace(m)\nprint(traced)\nprint(traced.graph)\ntraced.graph.print_tabular()\n```\n\nI get the result\uff1a\n\n```shell\ndef forward(self, x, y):\n    add = x + y;  x = y = None\n    return add\n\ngraph():\n    %x : [#users=1] = placeholder[target=x]\n    %y : [#users=1] = placeholder[target=y]\n    %add : [#users=1] = call_function[target=operator.add](args = (%x, %y), kwargs = {})\n    return add\nopcode         name    target                   args    kwargs\n-------------  ------  -----------------------  ------  --------\nplaceholder    x       x                        ()      {}\nplaceholder    y       y                        ()      {}\ncall_function  add     <built-in function add>  (x, y)  {}\noutput         output  output                   (add,)  {}\n```\n\nThis pr modified the document\u3002\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/68043\n\nReviewed By: driazati\n\nDifferential Revision: D32287178\n\nPulled By: jamesr66a\n\nfbshipit-source-id: 48ebd0e6c09940be9950cd57ba0c03274a849be5", "pr_number": "68043", "files_changed": ["docs/source/fx.rst"], "labels": ["triaged", "open source", "Merged", "cla signed", "ciflow/default"]}, "be4150139a": {"title": "bugfix for conditional functionalization (#67715)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/67715\n\nI had original made the `vector<ViewMeta>` and `Tensor`s stored on the `Update` struct references, but will pointed out a bug in the conditional-functionalization PR due to a use-after-free error. This happens because the queued-up updates might not be synced until later, and can out-live the original tensor that was used to create them.\n\nIt was kind of strange that this doesn't show up in the existing `test/test_functionalization.py` tests that I have in this stack, which technically also should have this bug (they call sync_() after the mutated tensors have gone out of scope). I looked at it with gdb, and I'm wondering if it's just because the stored values in the free'd `ViewMeta`/`Tensor` just happen to not get clobbered by the time the sync is called in the test.\n\nEither way, copying the Tensor + vector<ViewMeta> is probably not ideal for performance, but I couldn't think of an easy work-around for now.\n\nTest Plan: Imported from OSS\n\nReviewed By: malfet\n\nDifferential Revision: D32136007\n\nPulled By: bdhirsh\n\nfbshipit-source-id: 707c6392a31b967e8965b9b77f297fd10a0a095a", "pr_number": "67715", "files_changed": ["aten/src/ATen/FunctionalStorageImpl.h"], "labels": ["Merged", "cla signed", "ciflow/default"]}, "fe46d6c68f": {"title": "functionalization: map copy_() -> to().expand_as() (#67878)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/67878\n\nThe functionalization pass doesn't work with `copy_()` which is a problem with functorch. Originally we were going to make a functional `copy()` operator to fix this problem, but zou3519 that we can get (most of) the existing functionality by mapping `self.copy_(src)` to `src.to(self).expand_as(self)`. This makes the codegen a bit uglier, but has the benefit of avoiding a totally unnecessary tensor allocation in functorch.\n\nTest Plan: Imported from OSS\n\nReviewed By: zou3519\n\nDifferential Revision: D32280588\n\nPulled By: bdhirsh\n\nfbshipit-source-id: 2c6ee65f0929e0846566987183ba2498c88496c2", "pr_number": "67878", "files_changed": ["tools/codegen/gen_functionalization_type.py"], "labels": ["Merged", "cla signed", "ciflow/default"]}, "790763b0fe": {"title": "Add an option to disable reduced precision reductions for FP16 GEMM (#67946)", "body": "Summary:\nhttps://github.com/pytorch/pytorch/issues/67578 disabled reduced precision reductions for FP16 GEMMs. After benchmarking, we've found that this has substantial performance impacts for common GEMM shapes (e.g., those found in popular instantiations of multiheaded-attention) on architectures such as Volta. As these performance regressions may come as a surprise to current users, this PR adds a toggle to disable reduced precision reductions\n`torch.backends.cuda.matmul.allow_fp16_reduced_precision_reduction = `\nrather than making it the default behavior.\n\nCC ngimel ptrblck\nstas00 Note that the behavior after the previous PR can be replicated with\n`torch.backends.cuda.matmul.allow_fp16_reduced_precision_reduction = False`\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/67946\n\nReviewed By: zou3519\n\nDifferential Revision: D32289896\n\nPulled By: ngimel\n\nfbshipit-source-id: a1ea2918b77e27a7d9b391e030417802a0174abe", "pr_number": "67946", "files_changed": ["aten/src/ATen/Context.cpp", "aten/src/ATen/Context.h", "aten/src/ATen/cuda/CUDABlas.cpp", "docs/source/backends.rst", "docs/source/notes/cuda.rst", "docs/source/notes/numerical_accuracy.rst", "test/test_cuda.py", "torch/_C/__init__.pyi.in", "torch/backends/cuda/__init__.py", "torch/csrc/Module.cpp"], "labels": ["triaged", "open source", "Merged", "cla signed", "ciflow/default"]}, "91af74c934": {"title": "remove Generate* macro files (#67940)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/67940\n\nReviewed By: mruberry\n\nDifferential Revision: D32250987\n\nPulled By: ngimel\n\nfbshipit-source-id: 3feb0bc876bc26d0a42784e5c6001670ed71e971", "pr_number": "67940", "files_changed": ["aten/src/TH/CMakeLists.txt", "aten/src/TH/THGeneral.h.in", "aten/src/TH/THGenerateAllTypes.h", "aten/src/TH/THGenerateBFloat16Type.h", "aten/src/TH/THGenerateBoolType.h", "aten/src/TH/THGenerateCharType.h", "aten/src/TH/THGenerateComplexDoubleType.h", "aten/src/TH/THGenerateComplexFloatType.h", "aten/src/TH/THGenerateComplexTypes.h", "aten/src/TH/THGenerateDoubleType.h", "aten/src/TH/THGenerateFloatType.h", "aten/src/TH/THGenerateFloatTypes.h", "aten/src/TH/THGenerateHalfType.h", "aten/src/TH/THGenerateIntType.h", "aten/src/TH/THGenerateIntTypes.h", "aten/src/TH/THGenerateLongType.h", "aten/src/TH/THGenerateQInt32Type.h", "aten/src/TH/THGenerateQInt8Type.h", "aten/src/TH/THGenerateQTypes.h", "aten/src/TH/THGenerateQUInt2x4Type.h", "aten/src/TH/THGenerateQUInt4x2Type.h", "aten/src/TH/THGenerateQUInt8Type.h", "aten/src/TH/THGenerateShortType.h", "aten/src/TH/THTensor.hpp", "aten/src/THC/CMakeLists.txt", "aten/src/THC/THCGenerateAllTypes.h", "aten/src/THC/THCGenerateBFloat16Type.h", "aten/src/THC/THCGenerateBoolType.h", "aten/src/THC/THCGenerateCharType.h", "aten/src/THC/THCGenerateComplexDoubleType.h", "aten/src/THC/THCGenerateComplexFloatType.h", "aten/src/THC/THCGenerateComplexTypes.h", "aten/src/THC/THCGenerateDoubleType.h", "aten/src/THC/THCGenerateFloatType.h", "aten/src/THC/THCGenerateFloatTypes.h", "aten/src/THC/THCGenerateHalfType.h", "aten/src/THC/THCGenerateIntType.h", "aten/src/THC/THCGenerateLongType.h", "aten/src/THC/THCGenerateShortType.h", "aten/src/THC/THCTensor.cpp", "aten/src/THC/THCTensor.h", "aten/src/THC/THCTensor.hpp", "aten/src/THC/generic/THCTensor.cpp", "aten/src/THC/generic/THCTensor.h"], "labels": ["Merged", "cla signed", "ciflow/default"]}, "94b6fa6f8b": {"title": "Adds an optimizer instance variable to ChainedScheduler (#68010)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/67601.\n\nAs simple a fix as I could make it. I even managed to delete some testing code!\n\nI checked calling `super()` and, as I had feared, it doesn't work out the box, so perhaps that ought to be revisited later.\n\nAs it stands,  https://github.com/pytorch/pytorch/issues/20124, still applies to the chained scheduler, but I think this change is still an improvement.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/68010\n\nReviewed By: zou3519\n\nDifferential Revision: D32278139\n\nPulled By: albanD\n\nfbshipit-source-id: 4c6f9f1b2822affdf63a6d22ddfdbcb1c6afd579", "pr_number": "68010", "files_changed": ["test/test_optim.py", "torch/optim/lr_scheduler.py"], "labels": ["triaged", "open source", "Merged", "cla signed", "ciflow/default"]}, "a6c0edff1a": {"title": "fix gradcheck to generate valid input for forward AD complex (#68001)", "body": "Summary:\nThis fixed a few of the linalg checks that we disabled before!\n\nThis also seems to break sgn, abs and angle (sending on CI here to see if there are more). These two functions used to only ever get pure imaginary or real values.\nThis is very much likely that something is wrong with their formula.\nBut they are implemented as element-wise, so not sure where the error can come from. I tried to look at it but nothing obvious seems wrong there (especially because it is correct in backward mode).\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/68001\n\nReviewed By: soulitzer\n\nDifferential Revision: D32280475\n\nPulled By: albanD\n\nfbshipit-source-id: e68b1ce0e2e97f8917c3d393141d649a7669aa9d", "pr_number": "68001", "files_changed": ["torch/autograd/gradcheck.py", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "cla signed", "ciflow/default"]}, "6011c35a79": {"title": "[LTC] Upstream class BackendDevice (#68027)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/68027\n\nThis commit upstreams class BackendDevice to the master, which is a backend\nspecific representation of the actual hardware, for instances, CPU, GPU, or\nTPU.\n\nThis concept is important for backend like XLA where it needs to tell the\nactual hardware type from the c10::DeviceType::Lazy virtual device during\nboth IR constructions and lowerings.\n\nTest Plan: ./build/bin/test_lazy --gtest_filter=BackendDeviceTest.*\n\nReviewed By: wconstab\n\nDifferential Revision: D32261838\n\nPulled By: alanwaketan\n\nfbshipit-source-id: 579c3fc5f9da7847c887a383c6047e8ecb9cc5bc", "pr_number": "68027", "files_changed": ["test/cpp/lazy/CMakeLists.txt", "test/cpp/lazy/test_backend_device.cpp", "test/cpp/lazy/test_shape.cpp", "tools/build_variables.bzl", "torch/csrc/lazy/backend/backend_device.cpp", "torch/csrc/lazy/backend/backend_device.h"], "labels": ["Merged", "cla signed", "ciflow/default"]}, "147de8243b": {"title": "Fixed deprection warnings with `.data<T>()` in SpectalOps.cpp (#67993)", "body": "Summary:\nDescription:\n- Fixed deprection warnings `.data<T>()` -> `.data_ptr<T>()` in SpectralOps.cpp shown while building pytorch from source\n\n```c++\n../aten/src/ATen/native/mkl/SpectralOps.cpp:213:10: warning: \u2018T* at::Tensor::data() const [with T = c10::complex<double>]\u2019 is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.\ndata_ptr<T>() instead. [-Wdeprecated-declarations]\n  213 |   return reinterpret_cast<std::complex<T>*>(t.data<c10::complex<T>>());\n```\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/67993\n\nReviewed By: H-Huang\n\nDifferential Revision: D32246945\n\nPulled By: mruberry\n\nfbshipit-source-id: 5cd6b0ac6ddff0afc56e99641971e1e3b6434af6", "pr_number": "67993", "files_changed": ["aten/src/ATen/native/mkl/SpectralOps.cpp"], "labels": ["open source", "Merged", "cla signed", "ciflow/default"]}, "d6e6064efc": {"title": "[LT] Upstream backend interfaces (#67927)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/67927\n\nBackendData - represents 'tensor data' in opaque backend storage\nLoweringContext - interface for performing backend-specific IR lowering\nBackendImplInterface - interface for lazy tensors backends to implement\n\nReorgs backend-related files into lazy/backend subdir\n\nincludes a few small fixes, which were made on lazy_tensor_staging but need to be back-ported to master.\n\nTest Plan: used by lazy_tensor_staging branch\n\nReviewed By: desertfire\n\nDifferential Revision: D32142032\n\nfbshipit-source-id: 828c717bcd0d511876e64ad209b50f7bfb10cec5", "pr_number": "67927", "files_changed": ["test/cpp/lazy/test_ir_util.cpp", "tools/build_variables.bzl", "torch/csrc/lazy/backend/backend_data.h", "torch/csrc/lazy/backend/backend_interface.h", "torch/csrc/lazy/backend/lowering_context.cpp", "torch/csrc/lazy/backend/lowering_context.h", "torch/csrc/lazy/core/hash.h", "torch/csrc/lazy/core/ir_metadata.h"], "labels": ["fb-exported", "Merged", "cla signed", "ciflow/default"]}, "22e73f616c": {"title": "Update unpack_dual to return named tuple (#68062)", "body": "Summary:\nAlso updates the doc\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/68062\n\nReviewed By: gchanan\n\nDifferential Revision: D32315089\n\nPulled By: soulitzer\n\nfbshipit-source-id: 567c812da093daeb6549b0dc7ecbffd58eb8ccc2", "pr_number": "68062", "files_changed": ["test/test_autograd.py", "torch/autograd/forward_ad.py"], "labels": ["Merged", "cla signed", "ciflow/default"]}, "a6a2616558": {"title": "Automated submodule update: kineto (#67445)", "body": "Summary:\nThis is an automated pull request to update the first-party submodule for [pytorch/kineto](https://github.com/pytorch/kineto).\n\nNew submodule commit: https://github.com/pytorch/kineto/commit/f60ad2cb0f7cad6784c89c62c7bbb12f10b04b67\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/67445\n\nTest Plan: Ensure that CI jobs succeed on GitHub before landing.\n\nReviewed By: robieta\n\nDifferential Revision: D31993939\n\nfbshipit-source-id: 3d4aa2f900434d4bbe5134db8453deb227ef5685", "pr_number": "67445", "files_changed": ["third_party/kineto"], "labels": ["triaged", "open source", "Merged", "cla signed", "ciflow/default"]}, "aea4e61ec3": {"title": "skip test_jit_legacy (#68129)", "body": "Summary:\ndisables failing tests in [https://github.com/pytorch/pytorch/issues/66429](https://github.com/pytorch/pytorch/issues/67646)\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/68129\n\nReviewed By: suo, janeyx99\n\nDifferential Revision: D32326118\n\nPulled By: Krovatkin\n\nfbshipit-source-id: ca00d2214503f418be45dc756057b990fb6e6370", "pr_number": "68129", "files_changed": ["test/test_jit_fuser.py"], "labels": ["Merged", "cla signed", "ciflow/default"]}, "f02efc749a": {"title": "[Dist CI][BE] Run each test in its own process for test_distributed_spawn (#67901)", "body": "Summary:\nContext: https://github.com/pytorch/pytorch/issues/67061\n\nUse `run_test.py`'s provided flag `\"--subprocess\"`, passed in like `extra_unittest_args=[\"--subprocess\"]` when running test_distributed_spawn. This will ensure that each test is run separately in its own process. The goal is to more closely simulate how a developer would run a single test when reproducing a CI failure and make reproducibility easier in general.\n\nAlso, when a test fails, print out the exact command that was issued so developer knows how to reproduce it.\n\nFor example test fails, it will print out something like the following to logs -\n\n```\nTest exited with non-zero exitcode 1. Command to reproduce: BACKEND=gloo WORLD_SIZE=3 /fsx/users/rvarm1/conda/envs/pytorch/bin/python distributed/test_distributed_spawn.py -v TestDistBackendWithSpawn.test_Backend_enum_class\n```\n\nrunning test_distributed_spawn is still the same cmd as before:\n\n`\npython test/run_test.py --verbose -i distributed/test_distributed_spawn\n`\n\nas seen in [distributed contributing](https://github.com/pytorch/pytorch/blob/master/torch/distributed/CONTRIBUTING.md) guide.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/67901\n\nReviewed By: cbalioglu, mruberry\n\nDifferential Revision: D32225172\n\nPulled By: rohan-varma\n\nfbshipit-source-id: 7e8d4c7a41858044bd2a4e0d1f0bf8f1ac671d67", "pr_number": "67901", "files_changed": ["test/run_test.py", "torch/testing/_internal/common_utils.py"], "labels": ["better-engineering", "Merged", "cla signed", "ci/master", "ciflow/default"]}, "f89572f417": {"title": "Add feature: zeros_like() from a dense tensor to a sparse tensor (#68108)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/67904.\n - Create a sparse tensor when the sparse layout is given even if the input tensor is not sparse.\n\ncc nikitaved pearu cpuhrsch IvanYashchuk\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/68108\n\nReviewed By: anjali411\n\nDifferential Revision: D32316269\n\nPulled By: cpuhrsch\n\nfbshipit-source-id: 923dbd4dc7c74f51f7cdbafb2375a30271a6a886", "pr_number": "68108", "files_changed": ["aten/src/ATen/native/TensorFactories.cpp", "test/test_sparse.py"], "labels": ["module: sparse", "open source", "Merged", "cla signed", "ciflow/default"]}, "35f1617001": {"title": "Implement Entropy methods for Binomial and Multinomial distributions (#67609)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/60866.\n\nBecause it seems https://github.com/pytorch/pytorch/pull/61719 shows no response for a long time, I made this PR.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/67609\n\nReviewed By: malfet\n\nDifferential Revision: D32310866\n\nPulled By: mruberry\n\nfbshipit-source-id: b3a8dde452f448e5981f5405f5f925f860b0d84f", "pr_number": "67609", "files_changed": ["test/distributions/test_distributions.py", "torch/distributions/binomial.py", "torch/distributions/multinomial.py"], "labels": ["triaged", "open source", "Merged", "cla signed", "ciflow/default"]}, "bd5f33f91e": {"title": "demo backend decoupled from operators (#66100)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66100\n\nA backend should not directly dependent on ATen operators. The demo backend is changed to that way for testing purpose.\n\nTest Plan: Imported from OSS\n\nReviewed By: pavithranrao\n\nDifferential Revision: D31384614\n\nPulled By: iseeyuan\n\nfbshipit-source-id: c97f0c4aa12feb1d124f1d7a852e9955a7a2ce42", "pr_number": "66100", "files_changed": ["test/cpp/jit/test_backend_compiler_lib.cpp", "test/jit/test_backends.py"], "labels": ["oncall: jit", "Merged", "cla signed", "ciflow/default"]}, "fe90313d02": {"title": "Avoid index_put_ overhead in histogram kernel's inner loop (#67815)", "body": "Summary:\n**TLDR**: Makes torch.histc run 400x faster on large inputs. Should fix [a broken test on internal CI](https://www.internalfb.com/intern/test/281475013640093/).\n\nHistogramKernel presently calls torch.Tensor.index_put_ once for each element of its input tensor. Obtaining a data pointer and manipulating it directly avoids the considerable dispatch overhead from calling index_put_. Behavior is unchanged because the tensor being operated on is known to be contiguous and in CPU memory.\n\nFixes performance regression introduced in https://github.com/pytorch/pytorch/pull/65318.\n\nBenchmark: time taken to compute histc on a tensor with 10,000,000 elements\n\n1. Before https://github.com/pytorch/pytorch/pull/65318: **0.003s**\n2. After https://github.com/pytorch/pytorch/pull/65318: **2.154s**\n3. After this change: **0.005s**\n\nBenchmark code:\n```\nimport torch as t\nfrom timeit import default_timer as timer\n\nx = t.randperm(10000000, dtype=t.float32)\n\nstart = timer()\nt.histc(x)\nend = timer()\nprint(end - start)\n```\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/67815\n\nReviewed By: anjali411\n\nDifferential Revision: D32357663\n\nPulled By: saketh-are\n\nfbshipit-source-id: f8fa59173ea4772c8ad1332548ef4d9ea8f01178", "pr_number": "67815", "files_changed": ["aten/src/ATen/native/cpu/HistogramKernel.cpp"], "labels": ["Merged", "cla signed", "ciflow/default"]}, "b473ca999b": {"title": "[lint] add cmakelint to lintrunner (#68191)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/68191\n\n+ fix filename of exec_linter\n\nTest Plan: Imported from OSS\n\nReviewed By: anjali411\n\nDifferential Revision: D32364022\n\nPulled By: suo\n\nfbshipit-source-id: 740892d9580edc348c3e818664fd37f145669fda", "pr_number": "68191", "files_changed": [".lintrunner.toml", "tools/linter/adapters/cmake_linter.py"], "labels": ["Merged", "cla signed", "ciflow/default"]}, "cd4e31ff21": {"title": "[LTC] Add some comments to BackendDevice() (#68156)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/68156\n\n[skip ci]\n\nTest Plan: Imported from OSS\n\nReviewed By: wconstab\n\nDifferential Revision: D32346302\n\nPulled By: alanwaketan\n\nfbshipit-source-id: 06de6afbe2f937511abce485b24cec0a85bfbe97", "pr_number": "68156", "files_changed": ["torch/csrc/lazy/backend/backend_device.h"], "labels": ["Merged", "cla signed", "ciflow/default"]}, "db014b8529": {"title": "Add `set_deterministic_debug_mode` and `get_deterministic_debug_mode` (#67778)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/67386\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/67778\n\nReviewed By: ngimel\n\nDifferential Revision: D32310661\n\nPulled By: mruberry\n\nfbshipit-source-id: 300129e96ca51c22fa711182ce6a9f4d4d2ce57f", "pr_number": "67778", "files_changed": ["docs/source/torch.rst", "test/test_torch.py", "torch/_C/__init__.pyi.in", "torch/__init__.py", "torch/overrides.py"], "labels": ["triaged", "open source", "Merged", "cla signed", "ciflow/default"]}, "4fe3965b3a": {"title": "Fix dtype arg typing for Tensor.type doc string (#67019)", "body": "Summary:\nFix typing error in PyCharm when using torch.Tensor.type(dtype=torch.int64)\n\n<img width=\"386\" alt=\"Screenshot 2021-10-21 at 15 30 50\" src=\"https://user-images.githubusercontent.com/59562934/138288062-cc2ba45e-ece0-4fca-9369-55d020404c28.png\">\n\nThanks for your great work! :)\n\ncc brianjo mruberry\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/67019\n\nReviewed By: malfet\n\nDifferential Revision: D32311313\n\nPulled By: mruberry\n\nfbshipit-source-id: 90fc453bc4129a301d567d4b39137b93c5dac01e", "pr_number": "67019", "files_changed": ["torch/_tensor_docs.py"], "labels": ["module: docs", "triaged", "open source", "Merged", "cla signed", "ciflow/default"]}, "f9ea41f257": {"title": "Fixes spelling error writeable to writable, improves warning, and documentation (#67664)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/46741\npytorchbot\n\ncontributors: nickleus27, yanivsagy, and khanhthien123\n\nSmrutiSikha this is mostly your work.  We just did very minor clean up.\n\ncc mruberry\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/67664\n\nReviewed By: gchanan\n\nDifferential Revision: D32311838\n\nPulled By: mruberry\n\nfbshipit-source-id: 0e5d4d888caeccb0fd7c80e6ff11b1b1fa8e00d6", "pr_number": "67664", "files_changed": ["test/test_torch.py", "torch/_torch_docs.py", "torch/csrc/utils/tensor_numpy.cpp"], "labels": ["triaged", "open source", "Merged", "cla signed", "ciflow/default"]}, "0420545639": {"title": "Enable all dtype combinations in `torch.Tensor.view(dtype)` (#66493)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/29013\n\nNote: This PR does not enable autograd. This can be done in a future PR.\n\ncc mruberry rgommers\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66493\n\nReviewed By: gchanan\n\nDifferential Revision: D32314680\n\nPulled By: mruberry\n\nfbshipit-source-id: 69d325573b2331f32b83c05c91ffbe80571e7ae2", "pr_number": "66493", "files_changed": ["aten/src/ATen/native/TensorConversions.cpp", "test/test_view_ops.py", "torch/_tensor_docs.py"], "labels": ["triaged", "module: numpy", "open source", "Merged", "module: viewing and reshaping", "cla signed", "ciflow/default"]}, "d049772538": {"title": "Bump dlpack.h to latest version (#65047)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/64995\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65047\n\nReviewed By: ngimel\n\nDifferential Revision: D32039318\n\nPulled By: mruberry\n\nfbshipit-source-id: 7dfc653e1e77799d1f26a95fa9bbae3c7ffc887c", "pr_number": "65047", "files_changed": ["aten/src/ATen/DLConvertor.cpp", "aten/src/ATen/DLConvertor.h", "aten/src/ATen/dlpack.h", "aten/src/ATen/test/cuda_dlconvertor_test.cpp"], "labels": ["open source", "Merged", "cla signed", "ciflow/default"]}, "9571eb599c": {"title": "[lint] fix up clangtidy lintrunner integration (#68192)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/68192\n\n- Run on exactly the same stuff as the existing linter checks.\n- Exclude deploy interpreter headers from being reported.\n\nTest Plan: Imported from OSS\n\nReviewed By: janeyx99\n\nDifferential Revision: D32364023\n\nPulled By: suo\n\nfbshipit-source-id: c27eca4a802534875d609d004fa9f6fca59ae6a5", "pr_number": "68192", "files_changed": [".clang-tidy", ".lintrunner.toml", "tools/linter/adapters/clangtidy_linter.py"], "labels": ["Merged", "cla signed", "ciflow/default"]}, "89d556f648": {"title": "add VS extension in doc (#63944)", "body": "Summary:\nadd VS  extension in https://pytorch.org/cppdocs/installing.html\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/63944\n\nReviewed By: malfet\n\nDifferential Revision: D30546156\n\nPulled By: seemethere\n\nfbshipit-source-id: a65448d8702f9fd400c9dd2ef2d9f961f30c4983", "pr_number": "63944", "files_changed": ["docs/cpp/source/installing.rst"], "labels": ["open source", "Merged", "cla signed"]}, "613c1aca6d": {"title": "Adds support for automated error and warning testing (#67354)", "body": "Summary:\nAdds a new class `ErrorOrWarningInput` that is a `SampleInput` with some additional metadata for validating that `SampleInput` throws the desired warning or error. The architecture to support these new tests is modeled after the existing reference tests and sample input functions.\n\nExisting invalid input tests for neg and kthvalue are ported to the new scheme to validate it.\n\nThere may be a simpler/clearer naming scheme we can use here.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/67354\n\nReviewed By: jbschlosser\n\nDifferential Revision: D31989888\n\nPulled By: mruberry\n\nfbshipit-source-id: 4fa816e1e8d0eef21b81c2f80813d42b2c26714e", "pr_number": "67354", "files_changed": ["test/test_ops.py", "test/test_sort_and_select.py", "test/test_unary_ufuncs.py", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "cla signed", "ciflow/default"]}, "1181628d85": {"title": "BE: Use TORCH_CHECK instead of explicit c10::Error (#68187)", "body": "Summary:\n`if (cond) { raise c10::error(\"\", msg)}` is identical to `TORCH_CHECK(!cond, msg);`, but with better attribution\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/68187\n\nReviewed By: xuzhao9\n\nDifferential Revision: D32360956\n\nPulled By: malfet\n\nfbshipit-source-id: e554b99926d7ad0c79a1cd54d35f47339fa2429d", "pr_number": "68187", "files_changed": ["aten/src/ATen/core/ivalue.h"], "labels": ["better-engineering", "Merged", "cla signed", "ciflow/default"]}, "dc24503a89": {"title": "Fix Hash(c10::Scalar), account for garbage data in union (#68201)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/68201\n\nHash(c10::Scalar) made a bad assumption that it was valid to just hash over all the bytes of data of the c10::Scalar struct.\n\nBecuase c10::Scalar stores a union of different (float/int/complex) types with different sizes, not all bytes are valid in all cases.  Hash() should only read the bytes corresponding to the currently active type.\n\nTest Plan: Added new unit tests.  Verified HashTest.Scalar failed with the original Hash() impl and then fixed.\n\nReviewed By: alanwaketan\n\nDifferential Revision: D32367564\n\nfbshipit-source-id: ac30dd4f6dd0513954986d3d23c0c11ba802c37b", "pr_number": "68201", "files_changed": ["test/cpp/lazy/test_misc.cpp", "torch/csrc/lazy/core/hash.h"], "labels": ["fb-exported", "Merged", "cla signed", "ciflow/default"]}, "98bab78e11": {"title": "Revert D32039318: [pytorch][PR] Bump dlpack.h to latest version", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD32039318 (https://github.com/pytorch/pytorch/commit/d04977253895a772f711f94ab865200ffba05bcb)\n\nOriginal commit changeset: 7dfc653e1e77\n\nfbshipit-source-id: 0d4b1af7381a2638ca9f3c3af26c2ff0b7bd7469", "pr_number": null, "files_changed": ["aten/src/ATen/DLConvertor.cpp", "aten/src/ATen/DLConvertor.h", "aten/src/ATen/dlpack.h", "aten/src/ATen/test/cuda_dlconvertor_test.cpp"], "labels": []}, "6ddaf3bd37": {"title": "[LT] Upstream TsNode, TsNodeLowering, TsLoweringContext (#68154)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/68154\n\nTest Plan: added a basic test; cover more by using lazy_tensor_staging tests\n\nReviewed By: Krovatkin, alanwaketan\n\nDifferential Revision: D32224303\n\nfbshipit-source-id: ac3e1161229b8ae60fdb15ffa72e17072b595914", "pr_number": "68154", "files_changed": ["test/cpp/lazy/test_ir.cpp", "tools/build_variables.bzl", "torch/csrc/lazy/ts_backend/config.cpp", "torch/csrc/lazy/ts_backend/config.h", "torch/csrc/lazy/ts_backend/ts_lowering_context.h", "torch/csrc/lazy/ts_backend/ts_node.cpp", "torch/csrc/lazy/ts_backend/ts_node.h", "torch/csrc/lazy/ts_backend/ts_node_lowering.h"], "labels": ["fb-exported", "Merged", "cla signed", "ciflow/default"]}, "a82e51a7ae": {"title": "Move some cub templates out of the header file (#67650)", "body": "Summary:\nCub routines are both expensive to compile and used in multiple\ndifferent operators throughout the cuda folder. So, it makes sense to\ncompile them in one centralized place where possible (i.e. when\ncustom operators aren't involved).\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/67650\n\nReviewed By: mruberry\n\nDifferential Revision: D32259660\n\nPulled By: ngimel\n\nfbshipit-source-id: 5f7dbdb134297e1ffdc1c7fc5aefee70a2fa5422", "pr_number": "67650", "files_changed": ["BUILD.bazel", "aten/src/ATen/cuda/cub.cu", "aten/src/ATen/cuda/cub.cuh", "aten/src/ATen/cuda/cub.h", "aten/src/ATen/native/cuda/Embedding.cu", "aten/src/ATen/native/cuda/EmbeddingBackwardKernel.cu", "aten/src/ATen/native/cuda/EmbeddingBag.cu", "aten/src/ATen/native/cuda/IndexKernel.cu", "aten/src/ATen/native/cuda/Indexing.cu", "aten/src/ATen/native/cuda/Randperm.cu", "aten/src/ATen/native/cuda/SegmentReduce.cu", "aten/src/ATen/native/cuda/UniqueCub.cu"], "labels": ["triaged", "open source", "Merged", "cla signed", "ciflow/cuda"]}, "8bf150f21b": {"title": "Revert D32178667: [pytorch][PR] Python tracer for profiler", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD32178667 (https://github.com/pytorch/pytorch/commit/33353fb8284af2576c4f42fa298469cf11e9e78b)\n\nOriginal commit changeset: 118547104a7d\n\nfbshipit-source-id: 47510607589fc39c730ba913f47c01a7d107b7b0", "pr_number": null, "files_changed": ["tools/build_variables.bzl", "torch/csrc/autograd/init.cpp", "torch/csrc/autograd/profiler_kineto.cpp", "torch/csrc/autograd/profiler_kineto.h", "torch/csrc/autograd/profiler_python.cpp", "torch/csrc/autograd/profiler_python.h", "torch/profiler/python_tracer.py"], "labels": []}, "da5ffe752a": {"title": "Add reporting for flaky tests in CI (#68150)", "body": "Summary:\nThis PR does NOT change how signal is displayed in CI but rather just reports stats of flaky tests to RDS. **None of the below will be enabled after landing this PR--it will be done in a separate PR with environment variables.**\n\nWe report flaky tests stats when a test first fails, and when we rerun it MAX_NUM_RETRIES times, we get at least one success.\nFor tests that fail all the reruns, we assume it is because it is a real test failure.\nFor tests that succeed the first time, we do not rerun the test, even if it was previously noted as flaky.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/68150\n\nTest Plan:\nFirst, I modified:\ntest_async_python to always fail (will be our \"failing test\")\ntest_async_future_type_python to fail 40% of the time\ntest_async_script_capture to fail 60% of the time\n\nThen, running `python test/test_jit.py -v -k test_async` while setting IN_CI to 1:\n```\n(pytorch) janeyx@janeyx-mbp pytorch % python test/test_jit.py -v -k test_async\n...\n\nRunning tests...\n----------------------------------------------------------------------\n  test_async_future_type_python (jit.test_async.TestAsync) ... ok (0.004s)\n  test_async_grad_guard_no_grad (jit.test_async.TestAsync) ... ok (0.020s)\n  test_async_grad_guard_with_grad (jit.test_async.TestAsync) ... ok (0.008s)\n  test_async_kwargs (jit.test_async.TestAsync) ... ok (0.045s)\n  test_async_parsing (jit.test_async.TestAsync) ... ok (0.010s)\n  test_async_python (jit.test_async.TestAsync) ... FAIL (0.003s)\n    test_async_python failed - num_retries_left: 3\n  test_async_python (jit.test_async.TestAsync) ... FAIL (0.003s)\n    test_async_python failed - num_retries_left: 2\n  test_async_python (jit.test_async.TestAsync) ... FAIL (0.003s)\n    test_async_python failed - num_retries_left: 1\n  test_async_python (jit.test_async.TestAsync) ... FAIL (0.003s)\n    test_async_python failed - num_retries_left: 0\n  test_async_script (jit.test_async.TestAsync) ... ok (0.008s)\n  test_async_script_capture (jit.test_async.TestAsync) ... FAIL (0.010s)\n    test_async_script_capture failed - num_retries_left: 3\n  test_async_script_capture (jit.test_async.TestAsync) ... FAIL (0.010s)\n    test_async_script_capture failed - num_retries_left: 2\n  test_async_script_capture (jit.test_async.TestAsync) ... ok (0.011s)\n    test_async_script_capture succeeded - num_retries_left: 1\n  test_async_script_capture (jit.test_async.TestAsync) ... FAIL (0.010s)\n    test_async_script_capture failed - num_retries_left: 0\n  test_async_script_error (jit.test_async.TestAsync) ... ok (0.040s)\n  test_async_script_multi_forks (jit.test_async.TestAsync) ... ok (0.025s)\n  test_async_script_multi_waits (jit.test_async.TestAsync) ... ok (0.009s)\n...\n\n======================================================================\nFAIL [0.003s]: test_async_python (jit.test_async.TestAsync)\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File \"/Users/janeyx/pytorch/test/jit/test_async.py\", line 30, in test_async_python\n    self.assertTrue(False)\nAssertionError: False is not true\n\n======================================================================\nFAIL [0.010s]: test_async_script_capture (jit.test_async.TestAsync)\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File \"/Users/janeyx/pytorch/test/jit/test_async.py\", line 123, in test_async_script_capture\n    self.assertTrue(False)\nAssertionError: False is not true\n\n----------------------------------------------------------------------\nRan 28 tests in 0.399s\n\nFAILED (failures=2, expected failures=5, unexpected successes=1)\n```\nYielding this as the test report (I changed the extension from xml to txt so it uploads here):\n[TEST-jit.test_async.TestAsync-20211110222055.txt](https://github.com/pytorch/pytorch/files/7517532/TEST-jit.test_async.TestAsync-20211110222055.txt)\n\nAnd then running print_test_stats correctly excludes the all failing test `test_async_python` and calculates red and green appropriately:\n```\n(pytorch) janeyx@janeyx-mbp pytorch % python tools/stats/print_test_stats.py test-reports/python-unittest/test.test_jit\n[scribe] Not invoking RDS lambda outside GitHub Actions:\n[{'create_table': {'table_name': 'flaky_tests', 'fields': {'name': 'string', 'suite': 'string', 'file': 'string', 'num_green': 'int', 'num_red': 'int', 'pr': 'string', 'ref': 'string', 'branch': 'string', 'workflow_id': 'string', 'build_environment': 'string'}}}]\n[scribe] Writing for None\n[scribe] Wrote stats for flaky_tests\n[scribe] Not invoking RDS lambda outside GitHub Actions:\n[{'write': {'table_name': 'flaky_tests', 'values': {'name': 'test_async_script_capture', 'suite': 'jit.test_async.TestAsync', 'file': 'test/test_jit', 'num_green': 1, 'num_red': 3, 'pr': None, 'ref': None, 'branch': None, 'workflow_id': None, 'build_environment': 'linux-xenial-gcc5.4-py3'}}}]\n(pytorch) janeyx@janeyx-mbp pytorch %\n```\n\n-------------------\nIf you're curious, I also included the code for when we would like to override the report_only feature and also hide flaky signal in CI. The results for the same test command correctly still fail the test suite, but mark the flaky test_async_future_type_python as passed:\n```\n(pytorch) janeyx@janeyx-mbp pytorch % python test/test_jit.py -v -k test_async\n...\n\nRunning tests...\n----------------------------------------------------------------------\n  test_async_future_type_python (jit.test_async.TestAsync) ... FAIL (0.004s)\n    test_async_future_type_python failed - num_retries_left: 3\n  test_async_future_type_python (jit.test_async.TestAsync) ... ok (0.001s)\n  test_async_grad_guard_no_grad (jit.test_async.TestAsync) ... ok (0.017s)\n  test_async_grad_guard_with_grad (jit.test_async.TestAsync) ... ok (0.008s)\n  test_async_kwargs (jit.test_async.TestAsync) ... ok (0.091s)\n  test_async_parsing (jit.test_async.TestAsync) ... ok (0.010s)\n  test_async_python (jit.test_async.TestAsync) ... FAIL (0.003s)\n    test_async_python failed - num_retries_left: 3\n  test_async_python (jit.test_async.TestAsync) ... FAIL (0.003s)\n    test_async_python failed - num_retries_left: 2\n  test_async_python (jit.test_async.TestAsync) ... FAIL (0.004s)\n    test_async_python failed - num_retries_left: 1\n  test_async_python (jit.test_async.TestAsync) ... FAIL (0.003s)\n    test_async_python failed - num_retries_left: 0\n  test_async_script (jit.test_async.TestAsync) ... ok (0.008s)\n  test_async_script_capture (jit.test_async.TestAsync) ... ok (0.011s)\n  test_async_script_error (jit.test_async.TestAsync) ... ok (0.039s)\n...\n\n======================================================================\nFAIL [0.003s]: test_async_python (jit.test_async.TestAsync)\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File \"/Users/janeyx/pytorch/test/jit/test_async.py\", line 30, in test_async_python\n    self.assertTrue(False)\nAssertionError: False is not true\n\n----------------------------------------------------------------------\nRan 26 tests in 0.390s\n\nFAILED (failures=1, expected failures=4)\n```\nWith test reports:\n[TEST-jit.test_async.TestAsync-20211110224810.txt](https://github.com/pytorch/pytorch/files/7517663/TEST-jit.test_async.TestAsync-20211110224810.txt)\nAnd running print_test_stats:\n```\n(pytorch) janeyx@janeyx-mbp pytorch % python tools/stats/print_test_stats.py test-reports/python-unittest/test.test_jit\n[scribe] Not invoking RDS lambda outside GitHub Actions:\n[{'create_table': {'table_name': 'flaky_tests', 'fields': {'name': 'string', 'suite': 'string', 'file': 'string', 'num_green': 'int', 'num_red': 'int', 'pr': 'string', 'ref': 'string', 'branch': 'string', 'workflow_id': 'string', 'build_environment': 'string'}}}]\n[scribe] Writing for None\n[scribe] Wrote stats for flaky_tests\n[scribe] Not invoking RDS lambda outside GitHub Actions:\n[{'write': {'table_name': 'flaky_tests', 'values': {'name': 'test_async_future_type_python', 'suite': 'jit.test_async.TestAsync', 'file': 'test/test_jit', 'num_green': 1, 'num_red': 1, 'pr': None, 'ref': None, 'branch': None, 'workflow_id': None, 'build_environment': 'linux-xenial-gcc5.4-py3'}}}]\n```\n\nReviewed By: saketh-are\n\nDifferential Revision: D32393907\n\nPulled By: janeyx99\n\nfbshipit-source-id: 37df890481ab84c62809c022dc6338b50972899c", "pr_number": "68150", "files_changed": ["tools/stats/print_test_stats.py", "torch/testing/_internal/common_utils.py"], "labels": ["Merged", "cla signed", "ciflow/default"]}, "a8b93cb3ec": {"title": "More aggressively market functorch.vmap when torch.vmap gets called (#67347)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/67347\n\nThis PR:\n- changes the warning when torch.vmap gets called to suggest using\nfunctorch.vmap\n- changes the warning when a batching rule isn't implemented to suggest\nusing functorch.vmap\n\nTest Plan: - test/test_vmap.py\n\nReviewed By: H-Huang\n\nDifferential Revision: D31966603\n\nPulled By: zou3519\n\nfbshipit-source-id: b01dc1c2e298ce899b4a3a5fb333222a8d5bfb56", "pr_number": "67347", "files_changed": ["aten/src/ATen/BatchedFallback.cpp", "test/test_vmap.py", "torch/_vmap_internals.py"], "labels": ["Merged", "cla signed", "ciflow/default"]}, "80339e85c5": {"title": "Fix disabling bot with subprocessing (#68290)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/68270\n\nTested locally + tests get disabled properly\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/68290\n\nReviewed By: mrshenli\n\nDifferential Revision: D32403956\n\nPulled By: janeyx99\n\nfbshipit-source-id: 86629daa86f83f6777f2279524ef973af51046b9", "pr_number": "68290", "files_changed": ["torch/testing/_internal/common_utils.py"], "labels": ["Merged", "cla signed", "ciflow/default"]}, "24b60b2cbf": {"title": "[lint] lintrunner fixes/improvements (#68292)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/68292\n\n- noqa was typo-d to be the same as type: ignore\n- generalize clang-tidy initialization and use it for clang_format as well\n- Add a script that lets you update the binaries in s3 relatively easily\n\nTest Plan: Imported from OSS\n\nReviewed By: malfet\n\nDifferential Revision: D32403934\n\nPulled By: suo\n\nfbshipit-source-id: 4e21b22605216f013d87d636a205707ca8e0af36", "pr_number": "68292", "files_changed": [".gitignore", ".lintrunner.toml", "tools/linter/adapters/clangformat_linter.py", "tools/linter/adapters/clangtidy_init.py", "tools/linter/adapters/s3_init.py", "tools/linter/adapters/s3_init_config.json", "tools/linter/adapters/update_s3.py"], "labels": ["Merged", "cla signed", "ciflow/default"]}, "5b05983497": {"title": "[bugfix] fix two edge cases in functionalization (#68269)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/68269\n\nTest Plan: Imported from OSS\n\nReviewed By: mrshenli\n\nDifferential Revision: D32396357\n\nPulled By: bdhirsh\n\nfbshipit-source-id: 1d374b693f3f526d027104cbdc08b8bbe9d38307", "pr_number": "68269", "files_changed": ["aten/src/ATen/FunctionalStorageImpl.cpp", "aten/src/ATen/FunctionalTensorWrapper.cpp", "test/test_functionalization.py"], "labels": ["Merged", "cla signed", "ciflow/default"]}, "30cda0b28c": {"title": "[bugfix] functionalization pass for view ops without a 'self' first argumennt (#68339)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/68339\n\nTest Plan: Imported from OSS\n\nReviewed By: mrshenli\n\nDifferential Revision: D32429570\n\nPulled By: bdhirsh\n\nfbshipit-source-id: e6df243c508c2ba2ca1df7a53fa68f32db454f32", "pr_number": "68339", "files_changed": ["tools/codegen/gen_functionalization_type.py"], "labels": ["Merged", "cla signed", "ciflow/default"]}, "ccd9675569": {"title": "[lint] Disable modernize-use-nodiscard (#68354)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/68354\n\nLint rule: https://clang.llvm.org/extra/clang-tidy/checks/modernize-use-nodiscard.html\n\nThis check adds a ton of noise to our diffs. `[[nodiscard]]` is typically only useful when ignoring the return value of a function is a critical error, e.g. for `operator new`.\n\nTest Plan: Verified that the lint does not get triggered\n\nReviewed By: hlu1\n\nDifferential Revision: D32429731\n\nfbshipit-source-id: ca3d90686ec8d419d3f96167140dc406df6f4a53", "pr_number": "68354", "files_changed": [".clang-tidy"], "labels": ["fb-exported", "Merged", "cla signed", "ciflow/default"]}, "0823d18fcd": {"title": "make TSComputation ctor explicit (#68286)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/68286\n\nTest Plan: check it compiles\n\nReviewed By: alanwaketan\n\nDifferential Revision: D32402016\n\nfbshipit-source-id: b623afa8831cd906336d7fcafbcbad32f79254b0", "pr_number": "68286", "files_changed": ["torch/csrc/lazy/ts_backend/ts_lowering_context.h"], "labels": ["fb-exported", "Merged", "cla signed", "ciflow/default"]}, "065018d812": {"title": "[pytorch][xros] Ensure all pytorch mobile operators build ok in XROS mode (#68266)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/68266\n* Use `if...endif` to adjust pyTorch internals towards XROS\n\nTest Plan: CI\n\nReviewed By: kkosik20\n\nDifferential Revision: D32190771\n\nfbshipit-source-id: cce073dea53c2b5681d913321101cd83c6472019", "pr_number": "68266", "files_changed": ["aten/src/ATen/nnapi/nnapi_wrapper.cpp", "torch/csrc/autograd/engine.cpp"], "labels": ["fb-exported", "Merged", "cla signed", "ciflow/default"]}, "27cc11226d": {"title": "make broadcast fastpath the default for currently rolled-out ops (#68365)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/68365\n\ntitle. broadcast fastpath has been running fine for the enabled ops for a while now, so make it the default for these ops.\n\nTest Plan: diff is a no-op, so sandcastle\n\nDifferential Revision: D32107847\n\nfbshipit-source-id: b239b127b219985bf7df6a0eea2d879b8e9c79a4", "pr_number": "68365", "files_changed": ["caffe2/contrib/fakelowp/elementwise_fp16_fake_op.cc", "caffe2/ideep/operators/elementwise_sum_op.cc", "caffe2/ideep/operators/operator_fallback_ideep.cc", "caffe2/operators/elementwise_add_gradient_op.cc", "caffe2/operators/elementwise_add_op.cc", "caffe2/operators/elementwise_add_op.h", "caffe2/operators/elementwise_add_op_gpu.cc", "caffe2/operators/elementwise_div_gradient_op.cc", "caffe2/operators/elementwise_div_op.cc", "caffe2/operators/elementwise_div_op.h", "caffe2/operators/elementwise_mul_gradient_op.cc", "caffe2/operators/elementwise_mul_op.cc", "caffe2/operators/elementwise_mul_op.h", "caffe2/operators/elementwise_sub_gradient_op.cc", "caffe2/operators/elementwise_sub_op.cc", "caffe2/operators/elementwise_sub_op.h", "caffe2/quantization/server/elementwise_add_dnnlowp_op.cc", "caffe2/quantization/server/elementwise_mul_dnnlowp_op.cc"], "labels": ["fb-exported", "Merged", "cla signed", "ciflow/default"]}, "ec742c65d5": {"title": "Fix a sign comparison issue in BatchLinearAlgebraLib.cpp (#68293)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/68293\n\nTest Plan: Sandcastle\n\nReviewed By: ngimel\n\nDifferential Revision: D32403788\n\nfbshipit-source-id: 1afc5e62e7157f144ec36b029ee3bcc6c23d65a1", "pr_number": "68293", "files_changed": ["aten/src/ATen/native/cuda/BatchLinearAlgebraLib.cpp"], "labels": ["fb-exported", "Merged", "cla signed", "ciflow/default"]}, "438ca7603f": {"title": "Fix sign comparison issue in Histogram.cpp (#68294)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/68294\n\nTest Plan: Sandcastle\n\nReviewed By: ngimel\n\nDifferential Revision: D32403821\n\nfbshipit-source-id: cdbf1d83ab02b1e996559e4cfbbe699b7165483a", "pr_number": "68294", "files_changed": ["aten/src/ATen/native/Histogram.cpp"], "labels": ["fb-exported", "Merged", "cla signed", "ciflow/default"]}, "33e9a0b5f6": {"title": "[Reland] Python tracer. (#68325)", "body": "Summary:\nThere were two issues with the original PR:\n1) My assumption that bound C functions could be trusted to stay alive was not valid. I'm still not entirely sure what was dying, but I've just added a cache so that the first time I see a function I collect the repr just like I was already doing with Python functions.\n\n2) `std::regex` is known to be badly broken and prone to segfaults. Because I'm just doing a very simple prefix prune it's fine to do it manually; see `trimPrefix`. Long term we should move all of PyTorch to `re2` as the internal lint suggests, but CMake is hard and I couldn't get it to work.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/68325\n\nReviewed By: chaekit\n\nDifferential Revision: D32432596\n\nPulled By: robieta\n\nfbshipit-source-id: 06fb4bcdc6933a3e76f6021ca69dc77a467e4b2e", "pr_number": "68325", "files_changed": ["tools/build_variables.bzl", "torch/csrc/autograd/init.cpp", "torch/csrc/autograd/profiler_kineto.cpp", "torch/csrc/autograd/profiler_kineto.h", "torch/csrc/autograd/profiler_python.cpp", "torch/csrc/autograd/profiler_python.h", "torch/profiler/python_tracer.py"], "labels": ["Merged", "cla signed", "ciflow/cuda", "ciflow/all"]}, "bc3d380ed1": {"title": "Throw error when saving storages that view same data with different type (#66949)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/58970\n\ncc mruberry\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66949\n\nReviewed By: albanD\n\nDifferential Revision: D31926323\n\nPulled By: anjali411\n\nfbshipit-source-id: f6e7acc0c1968b70a94f9b0b69a32780e8e21a62", "pr_number": "66949", "files_changed": ["test/test_serialization.py", "torch/serialization.py"], "labels": ["module: serialization", "triaged", "open source", "Merged", "cla signed", "ciflow/default"]}, "5c3529a86d": {"title": "[lint] small pass to make lint clean (#68367)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/68367\n\n- bmm_test.py was using syntax not allowed in 3.6\n- Some suppressions were not placed on the correct line.\n\nWith this file,\n```\nlintrunner --paths-cmd='git grep -Il .'\n```\npasses successfully.\n\nTest Plan: Imported from OSS\n\nReviewed By: janeyx99, mrshenli\n\nDifferential Revision: D32436644\n\nPulled By: suo\n\nfbshipit-source-id: ae9300c6593d8564fb326822de157d00f4aaa3c2", "pr_number": "68367", "files_changed": ["benchmarks/operator_benchmark/pt/bmm_test.py", "test/jit/test_class_type.py", "test/jit/test_freezing.py", "test/jit/test_save_load.py", "test/jit/test_union.py", "test/test_jit.py", "torch/functional.py"], "labels": ["oncall: jit", "Merged", "cla signed", "ciflow/default"]}, "df129fa8d6": {"title": "[PyTorch] Support MaybeOwned<IValue> (#68157)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/68157\n\nDoes what it says on the tin. I don't have a use for `MaybeOwned<IValue>` itself right now, but following diffs will use `MaybeOwnedTraits<IValue>::{create,destroy}Borrow` and I thought it best to just provide the full thing.\nghstack-source-id: 143424915\n\nTest Plan: Extended MaybeOwned tests to cover this.\n\nReviewed By: hlu1\n\nDifferential Revision: D32347393\n\nfbshipit-source-id: 219658cb69b951d36dee80c2ae51387328224866", "pr_number": "68157", "files_changed": ["aten/src/ATen/core/ivalue.h", "aten/src/ATen/core/ivalue_inl.h", "aten/src/ATen/test/MaybeOwned_test.cpp"], "labels": ["Merged", "cla signed", "ciflow/default"]}, "ed00a763a2": {"title": "[PyTorch] Don't force refcount bump when accessing DictEntryRef key/value (#68158)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/68158\n\nto() sometimes returns a reference; let's forward that through.\nghstack-source-id: 143424916\n\nTest Plan: Combined with following diff, seeing a huge drop in dict_unpack self time in ctr_mobile_feed local_ro net. Following diff by itself didn't work.\n\nReviewed By: suo\n\nDifferential Revision: D32347391\n\nfbshipit-source-id: da96295bf83ea30867a2e3fceedc9b4e0a33ffa3", "pr_number": "68158", "files_changed": ["aten/src/ATen/core/Dict.h", "aten/src/ATen/core/ivalue_inl.h"], "labels": ["Merged", "cla signed", "ciflow/default"]}, "86399d8e0c": {"title": "Add histogramdd to torch.rst (#68273)", "body": "Summary:\nThe `torch.histogramdd` operator is documented in `torch/functional.py` but does not appear in the generated docs because it is missing from `docs/source/torch.rst`.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/68273\n\nReviewed By: cpuhrsch\n\nDifferential Revision: D32470522\n\nPulled By: saketh-are\n\nfbshipit-source-id: a23e73ba336415457a30bae568bda80afa4ae3ed", "pr_number": "68273", "files_changed": ["docs/source/torch.rst"], "labels": ["Merged", "cla signed", "ciflow/default"]}, "515d9fb2a9": {"title": "Add OpInfo for torch.histc (#67452)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/67452\n\nReviewed By: davidberard98\n\nDifferential Revision: D32453690\n\nPulled By: saketh-are\n\nfbshipit-source-id: 6311519dc1b2e92a200d0455d32a9c7301a45d51", "pr_number": "67452", "files_changed": ["aten/src/ATen/native/Histogram.cpp", "aten/src/ATen/native/cuda/SummaryOps.cu", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "cla signed", "ciflow/slow", "ciflow/default", "ciflow/all"]}, "fd85d925b0": {"title": "Fix some sign issues (#68361)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/68361\n\nFixes\n```\ncaffe2/aten/src/ATen/FunctionalizeFallbackKernel.cpp:36:31: error: comparison of integers of different signs: 'int64_t' (aka 'long') and 'const unsigned long' [-Werror,-Wsign-compare]\n    for (int64_t idx = 0; idx < num_returns; ++idx) {\n                          ~~~ ^ ~~~~~~~~~~~\ncaffe2/aten/src/ATen/native/cuda/Sorting.cpp:87:16: error: comparison of integers of different signs: 'int64_t' (aka 'long') and 'std::vector::size_type' (aka 'unsigned long') [-Werror,-Wsign-compare]\n    assert(dim < out_shape.size());\n           ~~~ ^ ~~~~~~~~~~~~~~~~\n```\n\nTest Plan: Sandcastle\n\nReviewed By: malfet\n\nDifferential Revision: D32433063\n\nfbshipit-source-id: b896dbab81861f3f074e00db73d20d9523037dd1", "pr_number": "68361", "files_changed": ["aten/src/ATen/FunctionalizeFallbackKernel.cpp", "aten/src/ATen/native/cuda/Sorting.cpp"], "labels": ["fb-exported", "Merged", "cla signed", "ciflow/default"]}, "6186b90c53": {"title": "[Contrib][Fakelowp] Change Lut Size for Tanh (#68334)", "body": "Summary:\nReference code LUT size increased and now mininum\nstarts from 0, instead of 7000 earlier\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/68334\n\nReviewed By: jiecaoyu\n\nDifferential Revision: D32467332\n\nPulled By: hl475\n\nfbshipit-source-id: 3e4510e09374519aebe657a31f0b1ccde117e761", "pr_number": "68334", "files_changed": ["caffe2/contrib/fakelowp/quant_lut_fp16_fake_op.h"], "labels": ["caffe2", "caffe2-op", "triaged", "open source", "Merged", "cla signed", "ciflow/default"]}, "cac3cd1433": {"title": "add torch.diff support for n greater than 1 (#67260)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/67260\n\nAddressing 54853\n\nTest Plan: Imported from OSS\n\nReviewed By: mruberry\n\nDifferential Revision: D31930294\n\nPulled By: mikaylagawarecki\n\nfbshipit-source-id: 97c7a27e9200c6688242680ff96b73dfff828479", "pr_number": "67260", "files_changed": ["aten/src/ATen/native/ReduceOps.cpp", "test/test_torch.py", "torch/_torch_docs.py", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "cla signed", "ciflow/slow", "ciflow/default", "ciflow/all"]}, "693fe2fd9b": {"title": "docs: Added Union to supported types in documentation (#68435)", "body": "Summary:\nThis PR simply updates the documentation following up on https://github.com/pytorch/pytorch/pull/64234, by adding `Union` as a supported type.\n\nAny feedback is welcome!\n\ncc ansley albanD gmagogsfm\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/68435\n\nReviewed By: davidberard98\n\nDifferential Revision: D32494271\n\nPulled By: ansley\n\nfbshipit-source-id: c3e4806d8632e1513257f0295568a20f92dea297", "pr_number": "68435", "files_changed": ["docs/source/jit_language_reference.rst"], "labels": ["open source", "Merged", "cla signed", "ciflow/default"]}, "4e21d77dbb": {"title": "Use TORCH_CHECK in MapAllocator (#68424)", "body": "Summary:\nWhen porting `THAllocator` to ATen I changed `AT_ERROR` to `TORCH_INTERNAL_ASSERT` but the direct translation should have been `TORCH_CHECK`.\n\nhttps://github.com/pytorch/pytorch/blob/33e9a0b5f6674bd429bda689534e1c987b38cf6e/c10/util/Exception.h#L619-L623\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/68424\n\nReviewed By: VitalyFedyunin\n\nDifferential Revision: D32465548\n\nPulled By: ngimel\n\nfbshipit-source-id: 7fa9c1fe27e4849b76248badb681d7b6877ce9e8", "pr_number": "68424", "files_changed": ["aten/src/ATen/MapAllocator.cpp"], "labels": ["open source", "Merged", "cla signed", "ciflow/default"]}, "3b4f072383": {"title": "Remove TH/THC Storage data and copy functions (#68127)", "body": "Summary:\nPart of https://github.com/pytorch/pytorch/issues/67852\n\ncc ezyang bhosmer smessmer ljk53 bdhirsh\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/68127\n\nReviewed By: mrshenli\n\nDifferential Revision: D32441885\n\nPulled By: ngimel\n\nfbshipit-source-id: 1bbe7c8bed30bfe1737511a4f347fd9a8024dd99", "pr_number": "68127", "files_changed": ["BUILD.bazel", "aten/src/TH/CMakeLists.txt", "aten/src/TH/THStorageFunctions.cpp", "aten/src/TH/THStorageFunctions.h", "aten/src/TH/generic/THStorage.cpp", "aten/src/TH/generic/THStorage.h", "aten/src/TH/generic/THStorageCopy.cpp", "aten/src/TH/generic/THStorageCopy.h", "aten/src/THC/CMakeLists.txt", "aten/src/THC/THC.h", "aten/src/THC/THCStorageCopy.cpp", "aten/src/THC/THCStorageCopy.cu", "aten/src/THC/THCStorageCopy.h", "aten/src/THC/generic/THCStorage.cpp", "aten/src/THC/generic/THCStorage.cu", "aten/src/THC/generic/THCStorage.h", "aten/src/THC/generic/THCStorageCopy.cpp", "aten/src/THC/generic/THCStorageCopy.cu", "aten/src/THC/generic/THCStorageCopy.h", "torch/csrc/copy_utils.h", "torch/csrc/generic/Storage.cpp", "torch/csrc/generic/StorageMethods.cpp", "torch/csrc/generic/StorageSharing.cpp", "torch/csrc/generic/serialization.cpp", "torch/csrc/utils.cpp", "torch/csrc/utils.h"], "labels": ["module: internals", "triaged", "module: porting", "open source", "Merged", "cla signed", "ciflow/default"]}, "53bfb00ee1": {"title": "[bugfix] TensorList args in functionalization pass (#68395)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/68395\n\nAt the time that I wrote the pass, I thought that `c10::TensorList` and `c10::List<Tensor>` were the same thing. But it looks like a `TensorList` is actually an `ArrayRef<Tensor>`. This led to a nasty bug when I tried to add conditional functionalization to `block_diag`, where in the boxed kernel, I would:\n\n(1) unwrap the first `IValue` by calling `.toTensorList()` (this actually returns a `List<Tensor>`, not a `TensorList`).\n(2) call `TensorList to_functional_tensor(List<Tensor>)` to get out a `TensorList` with the functionalized tensors\n(3) wrap that back into an `IValue` and put in on the stack.\n\nSomewhere in that sequence of operations, something bad happens and we segfault. Fixing up the signature of `to_functional_tensor` to be `List<Tensor> to_functional_tensor(List<Tensor>)` fixes the bug. I have a feeling that there's a latent TensorList-related bug in the boxing/unboxing logic that made this worse, but I'm okay to stick with my narrow fix for now.\n\nAdditionally tested by running `pytest test/test_ops.py test/test_vmap.py -v -k block_diag` on top of this PR: https://github.com/pytorch/functorch/pull/235\n\nTest Plan: Imported from OSS\n\nReviewed By: zou3519\n\nDifferential Revision: D32448258\n\nPulled By: bdhirsh\n\nfbshipit-source-id: 3b2b6c7cd5e4c29533d0502f24272d826bfe03c1", "pr_number": "68395", "files_changed": ["aten/src/ATen/FunctionalTensorWrapper.cpp", "aten/src/ATen/FunctionalTensorWrapper.h", "test/test_functionalization.py"], "labels": ["Merged", "cla signed", "ciflow/default"]}, "9d9ca88f5c": {"title": "[predictor][trt] Expose more CUDA/CuDNN info to at::Context and BC stage 1 (#68146)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/68146\n\nExpose more CUDA/CuDNN info to at::Context\n\nTest Plan: CI; lint;\n\nReviewed By: houseroad\n\nDifferential Revision: D32264935\n\nfbshipit-source-id: ad43d5d245dba4a054e09346240414159832585e", "pr_number": "68146", "files_changed": ["aten/src/ATen/Context.h"], "labels": ["fb-exported", "Merged", "cla signed", "ciflow/default"]}, "9807787135": {"title": "`scatter_reduce` (#68115)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/63780\n\nBasic functionality of a `scatter_reduce` algorithm with `reduce=\"sum\"`:\n\n* `scatter_reduce` is named as `scatter_reduce2` due to compiling issues\n* It currently re-uses functionality from `scatter_add`\n* Tests are missing: WIP\n\nThe error when the `scatter_reduce` naming is used:\n```\nIn file included from aten/src/ATen/core/TensorBody.h:3,\n                 from ../aten/src/ATen/core/Tensor.h:3,\n                 from ../aten/src/ATen/DeviceGuard.h:4,\n                 from ../aten/src/ATen/ATen.h:11,\n                 from aten/src/ATen/native/cpu/CopyKernel.cpp.DEFAULT.cpp:1:\naten/src/ATen/Operators.h:13949:18: error: redefinition of \u2018struct at::_ops::scatter_reduce\u2019\n13949 | struct TORCH_API scatter_reduce {\n      |                  ^~~~~~~~~~~~~~\naten/src/ATen/Operators.h:13817:18: note: previous definition of \u2018struct at::_ops::scatter_reduce\u2019\n13817 | struct TORCH_API scatter_reduce {\n      |                  ^~~~~~~~~~~~~~\naten/src/ATen/Operators.h:13960:18: error: redefinition of \u2018struct at::_ops::scatter_reduce_out\u2019\n13960 | struct TORCH_API scatter_reduce_out {\n      |                  ^~~~~~~~~~~~~~~~~~\naten/src/ATen/Operators.h:13839:18: note: previous definition of \u2018struct at::_ops::scatter_reduce_out\u2019\n13839 | struct TORCH_API scatter_reduce_out {\n      |                  ^~~~~~~~~~~~~~~~~~\nIn file included from ../aten/src/ATen/core/Tensor.h:3,\n                 from ../aten/src/ATen/DeviceGuard.h:4,\n                 from ../aten/src/ATen/ATen.h:11,\n                 from aten/src/ATen/native/cpu/CopyKernel.cpp.DEFAULT.cpp:1:\naten/src/ATen/core/TensorBody.h: In member function \u2018at::Tensor at::Tensor::scatter_reduce(int64_t, const at::Tensor&, c10::string_view, c10::optional<long int>) const\u2019:\naten/src/ATen/core/TensorBody.h:3976:83: error: cannot convert \u2018c10::string_view\u2019 {aka \u2018c10::basic_string_view<char>\u2019} to \u2018const at::Tensor&\u2019\n 3976 |     return at::_ops::scatter_reduce::call(const_cast<Tensor&>(*this), dim, index, reduce, output_size);\n      |                                                                                   ^~~~~~\n      |                                                                                   |\n      |                                                                                   c10::string_view {aka c10::basic_string_view<char>}\nIn file included from aten/src/ATen/core/TensorBody.h:3,\n                 from ../aten/src/ATen/core/Tensor.h:3,\n                 from ../aten/src/ATen/DeviceGuard.h:4,\n                 from ../aten/src/ATen/ATen.h:11,\n                 from aten/src/ATen/native/cpu/CopyKernel.cpp.DEFAULT.cpp:1:\naten/src/ATen/Operators.h:13824:109: note:   initializing argument 4 of \u2018static at::Tensor at::_ops::scatter_reduce::call(const at::Tensor&, int64_t, const at::Tensor&, const at::Tensor&, c10::string_view)\u2019\n13824 |   static at::Tensor call(const at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Tensor & src, c10::string_view reduce);\n      |                                                                                          ~~~~~~~~~~~~~~~~~~~^~~\nIn file included from ../aten/src/ATen/ATen.h:15,\n                 from aten/src/ATen/native/cpu/CopyKernel.cpp.DEFAULT.cpp:1:\naten/src/ATen/Functions.h: In function \u2018at::Tensor at::scatter_reduce(const at::Tensor&, int64_t, const at::Tensor&, c10::string_view, c10::optional<long int>)\u2019:\naten/src/ATen/Functions.h:7119:61: error: cannot convert \u2018c10::string_view\u2019 {aka \u2018c10::basic_string_view<char>\u2019} to \u2018const at::Tensor&\u2019\n 7119 |     return at::_ops::scatter_reduce::call(self, dim, index, reduce, output_size);\n      |                                                             ^~~~~~\n      |                                                             |\n      |                                                             c10::string_view {aka c10::basic_string_view<char>}\nIn file included from aten/src/ATen/core/TensorBody.h:3,\n                 from ../aten/src/ATen/core/Tensor.h:3,\n                 from ../aten/src/ATen/DeviceGuard.h:4,\n                 from ../aten/src/ATen/ATen.h:11,\n                 from aten/src/ATen/native/cpu/CopyKernel.cpp.DEFAULT.cpp:1:\naten/src/ATen/Operators.h:13824:109: note:   initializing argument 4 of \u2018static at::Tensor at::_ops::scatter_reduce::call(const at::Tensor&, int64_t, const at::Tensor&, const at::Tensor&, c10::string_view)\u2019\n13824 |   static at::Tensor call(const at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Tensor & src, c10::string_view reduce);\n      |                                                                                          ~~~~~~~~~~~~~~~~~~~^~~\nIn file included from ../aten/src/ATen/ATen.h:15,\n                 from aten/src/ATen/native/cpu/CopyKernel.cpp.DEFAULT.cpp:1:\naten/src/ATen/Functions.h: In function \u2018at::Tensor& at::scatter_reduce_out(at::Tensor&, const at::Tensor&, int64_t, const at::Tensor&, c10::string_view, c10::optional<long int>)\u2019:\naten/src/ATen/Functions.h:7124:65: error: cannot convert \u2018c10::string_view\u2019 {aka \u2018c10::basic_string_view<char>\u2019} to \u2018const at::Tensor&\u2019\n 7124 |     return at::_ops::scatter_reduce_out::call(self, dim, index, reduce, output_size, out);\n      |                                                                 ^~~~~~\n      |                                                                 |\n      |                                                                 c10::string_view {aka c10::basic_string_view<char>}\nIn file included from aten/src/ATen/core/TensorBody.h:3,\n                 from ../aten/src/ATen/core/Tensor.h:3,\n                 from ../aten/src/ATen/DeviceGuard.h:4,\n                 from ../aten/src/ATen/ATen.h:11,\n                 from aten/src/ATen/native/cpu/CopyKernel.cpp.DEFAULT.cpp:1:\naten/src/ATen/Operators.h:13846:111: note:   initializing argument 4 of \u2018static at::Tensor& at::_ops::scatter_reduce_out::call(const at::Tensor&, int64_t, const at::Tensor&, const at::Tensor&, c10::string_view, at::Tensor&)\u2019\n13846 |   static at::Tensor & call(const at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Tensor & src, c10::string_view reduce, at::Tensor & out);\n      |                                                                                            ~~~~~~~~~~~~~~~~~~~^~~\nIn file included from ../aten/src/ATen/ATen.h:15,\n                 from aten/src/ATen/native/cpu/CopyKernel.cpp.DEFAULT.cpp:1:\naten/src/ATen/Functions.h: In function \u2018at::Tensor& at::scatter_reduce_outf(const at::Tensor&, int64_t, const at::Tensor&, c10::string_view, c10::optional<long int>, at::Tensor&)\u2019:\naten/src/ATen/Functions.h:7129:65: error: cannot convert \u2018c10::string_view\u2019 {aka \u2018c10::basic_string_view<char>\u2019} to \u2018const at::Tensor&\u2019\n 7129 |     return at::_ops::scatter_reduce_out::call(self, dim, index, reduce, output_size, out);\n      |                                                                 ^~~~~~\n      |                                                                 |\n      |                                                                 c10::string_view {aka c10::basic_string_view<char>}\nIn file included from aten/src/ATen/core/TensorBody.h:3,\n                 from ../aten/src/ATen/core/Tensor.h:3,\n                 from ../aten/src/ATen/DeviceGuard.h:4,\n                 from ../aten/src/ATen/ATen.h:11,\n                 from aten/src/ATen/native/cpu/CopyKernel.cpp.DEFAULT.cpp:1:\naten/src/ATen/Operators.h:13846:111: note:   initializing argument 4 of \u2018static at::Tensor& at::_ops::scatter_reduce_out::call(const at::Tensor&, int64_t, const at::Tensor&, const at::Tensor&, c10::string_view, at::Tensor&)\u2019\n13846 |   static at::Tensor & call(const at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Tensor & src, c10::string_view reduce, at::Tensor & out);\n      |                                                                                            ~~~~~~~~~~~~~~~~~~~^~~\nIn file included from aten/src/ATen/NativeFunctions.h:6,\n                 from ../aten/src/ATen/TensorIndexing.h:12,\n                 from ../aten/src/ATen/ATen.h:20,\n                 from aten/src/ATen/native/cpu/CopyKernel.cpp.DEFAULT.cpp:1:\naten/src/ATen/NativeMetaFunctions.h: At global scope:\naten/src/ATen/NativeMetaFunctions.h:496:18: error: redefinition of \u2018struct at::meta::structured_scatter_reduce\u2019\n  496 | struct TORCH_API structured_scatter_reduce : public at::impl::MetaBase {\n      |                  ^~~~~~~~~~~~~~~~~~~~~~~~~\naten/src/ATen/NativeMetaFunctions.h:481:18: note: previous definition of \u2018struct at::meta::structured_scatter_reduce\u2019\n  481 | struct TORCH_API structured_scatter_reduce : public at::impl::MetaBase {\n      |                  ^~~~~~~~~~~~~~~~~~~~~~~~~\nninja: build stopped: subcommand failed.\n```\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/68115\n\nReviewed By: albanD\n\nDifferential Revision: D32488450\n\nPulled By: cpuhrsch\n\nfbshipit-source-id: 65e79c6d0555c0d5715535bb52aade8d5fcd9722", "pr_number": "68115", "files_changed": ["aten/src/ATen/native/TensorAdvancedIndexing.cpp", "aten/src/ATen/native/native_functions.yaml", "test/test_torch.py", "torch/overrides.py"], "labels": ["open source", "Merged", "cla signed", "ciflow/default"]}, "dbbb02474b": {"title": "[GPU host alloc] Fast path for size 0 malloc (#68532)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/68532\n\nDiff to better handle size 0 pinned memory allocation requests.\n----\n### Behavior before fix\nThe very first size 0 malloc comes in. It will create a block with `{key: 0, value: Block(0, 0, true)}`.\n\nAnother size 0 malloc comes in.\nIt will either 1) get a block with size > 0 (which is a waste of pinned memory) or 2) call `cudaHostAlloc()` with size 0 to eventually get *ptr=0.\nNote that this block is *not registered* to the block pool because we have a duplicate entry (and that's why we will keep wasting size > 0 pinned memory block, if `available.empty() == false`).\n\n----\n### Behavior after fix\n\nLet `malloc()` simply return a nullptr (0).\nThis avoids wasting valid size > 0 blocks as well as save the calls to `cudaHostAlloc()` which is expensive.\nThis is also safe since `free()` simply returns success for nullptrs.\n\n-----\n\nTest Plan: Unit tests.\n\nReviewed By: yinghai\n\nDifferential Revision: D32487522\n\nfbshipit-source-id: 6140cab54ff5a34ace7d046f218fb32805c692c0", "pr_number": "68532", "files_changed": ["aten/src/ATen/cuda/CachingHostAllocator.cpp"], "labels": ["fb-exported", "Merged", "cla signed", "ciflow/default"]}, "f0e2ad5037": {"title": "Stop warning spamming about vmap in gradcheck (#68586)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/68586\n\nWe updated the vmap warnings to be more descriptive in\nhttps://github.com/pytorch/pytorch/pull/67347 . However, gradcheck does\nsome warning squashing that matches on the warning message and we didn't\nupdate that. This PR updates the warning squashing in gradcheck.\n\nTest Plan: - check logs\n\nReviewed By: albanD\n\nDifferential Revision: D32530259\n\nPulled By: zou3519\n\nfbshipit-source-id: 9db380b57c38b3b72cbdb29574f71dbfe71e90d1", "pr_number": "68586", "files_changed": ["torch/autograd/gradcheck.py"], "labels": ["Merged", "cla signed", "ciflow/default"]}, "b1aa45a8a7": {"title": "Fix `_make_wrapper_subclass`'s storage_offset handling (#68268)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/68268\n\nPreviously, `_make_wrapper_subclass` ignored the storage offset it was\npassed. This PR fixes that by updating TensorMaker::computeStorageSize()\nand TensorMaker::make_tensor() to take into account storage_offset.\n\nTest Plan: - added test\n\nReviewed By: albanD, bdhirsh\n\nDifferential Revision: D32396330\n\nPulled By: zou3519\n\nfbshipit-source-id: 2c85bc4066044fe6cb5ab0fc192de6c9069855fd", "pr_number": "68268", "files_changed": ["aten/src/ATen/templates/Functions.cpp", "test/test_python_dispatch.py"], "labels": ["Merged", "cla signed", "ciflow/default"]}, "a2d187a672": {"title": "[BE] MapAllocator: report map error on Linux (#68545)", "body": "Summary:\nAdd `, strerror(errno), \" (\", errno, \")\"`  suffix to TORCH_CHECK messages that report failures from POSIX calls\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/68545\n\nReviewed By: ngimel\n\nDifferential Revision: D32509300\n\nPulled By: malfet\n\nfbshipit-source-id: 1d7792d07e3a1184d2d54d137e6a9105dbab7d4c", "pr_number": "68545", "files_changed": ["aten/src/ATen/MapAllocator.cpp"], "labels": ["Merged", "cla signed", "ciflow/default"]}, "533e72e0a4": {"title": "Fix DLPack CUDA stream convention (#67618)", "body": "Summary:\nApparently for the array API, cuda default stream and per thread stream should be 1 and 2 instead of 0 and 1:\n\nhttps://data-apis.org/array-api/latest/API_specification/array_object.html?dlpack-self-stream-none#dlpack-self-stream-none.\n\nThis caused a problem in the interop with CuPy https://github.com/cupy/cupy/pull/5970#discussion_r739912926.\n\ncc rgommers leofang mruberry\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/67618\n\nReviewed By: albanD\n\nDifferential Revision: D32521805\n\nPulled By: mruberry\n\nfbshipit-source-id: 95777e4014e5edf1f88ba10adc03c6e34c13248d", "pr_number": "67618", "files_changed": ["test/test_torch.py", "torch/utils/dlpack.py"], "labels": ["triaged", "open source", "Merged", "cla signed", "ciflow/default"]}, "9ce3c630ba": {"title": "[Docs] Mention `torch.bfloat16` in `torch.finfo` (#68496)", "body": "Summary:\nhttps://pytorch.org/docs/master/type_info.html#torch.torch.finfo seems to miss `torch.bfloat16`.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/68496\n\nReviewed By: mruberry\n\nDifferential Revision: D32538806\n\nPulled By: ngimel\n\nfbshipit-source-id: 1296b3eb34d024cfc7d85cf53efe771ee9f98ea2", "pr_number": "68496", "files_changed": ["docs/source/type_info.rst"], "labels": ["open source", "Merged", "cla signed", "ciflow/default"]}, "ff125a3624": {"title": "Minor changes in documentation (#68557)", "body": "Summary:\nFixed some small typos\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/68557\n\nReviewed By: mruberry\n\nDifferential Revision: D32538749\n\nPulled By: ngimel\n\nfbshipit-source-id: 09a9cd4031463b6a40d7307bd8fcb7d364444ac3", "pr_number": "68557", "files_changed": ["docs/source/jit_language_reference.rst", "docs/source/jit_language_reference_v2.rst", "docs/source/notes/cuda.rst", "docs/source/package.rst", "docs/source/pipeline.rst", "docs/source/quantization.rst", "docs/source/tensor_view.rst"], "labels": ["open source", "Merged", "cla signed", "ciflow/default"]}, "758d7dea9c": {"title": "torch.monitor - Initial C++ Stats (#68074)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/68074\n\nThis is the first step of many PRs towards implementing the `torch.monitor` RFC https://github.com/pytorch/rfcs/pull/30\n\nThis defines the aggregation types, the `Stat` class and provides some simple collection of the stats.\n\nThis doesn't match the RFC exactly as it incorporates some of the comments on the RFC as well as a few changes for performance.\n\nChanges:\n* added window_size to the stats. If specified it will always compute the stat using the `window_size` number of values. If there aren't enough values within that window it reports the previous stats.\n* This doesn't include the push metrics yet (will be coming).\n  After more discussion it looks like the best way to handle this is to support a hybrid where the metric can set how frequently it'll be logged. For fixed window_size metrics it'll be logged each time it hits the window size. This will allow performant counters as well as lower frequency push counters (window_size=1).\n\nPerformance considerations:\n* Updating the stats acquires a lock on that Stat object. This should be performant unless there's many-many threads writing to the same stat. Single thread will typically use futex so should be quite fast.\n* Adding/removing/fetching all stats sets a global lock on the stat list -- this shouldn't be an issue since these events happen infrequently.\n* Fetching stats accesses one stat at a time instead of a global lock. This means the exported values are linearizable but not serializable across multiple stats but I don't expect this to be an issue.\n\nNext steps:\n1. Add StatCollector interface for push style metrics\n1. Add pybind interfaces to expose to Python\n1. Add default metric providers\n1. Integrate into Kineto trace view\n\nTest Plan:\nbuck test //caffe2/test/cpp/monitor:monitor\n\nCI\n\nReviewed By: kiukchung\n\nDifferential Revision: D32266032\n\nfbshipit-source-id: dab8747b4712f5dba5644387817a3a0fda18b66a", "pr_number": "68074", "files_changed": ["test/cpp/monitor/test_counters.cpp", "tools/build_variables.bzl", "torch/csrc/monitor/counters.cpp", "torch/csrc/monitor/counters.h"], "labels": ["fb-exported", "Merged", "cla signed", "ciflow/default"]}, "18312313c4": {"title": "[Profiler] Add missing guards (#65812)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65812\n\nMultiple threads are recording events to a shared activity buffer and the buffer is at some point transferred to libkineto.\nThe access to and the transfer of the buffer needs to be done under lock.\n\nReviewed By: leitian, xw285cornell\n\nDifferential Revision: D31220061\n\nfbshipit-source-id: f11c879df1b55aa9068187e600730bb0e5e5455f", "pr_number": "65812", "files_changed": ["torch/csrc/autograd/profiler_kineto.cpp"], "labels": ["fb-exported", "Merged", "cla signed", "ciflow/default"]}, "f82f14de17": {"title": "[libkineto] Refactor 4/n: Simplify activity logger step 2/3 (#68329)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/68329\n\nPull Request resolved: https://github.com/pytorch/kineto/pull/466\n\n1. Generalize ChromeTraceLogger::handleGenericActivity to enable it to handle Cuda runtime activities as well as the Roctracer generic activities.\nThis primarily involves enabling generic support for CPU -> GPU flows.\n\n2. In the event of out-of-order GPU activities (an issue with Cuda11.0, likely fixed in later versions), no longer remove them but print warnings. Another diff will add these warnings to the metadata section.\n\nReviewed By: briancoutinho\n\nDifferential Revision: D31624496\n\nfbshipit-source-id: dab04b3e3c0dd6799496ac87f837363de79eea25", "pr_number": "68329", "files_changed": ["torch/csrc/autograd/profiler_kineto.cpp"], "labels": ["fb-exported", "cla signed", "ciflow/default"]}, "4bcff4733d": {"title": "Add OpInfos for parcel Elementwise Binary II (#68085)", "body": "Summary:\nAdds OpInfos for `torch.lcm`, `torch.gcd`, `torch.heaviside`, `torch.bitwise_or`, `torch.bitwise_xor`, `torch.isclose`.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/68085\n\nReviewed By: ngimel\n\nDifferential Revision: D32533310\n\nPulled By: saketh-are\n\nfbshipit-source-id: 1616ebec61164cd1b44672f36220787a878b96a4", "pr_number": "68085", "files_changed": ["torch/testing/_internal/common_device_type.py", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "cla signed", "ciflow/slow", "ciflow/default", "ciflow/all"]}, "f74779e403": {"title": "[android] Lite interpreter naming for android nightly publishing (#68651)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/68651\n\nTest Plan: Imported from OSS\n\nReviewed By: linbinyu\n\nDifferential Revision: D32564796\n\nPulled By: IvanKobzarev\n\nfbshipit-source-id: 57847bfb2778433cfb02ad7a5a79ae30a6b438c1", "pr_number": "68651", "files_changed": ["android/pytorch_android/gradle.properties", "android/pytorch_android_torchvision/gradle.properties"], "labels": ["Merged", "cla signed", "ciflow/default"]}, "f99f5ee088": {"title": "add support for None in assert_close (#67795)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/67795\n\nCloses #61035.\n\nTest Plan: Imported from OSS\n\nReviewed By: ngimel\n\nDifferential Revision: D32532207\n\nPulled By: mruberry\n\nfbshipit-source-id: 6a2b4245e0effce4ddea7d89eca63e3b163951a7", "pr_number": "67795", "files_changed": ["test/test_testing.py", "torch/testing/_comparison.py"], "labels": ["open source", "Merged", "cla signed", "module: testing", "ciflow/default"]}, "e358c49a5b": {"title": "Add OpInfo test and fix a couple cases (#66294)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66294\n\nIn this PR:\n- OpInfo for forward AD now checks batched forward grad when `op.check_batched_grad=True`\n- Adds setting to disable the test for individual ops `check_batched_forward_grad` and disable for the ops here: https://github.com/pytorch/pytorch/issues/66357\n\nFixes some more failures:\n- Make Forward AD metadata less strict by allowing stride to differ when size is 1\n- Fix sum batching rule when logical tensor is a scalar and dim is unspecified\n- Batching rule for `_reshape_alias`\n- ~Batching rules now preserve storage offset for view operator that return non-zero storage offset~ (moved to previous PR)\n\nTest Plan: Imported from OSS\n\nReviewed By: zou3519, albanD\n\nDifferential Revision: D31842020\n\nPulled By: soulitzer\n\nfbshipit-source-id: 3517a8fb9d6291fccb53c0b1631eab5bbb24ebd1", "pr_number": "66294", "files_changed": ["aten/src/ATen/BatchingRegistrations.cpp", "test/test_autograd.py", "test/test_ops.py", "test/test_vmap.py", "torch/autograd/gradcheck.py", "torch/csrc/autograd/autograd_meta.cpp", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["Merged", "cla signed", "ciflow/default"]}, "5456d8c8f3": {"title": "Add vectorized Jacobian and Hessian computation with forward AD (#67041)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/67041\n\nOriginal PR here: https://github.com/pytorch/pytorch/pull/62246 (The old PR does more things, but now that's split across this stack)\n\nThis PR:\n- Adds \"jacfwd\" and \"hessian_fwdrev\"\n- Modifies existing tests to also test the `forward_ad=True` case\n\nTest Plan: Imported from OSS\n\nReviewed By: gchanan, zou3519\n\nDifferential Revision: D32314424\n\nPulled By: soulitzer\n\nfbshipit-source-id: 785b0e39162b93dc3b3cb9413233447152eddd53", "pr_number": "67041", "files_changed": ["test/test_autograd.py", "torch/autograd/functional.py"], "labels": ["Merged", "cla signed", "ciflow/default"]}, "993b7a2052": {"title": "Remove doubly nested anonymous namespace (#68555)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/68555\n\nThe outer namespace is already anonymous, so this is not necessary.\n\nTest Plan: Imported from OSS\n\nReviewed By: dagitses\n\nDifferential Revision: D32565941\n\nPulled By: malfet\n\nfbshipit-source-id: 4daf1c46b25ff68e748e6c834c63d759ec6fde4f", "pr_number": "68555", "files_changed": ["aten/src/ATen/cpu/vec/vec_base.h"], "labels": ["open source", "Merged", "cla signed", "ciflow/default"]}, "578507cb7b": {"title": "Fix nanmedian result using more CUDA memory than necessary (#68591)", "body": "Summary:\nCUDA's `at::nanmedian` creates a sorted copy of the array, then indexes into it to create a single element view. This view necessarily keeps the entire `sorted` tensor's storage alive which can be avoided by returning a copy, which is what `at::median` does indirectly via `at::where`.\n\nThis also changes the index variable `k` to be a simple `int64_t` instead of the CUDA tensor that was used before. This saves the  additional host and device operations from calling `Tensor`'s `operator -` which helps balance out the cost of the `clone` added here.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/68591\n\nReviewed By: dagitses\n\nDifferential Revision: D32538538\n\nPulled By: ngimel\n\nfbshipit-source-id: abe9888f80cf9d24d50a83da756e649af1f6ea3b", "pr_number": "68591", "files_changed": ["aten/src/ATen/native/cuda/Sorting.cpp"], "labels": ["triaged", "open source", "Merged", "cla signed", "ciflow/default"]}, "e554d8b89c": {"title": "Fix retry on connect failure decorator (#68600)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/68541 by checking string contains instead of exact eror\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/68600\n\nReviewed By: dagitses, H-Huang\n\nDifferential Revision: D32535592\n\nPulled By: rohan-varma\n\nfbshipit-source-id: 864c3e3c6831f2351c2949b2348af4f48a308522", "pr_number": "68600", "files_changed": ["torch/testing/_internal/common_utils.py"], "labels": ["Merged", "cla signed", "ciflow/default"]}, "95f4cd0ba9": {"title": "Implement topk with sort for some cases (#68632)", "body": "Summary:\nBenchmark that compares original implementation and the sort implementation (this code should run on a branch without this patch):\n```python\nimport torch\nimport timeit\n\ndef tune_dtype(f):\n    def ret(*args, **kwargs):\n        for dtype in [torch.int8, torch.half, torch.float, torch.double]:\n            f(*args, **kwargs, dtype=dtype)\n    return ret\n\ndef tune_slice(f):\n    def ret(*args, **kwargs):\n        slice = 1\n        while slice <= 256:\n            f(*args, **kwargs, slice=slice)\n            slice *= 2\n    return ret\n\ndef tune_slice_size(f):\n    def ret(*args, **kwargs):\n        slice_size = 1\n        while slice_size <= 1_000_000:\n            f(*args, **kwargs, slice_size=slice_size)\n            slice_size *= 10\n    return ret\n\ndef tune_k(f):\n    def ret(*args, slice_size, **kwargs):\n        k = 1\n        while k <= slice_size:\n            f(*args, **kwargs, k=k, slice_size=slice_size)\n            k *= 10\n    return ret\n\ndef topk_with_sort(tensor, k, dim=-1, largest=True):\n    values, indices = tensor.sort(dim=dim, descending=largest)\n    return values.narrow(dim, 0, k), indices.narrow(dim, 0, k)\n\ndef run50sync(f):\n    for _ in range(50):\n        f()\n    torch.cuda.synchronize()\n\ndef warmup():\n    N = 1000000\n    for i in range(1, N // 10000):\n        torch.randn(i, device='cuda')\n\ndef benchmark_one(slice, slice_size, k, dtype):\n    input_ = torch.empty((slice, slice_size), dtype=dtype, device=\"cuda\").random_()\n    torch.cuda.synchronize()\n    time = timeit.timeit(lambda: run50sync(lambda: torch.topk(input_, k, dim=1)), number=1)\n    torch.cuda.synchronize()\n    time_sort = timeit.timeit(lambda: run50sync(lambda: topk_with_sort(input_, k, dim=1)), number=1)\n    method = \"orig\" if time < time_sort else \"sort\"\n    speedup = time / time_sort\n    print(f\"(dtype={dtype}, slice={slice}, slice_size={slice_size}, k={k}) -> (method={method}, speedup={speedup})\")\n\nif __name__ == \"__main__\":\n    warmup()\n    tune_dtype(tune_slice(tune_slice_size(tune_k(benchmark_one))))()\n\n```\nBenchmark result see next comment.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/68632\n\nReviewed By: dagitses\n\nDifferential Revision: D32566233\n\nPulled By: ngimel\n\nfbshipit-source-id: f7a508176ef3685b491048c4a6562121c60b8b2a", "pr_number": "68632", "files_changed": ["aten/src/ATen/native/cuda/TensorTopK.cpp", "test/test_sort_and_select.py"], "labels": ["triaged", "open source", "Merged", "cla signed", "ciflow/default"]}, "148f323856": {"title": "Revert D32541986: [pytorch][PR] [opinfo] use dtypes instead of dtypesIfCPU", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD32541986 (https://github.com/pytorch/pytorch/commit/d2a90f91bc18b4431dac1a6ba748420b2ca6334c)\n\nOriginal commit changeset: 793d7d22c3ec\n\nfbshipit-source-id: c60c4be3416f6feb658b5da1bdf75f0cbe6bee24", "pr_number": null, "files_changed": ["torch/testing/_internal/common_methods_invocations.py", "torch/testing/_internal/opinfo_helper.py"], "labels": []}, "ddc22ea3b2": {"title": "[Dist CI][BE] test_c10d_nccl run in subprocess (#68503)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/68503\n\nPer title\nghstack-source-id: 143928768\n\nTest Plan: CI\n\nReviewed By: H-Huang\n\nDifferential Revision: D32484990\n\nfbshipit-source-id: 6682f46256af0da5153e5087a91a7044156dd17f", "pr_number": "68503", "files_changed": ["test/run_test.py"], "labels": ["Merged", "cla signed", "ciflow/default"]}, "9554ebe44e": {"title": "[Dist CI][BE] c10d gloo tests run in subprocess (#68504)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/68504\n\nPer title\nghstack-source-id: 143928767\n\nTest Plan: CI\n\nReviewed By: H-Huang\n\nDifferential Revision: D32485100\n\nfbshipit-source-id: a55687aea4af69e3830aee6f0278550c72f142c2", "pr_number": "68504", "files_changed": ["test/run_test.py"], "labels": ["Merged", "cla signed", "ciflow/default"]}, "a2e35e167b": {"title": "refactor: update f-string for swa.utils.py (#68718)", "body": "Summary:\n_ Update some old-style formats to f-string, for whole and coherent consistency.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/68718\n\nReviewed By: jbschlosser\n\nDifferential Revision: D32593746\n\nPulled By: albanD\n\nfbshipit-source-id: fcc17958f8af6a3260beca883bc1065f019dcf0e", "pr_number": "68718", "files_changed": ["torch/optim/swa_utils.py"], "labels": ["open source", "Merged", "cla signed", "ciflow/default"]}, "74e6d2ce67": {"title": "fix typos in jit_language_reference.rst (#68706)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/68700\n\n- indent problem\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/68706\n\nReviewed By: mruberry\n\nDifferential Revision: D32598916\n\nPulled By: jbschlosser\n\nfbshipit-source-id: 42af216e83fb48bbd311fc3d41fc3e8f5a2fef08", "pr_number": "68706", "files_changed": ["docs/source/jit_language_reference.rst"], "labels": ["open source", "Merged", "cla signed", "ciflow/default"]}, "3282386aa4": {"title": "Added additional string to search cpu flags for vnni detection (#67686)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/67685\n\ncc jerryzh168 jianyuh raghuramank100 jamesr66a vkuzo\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/67686\n\nReviewed By: ejguan\n\nDifferential Revision: D32109038\n\nPulled By: malfet\n\nfbshipit-source-id: 3ea6e4cc1aa82831fd6277129a67c8241a5591a5", "pr_number": "67686", "files_changed": ["torch/testing/_internal/common_utils.py"], "labels": ["oncall: quantization", "triaged", "open source", "Merged", "cla signed", "ciflow/default"]}, "c0e6dc9ac7": {"title": "[pytorch] Fix loading from checkpoint after \"maximize\" flag was introduced in SGD (#68733)", "body": "Summary:\nAfter 'maximize' flag was introduced in  https://github.com/pytorch/pytorch/issues/46480 some jobs fail because they resume training from the checkpoints.\n\nAfter we load old checkpoints we will get an error during optimizer.step() call during backward pass in [torch/optim/sgd.py\", line 129] because there is no key 'maximize' in the parameter groups of the SGD.\n\nTo circumvent this I add a default value `group.setdefault('maximize', False)` when the optimizer state is restored.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/68733\n\nReviewed By: albanD\n\nDifferential Revision: D32480963\n\nPulled By: asanakoy\n\nfbshipit-source-id: 4e367fe955000a6cb95090541c143a7a1de640c2", "pr_number": "68733", "files_changed": ["torch/optim/sgd.py"], "labels": ["fb-exported", "Merged", "cla signed", "ciflow/default"]}, "8fb9ce4927": {"title": "Update Documentation to Make CUDA Call Explicit (#67973)", "body": "Summary:\nI am clarifying in the docs to make the call to cudaStreamWaitEvent explicit.\n\nFixes https://github.com/pytorch/pytorch/issues/67866\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/67973\n\nReviewed By: mruberry\n\nDifferential Revision: D32620261\n\nPulled By: ngimel\n\nfbshipit-source-id: 1fc8beb2062baaddb013ea4d7b10da2baa10f15e", "pr_number": "67973", "files_changed": ["torch/cuda/streams.py"], "labels": ["triaged", "open source", "Merged", "cla signed", "ciflow/default"]}, "31d36fd35d": {"title": "fix sccache issue on Windows CPU (#68870)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/68796\n\n```\n2021-11-24T10:12:40.7634007Z Compile requests                   4312\n2021-11-24T10:12:40.7634484Z Compile requests executed          4300\n2021-11-24T10:12:40.7634823Z Cache hits                         4227\n2021-11-24T10:12:40.7635122Z Cache hits (C/C++)                 4227\n2021-11-24T10:12:40.7636139Z Cache misses                         62\n2021-11-24T10:12:40.7636930Z Cache misses (C/C++)                 62\n2021-11-24T10:12:40.7637333Z Cache timeouts                        0\n2021-11-24T10:12:40.7637839Z Cache read errors                     0\n2021-11-24T10:12:40.7638161Z Forced recaches                       0\n2021-11-24T10:12:40.7638489Z Cache write errors                    0\n2021-11-24T10:12:40.7638828Z Compilation failures                  1\n2021-11-24T10:12:40.7639180Z Cache errors                         10\n2021-11-24T10:12:40.7639490Z Cache errors (C/C++)                 10\n2021-11-24T10:12:40.7639856Z Non-cacheable compilations            0\n2021-11-24T10:12:40.7640244Z Non-cacheable calls                   0\n2021-11-24T10:12:40.7640601Z Non-compilation calls                12\n2021-11-24T10:12:40.7640987Z Unsupported compiler calls            0\n2021-11-24T10:12:40.7641426Z Average cache write               0.104 s\n2021-11-24T10:12:40.7641763Z Average cache read miss           6.000 s\n2021-11-24T10:12:40.7642110Z Average cache read hit            0.046 s\n2021-11-24T10:12:40.7642485Z Failed distributed compilations       0\n```\nhttps://github.com/pytorch/pytorch/runs/4310176911?check_suite_focus=true\n\ncc seemethere malfet pytorch/pytorch-dev-infra\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/68870\n\nReviewed By: ejguan\n\nDifferential Revision: D32646289\n\nPulled By: janeyx99\n\nfbshipit-source-id: bf04446439e55a4ccaf9ce7c77812752ca717a7c", "pr_number": "68870", "files_changed": ["CMakeLists.txt"], "labels": ["module: ci", "triaged", "open source", "Merged", "cla signed", "ciflow/default"]}, "b69155f754": {"title": "Avoid dtype mismatch error in `torch.save` if storages are unallocated (#68787)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/58970\n\ncc mruberry\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/68787\n\nReviewed By: mruberry\n\nDifferential Revision: D32617425\n\nPulled By: anjali411\n\nfbshipit-source-id: fe7f2374e4ef4428346a0a202cae8e0d382e03ab", "pr_number": "68787", "files_changed": ["test/test_serialization.py", "torch/serialization.py"], "labels": ["module: serialization", "triaged", "open source", "Merged", "cla signed", "ciflow/default"]}, "cf54416925": {"title": "Add docs entry for `adjoint`. (#68869)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/68869\n\nAs per title.\n\ncc brianjo mruberry anjali411\n\nTest Plan: Imported from OSS\n\nReviewed By: ejguan\n\nDifferential Revision: D32647456\n\nPulled By: anjali411\n\nfbshipit-source-id: 2cb053a6884e2b22d3decc058e86d10f355fcb84", "pr_number": "68869", "files_changed": ["docs/source/torch.rst", "torch/_torch_docs.py"], "labels": ["module: docs", "open source", "Merged", "cla signed", "ciflow/default"]}, "d44e610efa": {"title": "[CUDA Pinned Memory] Event recording with non-blocking copies should track the storage context, not the tensor data pointer (#68749)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/68749\n\nThe logic for asynchronous copies (either HtoD or DtoH) using cudaMemcpyAsync relies on recording an event with the caching host allocator to notify it that a given allocation has been used on a stream - and thus it should wait for that stream to proceed before reusing the host memory.\n\nThis tracking is based on the allocator maintaining a map from storage allocation pointers to some state.\n\nIf we try to record an event for a pointer we don't understand, we will silently drop the event and ignore it (https://github.com/pytorch/pytorch/blob/9554ebe44e6e73dc75105d4935d41e626e03299b/aten/src/ATen/cuda/CachingHostAllocator.cpp#L171-L175).\n\nThus, if we use the data_ptr of a Tensor instead of the storage allocation, then reasonable code can lead to incorrectness due to missed events.\n\nOne way this can occur is simply by slicing a tensor into sub-tensors - which have different values of `data_ptr()` but share the same storage, for example:\n\n```\nimage_batch = torch.randn(M, B, C, H, W).pin_memory()\nfor m in range(M):\n  sub_batch = image_batch[m].cuda(non_blocking=True)\n  # sub_batch.data_ptr() != image_batch.data_ptr() except for m == 0.\n  # however, sub_batch.storage().data_ptr() == image_batch.storage().data_ptr() always.\n```\n\nTherefore, we instead use the storage context pointer when recording events, as this is the same state that is tracked by the caching allocator itself. This is a correctness fix, although it's hard to determine how widespread this issue is.\n\nUsing the storage context also allows us to use a more efficient structure internally to the caching allocator, which will be sent in future diffs.\n\nTest Plan: Test added which demonstrates the issue, although it's hard to demonstrate the race explicitly.\n\nReviewed By: ngimel\n\nDifferential Revision: D32588785\n\nfbshipit-source-id: d87cc5e49ff8cbf59052c3c97da5b48dd1fe75cc", "pr_number": "68749", "files_changed": ["aten/src/ATen/native/cuda/Copy.cu", "test/test_cuda.py"], "labels": ["fb-exported", "Merged", "cla signed", "ciflow/default"]}, "4afa5ea0ab": {"title": "native_functions.yaml: remove SparseXPU which is added by accident (#68791)", "body": "Summary:\ngen_backend_stubs.py will report 'assert' when generate code with\nSparseXPU dispatch key for external backends, if SparseXPU is in\nnative_functions.yaml.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/68791\n\nReviewed By: cpuhrsch, ejguan\n\nDifferential Revision: D32646303\n\nPulled By: bdhirsh\n\nfbshipit-source-id: 64e42cc40468bc8c696a31b4b7c0cc3728866a64", "pr_number": "68791", "files_changed": ["aten/src/ATen/native/native_functions.yaml"], "labels": ["triaged", "open source", "Merged", "cla signed", "ciflow/default"]}, "f14c16e509": {"title": "Revert D32599540: [pytorch][PR] implemented 'torch.distributions.constraints.symmetric' checking if the tensor is symmetric at last 2 dimension.", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD32599540 (https://github.com/pytorch/pytorch/commit/bc3bdbc8f48ff42238feb949fd7812b0a20bc329)\n\nOriginal commit changeset: 9227f7e99318\n\nfbshipit-source-id: edfe7072073d910a49be52e1b8c2d374ef71e9ec", "pr_number": null, "files_changed": ["test/distributions/test_constraints.py", "torch/distributions/constraints.py"], "labels": []}, "cffad597ea": {"title": "Tune test_reference_numerics_normal (#68019)", "body": "Summary:\nFixes #{issue number}\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/68019\n\nReviewed By: albanD\n\nDifferential Revision: D32482535\n\nPulled By: mruberry\n\nfbshipit-source-id: 48300a5c6a4484fb81789f9049d3f08272d9f31c", "pr_number": "68019", "files_changed": ["test/test_unary_ufuncs.py", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["triaged", "open source", "Merged", "cla signed", "ciflow/default", "ciflow/all"]}, "01ddd5dde6": {"title": "[opinfo] use dtypes instead of dtypesIfCPU (#68732)", "body": "Summary:\nReland https://github.com/pytorch/pytorch/issues/67619\n\nReplace usage of dtypesIfCPU with dtypes in OpInfo class and also make it a mandatory argument.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/68732\n\nReviewed By: jbschlosser\n\nDifferential Revision: D32594344\n\nPulled By: mruberry\n\nfbshipit-source-id: 660b38aef97752ba064228e8989041ed1d5777fe", "pr_number": "68732", "files_changed": ["torch/testing/_internal/common_methods_invocations.py", "torch/testing/_internal/opinfo_helper.py"], "labels": ["open source", "Merged", "cla signed", "ciflow/default"]}, "3315c4b31e": {"title": "add instructions for unhandled exceptions in assert_close (#68722)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/68722\n\nTest Plan: Imported from OSS\n\nReviewed By: ngimel\n\nDifferential Revision: D32684446\n\nPulled By: mruberry\n\nfbshipit-source-id: 04fe5730721d24e44692cdc9bb327484356ead3f", "pr_number": "68722", "files_changed": ["test/test_testing.py", "torch/testing/_comparison.py"], "labels": ["open source", "Merged", "cla signed", "module: testing", "ciflow/default"]}, "f398320e0d": {"title": "packaging: Include lazy headers in package_data (#68817)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/68817\n\nLooks like these files are getting used by downstream xla so we need to\ninclude them in our package_data\n\nSigned-off-by: Eli Uriegas <eliuriegas@fb.com>\n\nTest Plan: Imported from OSS\n\nReviewed By: mruberry\n\nDifferential Revision: D32622241\n\nPulled By: seemethere\n\nfbshipit-source-id: 7b64e5d4261999ee58bc61185bada6c60c2bb5cc", "pr_number": "68817", "files_changed": ["setup.py"], "labels": ["lazy", "Merged", "cla signed", "ciflow/default"]}, "3d504ae1b4": {"title": "[RELAND] Fix Dispatching not considering List[Optional[Tensor]] for dispatch (#68073)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/68073\n\nRelanding the original PR. Its body was as follows:\n\nFollowup to https://github.com/pytorch/pytorch/pull/60787\n\nIt turns out that the original PR was wrong for unboxed kernels. We\nrecently ran into this in\nhttps://github.com/facebookresearch/functorch/issues/124\n\nFor unboxed kernels, the correct type for a Tensor?[] argument is\nactually `List<optional<Tensor>>`, not `ArrayRef<optional<Tensor>>`\nghstack-source-id: 144204580\n\nTest Plan:\n- assert that https://github.com/facebookresearch/functorch/issues/124\nactually works\n\nReviewed By: gchanan\n\nDifferential Revision: D32313601\n\nPulled By: zou3519\n\nfbshipit-source-id: 8028d5f34eecabc53d603bd54d6b6748b5db461a", "pr_number": "68073", "files_changed": ["aten/src/ATen/core/dispatch/DispatchKeyExtractor.h", "test/test_python_dispatch.py"], "labels": ["Merged", "cla signed", "ciflow/default"]}, "787ded5103": {"title": "Add lazy::Shape::numel() (#68314)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/68314\n\nAdd a convenience to lazy::Shape for counting the number of elements (by multiplying out the dimensions).  This is a method on Tensor, and in switching other lazy tensor shape utils to use aten shape inference, we need numel counts.\n\nTest Plan: add unit tests\n\nReviewed By: alanwaketan\n\nDifferential Revision: D32409138\n\nfbshipit-source-id: 3ae725300f8826d38e45412f46501d5e5f776fb2", "pr_number": "68314", "files_changed": ["test/cpp/lazy/test_shape.cpp", "torch/csrc/lazy/core/shape.cpp", "torch/csrc/lazy/core/shape.h"], "labels": ["fb-exported", "Merged", "cla signed", "ciflow/default"]}, "250d0bd20b": {"title": "[RPC][Dist CI][BE] RPC tests run in subprocess (#68821)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/68821\n\nContinuing effort to move most distributed tests to run in subprocess\nfor better reproducibility + reduce flakiness.\nghstack-source-id: 144213520\n\nTest Plan: CI\n\nReviewed By: H-Huang\n\nDifferential Revision: D32624199\n\nfbshipit-source-id: 04448636320554d7a3ab29ae92bc1ca9fbe37da2", "pr_number": "68821", "files_changed": ["test/run_test.py"], "labels": ["Merged", "cla signed", "ciflow/default"]}, "3bd7dbf119": {"title": "[Dist CI][BE] Remainder of c10d/store tests run in subprocess (#68822)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/68822\n\nPer title, we switched over c10d_gloo and nccl and results look good\nso far, so switch the rest of them as well. After the only dist tests that\nwon't run in subprocess are pipe and fsdp tests, which historically haven't had\nmuch flakiness.\nghstack-source-id: 144213522\n\nTest Plan: CI\n\nReviewed By: H-Huang\n\nDifferential Revision: D32624330\n\nfbshipit-source-id: 469f613e5b0e4529e6b23ef259d948837d4af26b", "pr_number": "68822", "files_changed": ["test/run_test.py"], "labels": ["Merged", "cla signed", "ciflow/default"]}, "f5fa91ba2e": {"title": "Sparse: Add additional opinfo tests (#68886)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/68886\n\ncc nikitaved pearu cpuhrsch IvanYashchuk\n\nTest Plan: Imported from OSS\n\nReviewed By: jbschlosser\n\nDifferential Revision: D32697933\n\nPulled By: cpuhrsch\n\nfbshipit-source-id: fffdd1bc663cc1bc49abe8cf3680982d1cb497bc", "pr_number": "68886", "files_changed": ["test/test_sparse.py", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["module: sparse", "open source", "Merged", "cla signed", "ciflow/default"]}, "d9e7d85390": {"title": "Remove TH/THC Storage (#68556)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/67852\n\ncc ezyang bhosmer smessmer ljk53 bdhirsh\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/68556\n\nReviewed By: ejguan\n\nDifferential Revision: D32652758\n\nPulled By: ngimel\n\nfbshipit-source-id: 170956fca112606f9008abe09b92c6ddc411be09", "pr_number": "68556", "files_changed": ["BUILD.bazel", "aten/src/ATen/hip/impl/HIPCachingAllocatorMasqueradingAsCUDA.h", "aten/src/README.md", "aten/src/TH/CMakeLists.txt", "aten/src/TH/TH.h", "aten/src/TH/THStorage.h", "aten/src/TH/THStorageFunctions.cpp", "aten/src/TH/THStorageFunctions.h", "aten/src/TH/THStorageFunctions.hpp", "aten/src/TH/THTensor.cpp", "aten/src/TH/THTensor.h", "aten/src/TH/THTensor.hpp", "aten/src/TH/generic/THStorage.cpp", "aten/src/TH/generic/THStorage.h", "aten/src/TH/generic/THTensor.cpp", "aten/src/TH/generic/THTensor.h", "aten/src/THC/CMakeLists.txt", "aten/src/THC/THC.h", "aten/src/THC/THCStorage.cpp", "aten/src/THC/THCStorage.cu", "aten/src/THC/THCStorage.h", "aten/src/THC/THCStorage.hpp", "aten/src/THC/THCTensor.cpp", "aten/src/THC/THCTensor.h", "aten/src/THC/THCTensor.hpp", "aten/src/THC/generic/THCStorage.cpp", "aten/src/THC/generic/THCStorage.cu", "aten/src/THC/generic/THCStorage.h", "aten/src/THC/generic/THCTensor.cpp", "aten/src/THC/generic/THCTensor.h", "c10/util/intrusive_ptr.h", "tools/build_variables.bzl", "torch/csrc/DynamicTypes.cpp", "torch/csrc/PythonTypes.h", "torch/csrc/README.md", "torch/csrc/Storage.cpp", "torch/csrc/StorageDefs.h", "torch/csrc/THP.h", "torch/csrc/Types.h", "torch/csrc/cuda/override_macros.h", "torch/csrc/cuda/restore_macros.h", "torch/csrc/cuda/undef_macros.h", "torch/csrc/cuda/utils.h", "torch/csrc/generic/Storage.cpp", "torch/csrc/generic/Storage.h", "torch/csrc/generic/StorageMethods.cpp", "torch/csrc/generic/StorageSharing.cpp", "torch/csrc/generic/serialization.cpp", "torch/csrc/generic/serialization.h", "torch/csrc/generic/utils.h", "torch/csrc/utils.cpp", "torch/csrc/utils.h"], "labels": ["module: internals", "triaged", "module: porting", "open source", "Merged", "cla signed", "ciflow/default"]}, "b468566208": {"title": "Add ModuleInfo-based CPU / GPU parity tests (#68097)", "body": "Summary:\nContinuation of https://github.com/pytorch/pytorch/issues/64694; fixes issues with the diff there\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/68097\n\nReviewed By: mruberry\n\nDifferential Revision: D32300650\n\nPulled By: jbschlosser\n\nfbshipit-source-id: f3a5e72b019d4eddd7202854999eab61fffc9006", "pr_number": "68097", "files_changed": ["test/test_modules.py"], "labels": ["Merged", "cla signed", "ciflow/default"]}, "af49805a73": {"title": "Port lerp to structured kernels (#68924)", "body": "Summary:\nRef https://github.com/pytorch/pytorch/issues/55070\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/68924\n\nReviewed By: jbschlosser\n\nDifferential Revision: D32697409\n\nPulled By: bdhirsh\n\nfbshipit-source-id: b098533e46f8bdbb995c76db0e6a124ab2b076b8", "pr_number": "68924", "files_changed": ["aten/src/ATen/native/Lerp.cpp", "aten/src/ATen/native/Lerp.h", "aten/src/ATen/native/cpu/LerpKernel.cpp", "aten/src/ATen/native/cuda/Lerp.cu", "aten/src/ATen/native/native_functions.yaml"], "labels": ["open source", "Merged", "cla signed", "module: structured kernels", "ciflow/default"]}, "1342f19a8c": {"title": "Add ModuleInfo-based device transfer tests (#68092)", "body": "Summary:\nContinuation of https://github.com/pytorch/pytorch/issues/65488; addresses the problem that got it reverted.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/68092\n\nReviewed By: mruberry\n\nDifferential Revision: D32299103\n\nPulled By: jbschlosser\n\nfbshipit-source-id: bc298aca15368f2acb5082e6fb6eedea60b5d75f", "pr_number": "68092", "files_changed": ["test/test_modules.py"], "labels": ["Merged", "cla signed", "ciflow/default"]}, "27228656e6": {"title": "[FX][docs] Document gotcha about training flag (#68915)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/68913\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/68915\n\nReviewed By: jamesr66a\n\nDifferential Revision: D32705410\n\nPulled By: jubinchheda\n\nfbshipit-source-id: a44c17ab0e62465823ceb0ef983ae330b50fb073", "pr_number": "68915", "files_changed": ["docs/source/fx.rst"], "labels": ["Merged", "cla signed", "ciflow/default"]}, "7194faed7f": {"title": "[PyTorch] Optimzize mergeRunCallbacks for RecordFunction (#68387)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/68387\n\nFunction call overhead on tryRunCallback was notable.\nghstack-source-id: 144235788\n\nTest Plan:\nRun //caffe2/caffe2/fb/high_perf_models/pytorch/benchmark_framework_overheads:cpp_benchmark before/after this diff with arguemnts `--stressTestRecordFunction --op empty`.\n\nBefore: P467891339\nAfter: P467891381\n\nReviewed By: chaekit\n\nDifferential Revision: D32443863\n\nfbshipit-source-id: c0b3dd40bbd5bca976c2ebb0f21aa62e097b302e", "pr_number": "68387", "files_changed": ["aten/src/ATen/record_function.cpp"], "labels": ["Merged", "cla signed", "ciflow/default"]}, "1d0416397a": {"title": "[PyTorch] Change from unique_ptr to optional for RecordFunction state (#68397)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/68397\n\nNow that hot paths can avoid instantiating RecordFunction by using shouldRunRecordFunction, we can improve efficiency for profiling cases by avoiding a large heap allocation.\nghstack-source-id: 144235785\n\nTest Plan:\n1) Run //caffe2/caffe2/fb/high_perf_models/pytorch/benchmark_framework_overheads:cpp_benchmark before/after this diff with arguemnts --stressTestRecordFunction --op empty.\n\nBefore: P467891381\n\nAfter: P467902339\n\n2) Run without --stressTestRecordFunction to verify no regression in the regular dispatcher path.\n\nBefore: P467902381\n\nAfter: P467902403\n\nReviewed By: chaekit\n\nDifferential Revision: D32448365\n\nfbshipit-source-id: 2d32a3bd82c60d2bb11fc57bb88bf3f02aa3fa25", "pr_number": "68397", "files_changed": ["aten/src/ATen/record_function.cpp", "aten/src/ATen/record_function.h"], "labels": ["Merged", "cla signed", "ciflow/default"]}, "178010455d": {"title": "Vectorized: Use inline namespace instead of anonymous (#67655)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/67655\n\nSome of the CPU operators already use the `namespace CPU_CAPABILITY` trick to avoid anonymous namespacing, like [`PowKernel.cpp`](https://github.com/pytorch/pytorch/blob/cd51d2a3ecc8ac579bee910f6bafe41a4c41ca80/aten/src/ATen/native/cpu/PowKernel.cpp#L14). This extends that pattern to the `Vectorized` class, which avoids `Wsubobject-linage` warnings like I was getting in #67621.\n\nFor many functions, it was necessary to add `inline` because the functions are defined in a header. There were no link errors previously because the anonymous namespace ensured they were not exposed to linkage. Similarly, free functions defined in an anonymous namespace might need the `C10_UNUSED` attribute to silence warnings about the function not being called in the only translation unit that it's defined in. By removing the anonymous namespace, these decorators are no longer necessary.\n\nTest Plan: Imported from OSS\n\nReviewed By: dagitses\n\nDifferential Revision: D32566109\n\nPulled By: malfet\n\nfbshipit-source-id: 01d64003513b4946dec6b709bd73bbab05772134\n\nCo-authored-by: Nikita Shulga <nshulga@fb.com>", "pr_number": "67655", "files_changed": ["aten/src/ATen/cpu/vec/vec256/vec256.h", "aten/src/ATen/cpu/vec/vec256/vec256_bfloat16.h", "aten/src/ATen/cpu/vec/vec256/vec256_complex_double.h", "aten/src/ATen/cpu/vec/vec256/vec256_complex_float.h", "aten/src/ATen/cpu/vec/vec256/vec256_double.h", "aten/src/ATen/cpu/vec/vec256/vec256_float.h", "aten/src/ATen/cpu/vec/vec256/vec256_float_neon.h", "aten/src/ATen/cpu/vec/vec256/vec256_int.h", "aten/src/ATen/cpu/vec/vec256/vec256_qint.h", "aten/src/ATen/cpu/vec/vec256/vsx/vec256_bfloat16_vsx.h", "aten/src/ATen/cpu/vec/vec256/vsx/vec256_common_vsx.h", "aten/src/ATen/cpu/vec/vec256/vsx/vec256_complex_double_vsx.h", "aten/src/ATen/cpu/vec/vec256/vsx/vec256_complex_float_vsx.h", "aten/src/ATen/cpu/vec/vec256/vsx/vec256_double_vsx.h", "aten/src/ATen/cpu/vec/vec256/vsx/vec256_float_vsx.h", "aten/src/ATen/cpu/vec/vec256/vsx/vec256_int16_vsx.h", "aten/src/ATen/cpu/vec/vec256/vsx/vec256_int32_vsx.h", "aten/src/ATen/cpu/vec/vec256/vsx/vec256_int64_vsx.h", "aten/src/ATen/cpu/vec/vec256/vsx/vec256_qint32_vsx.h", "aten/src/ATen/cpu/vec/vec256/vsx/vec256_qint8_vsx.h", "aten/src/ATen/cpu/vec/vec256/vsx/vec256_quint8_vsx.h", "aten/src/ATen/cpu/vec/vec256/vsx/vsx_helpers.h", "aten/src/ATen/cpu/vec/vec512/vec512.h", "aten/src/ATen/cpu/vec/vec512/vec512_bfloat16.h", "aten/src/ATen/cpu/vec/vec512/vec512_complex_double.h", "aten/src/ATen/cpu/vec/vec512/vec512_complex_float.h", "aten/src/ATen/cpu/vec/vec512/vec512_double.h", "aten/src/ATen/cpu/vec/vec512/vec512_float.h", "aten/src/ATen/cpu/vec/vec512/vec512_int.h", "aten/src/ATen/cpu/vec/vec512/vec512_qint.h", "aten/src/ATen/cpu/vec/vec_base.h", "aten/src/ATen/cpu/vml.h", "aten/src/ATen/native/cpu/IsContiguous.h", "aten/src/ATen/native/cpu/Loops.h", "aten/src/ATen/native/cpu/PowKernel.cpp", "aten/src/ATen/native/cpu/Reduce.h", "aten/src/ATen/native/cpu/UnaryOpsKernel.cpp", "aten/src/ATen/native/cpu/utils.h", "aten/src/ATen/native/cpu/zmath.h"], "labels": ["open source", "Merged", "cla signed", "ciflow/default"]}, "61ea2fc35e": {"title": "Fix device type / dtype handling for parametrized test names (#65217)", "body": "Summary:\nThis PR absolves `_TestParametrizer`s (e.g. `ops`, `modules`, `parametrize`) of the responsibility of adding device type (e.g. `'cpu'`, `'cuda'`, etc.) / dtype (e.g. 'float32') to generated test names. This fixes repeated instances of the device string being added to generated test names (e.g. `test_batch_norm_training_True_cuda_track_running_stats_True_cuda_affine_True_cuda`).\n\nThe responsibility for placing device / dtype suffixes is now handled by `instantiate_device_type_tests()` instead so it is added a single time. It will place `<device>_<dtype>` at the end of the test name unconditionally, maintaining the current naming convention.\n\nAs part of this work, I also tightened the semantics through some additional error case handling:\n* Composing multiple decorators that each try to handle the same parameter will error out with a nice message. This includes the case to trying to compose `modules` + `ops`, as they each try to handle `dtype`. Similarly, `ops` + `dtypes` is forbidden when both try to handle `dtype`. This required changes in the following test files:\n  * `test/test_unary_ufuncs.py`\n  * `test/test_foreach.py`\n* The `modules` / `ops` decorators will now error out with a nice message if used with `instantiate_parametrized_tests()` instead of `instantiate_device_type_tests()`, since they're not (currently) written to work outside of a device-specific context.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65217\n\nReviewed By: mruberry\n\nDifferential Revision: D32627303\n\nPulled By: jbschlosser\n\nfbshipit-source-id: c2957228353ed46a0b7da8fa1a34c67598779312", "pr_number": "65217", "files_changed": ["test/test_foreach.py", "test/test_testing.py", "test/test_unary_ufuncs.py", "torch/testing/_internal/common_device_type.py", "torch/testing/_internal/common_methods_invocations.py", "torch/testing/_internal/common_modules.py", "torch/testing/_internal/common_utils.py"], "labels": ["Merged", "cla signed", "ciflow/default"]}, "e6a8d15a4c": {"title": "cpu_kernel_vec: Hoist stride checks out of loop (#68962)", "body": "Summary:\n`cpu_kernel_vec` does stride checks to determine whether to use the vectorized or scalar inner loop. Since it uses a 1d `for_each` loop, it re-does these stride checks after every loop over the inner dimension. For iterators with small inner dimensions, this means a significant proportion of the time may be spent just on stride checks.\n\nThis changes it to use a 2d loop so the stride checks are further amortized. With the below `copy_` benchmark, it saves 50% of the callgrind instruction count from 28.4 Million to 13.5 Million and 30% time speedup from 22.8 us to 16.4 us on my machine.\n\n```\nfrom torch.utils.benchmark import Timer\nimport timeit\ntimer = Timer(\n    stmt=\"b.copy_(a);\",\n    setup=\"\"\"\n    auto a = at::rand({10000, 8}, at::kComplexDouble).slice(0, 0, -1, 2);\n    auto b = at::empty_like(a);\n    \"\"\",\n    num_threads=1,\n    language='c++',\n    timer=timeit.default_timer\n)\n```\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/68962\n\nReviewed By: mrshenli\n\nDifferential Revision: D32684191\n\nPulled By: ngimel\n\nfbshipit-source-id: 582af038314a0f999f43669e66edace38ff8d2dc", "pr_number": "68962", "files_changed": ["aten/src/ATen/native/cpu/Loops.h"], "labels": ["open source", "Merged", "cla signed", "ciflow/default"]}, "ac1fe91dc9": {"title": "Clean up some THC includes (#69024)", "body": "Summary:\nThese seem to not be needed and cause ninja to rebuild the files at every build.\n\n(There also is THCStorage.cu, but hopefully this will go away with https://github.com/pytorch/pytorch/issues/68556 )\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/69024\n\nReviewed By: soulitzer\n\nDifferential Revision: D32705309\n\nPulled By: ngimel\n\nfbshipit-source-id: 5255297f213fdcf36e7203de7460a71291f8c9a0", "pr_number": "69024", "files_changed": ["aten/src/ATen/native/cuda/Bucketization.cu", "aten/src/ATen/native/cuda/Shape.cu", "torch/csrc/cuda/Event.h", "torch/csrc/cuda/Module.h", "torch/csrc/cuda/Stream.h", "torch/csrc/cuda/THCP.h", "torch/csrc/cuda/nccl.cpp", "torch/csrc/cuda/python_comm.cpp"], "labels": ["open source", "Merged", "cla signed", "ciflow/default"]}, "3186d36972": {"title": "[TensorExpr] Supress TracerWarnings in test_unsupported in test_jit_fuser_te.py. (#68757)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/68757\n\nTest Plan: Imported from OSS\n\nReviewed By: navahgar\n\nDifferential Revision: D32600951\n\nPulled By: ZolotukhinM\n\nfbshipit-source-id: 7b9859d7dee1e9803b8fde5d071890a72d30cec9", "pr_number": "68757", "files_changed": ["test/test_jit_fuser_te.py"], "labels": ["Merged", "cla signed", "ciflow/default"]}, "0cdeb586ae": {"title": "[LTC] Upstream some utilities (#69046)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/69046\n\nThis commit upstreams utilities including ExceptionCleanup, MaybeRef,\nIota, ToVector, ToOptionalVector and GetEnumValue.\n\nTest Plan: ./build/bin/test_lazy --gtest_filter=UtilTest.*\n\nReviewed By: wconstab, Chillee\n\nDifferential Revision: D32709090\n\nPulled By: alanwaketan\n\nfbshipit-source-id: 5147433becd4dbb07be7d36d66b0b8685054d714", "pr_number": "69046", "files_changed": ["test/cpp/lazy/CMakeLists.txt", "test/cpp/lazy/test_misc.cpp", "test/cpp/lazy/test_util.cpp", "torch/csrc/lazy/core/hash.h", "torch/csrc/lazy/core/util.h"], "labels": ["Merged", "cla signed", "ciflow/default"]}, "ec1339a48b": {"title": "[CUDA Pinned Memory] Alternative implementation of pinned memory allocator focusing on multi-threaded scalability (#68906)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/68906\n\nThe existing PyTorch pinned memory allocator has been a challenge for scalability in multi-GPU inference workloads. The existing allocator is mostly designed in the context of training, where in the process-per-GPU setup we have natural sharding of the global locks and lower allocation rates (perhaps O(100 allocs/sec) per process. In this setup we might have globally on the order of O(200k allocs/sec) - e.g. 20k QPS and 10 allocs/query. This is a different domain.\n\nIn the existing allocator, we observe tail latencies of cudaEventCreate and cudaEventDestroy (while holding the lock) can also completely stall all allocations, which is undesirable.\n\nThe idea here is to retain a similar design to the existing PyTorch allocator - eager collection of used memory, no lock-free or deferred tricks, identical semantics around events, but to:\n\na) split up the locks around the various critical datastructures, and\nb) do as little work as possible while holding any process-global mutexes (importantly, no CUDA runtime API calls)\nc) pool CUDA events manually (as cuda event creation is a bottleneck at high rates from multiple threads).\n\nThis does require a bit of care, but I believe it's correct. In general the threading and state transitions are fairly simple.\n\nWith these improvements, microbenchmarks show significant improvements (1.5x-3x). Importantly, real workloads also show significant improvements, especially WRT tail latency and stalls.\n\nTest Plan:\nUnit tests all pass.\n\nWith a synthetic benchmark such as:\n\n```\nstatic void BM_copies_baseline(benchmark::State& state) {\n  auto N = state.range(0);\n  auto scale = state.range(1);\n  auto object_size_min = N;\n  auto object_size_max = scale * N;\n\n  auto device = at::Device(at::kCUDA, at::cuda::current_device());\n\n  uint64_t bytes_copied = 0;\n  uint64_t allocs = 0;\n  auto stream = at::cuda::getCurrentCUDAStream();\n  for (auto _ : state) {\n    auto object_size = static_cast<int64_t>(expf(folly::Random::randDouble(\n        logf(object_size_min), logf(object_size_max))));\n    auto tensor = at::empty(\n        {object_size},\n        at::TensorOptions().dtype(at::kByte).pinned_memory(true));\n    at::cuda::CachingHostAllocator_recordEvent(\n        tensor.storage().data_ptr().get_context(), stream);\n    bytes_copied += object_size;\n    allocs += 1;\n  }\n  state.counters[\"BW\"] =\n      benchmark::Counter(bytes_copied, benchmark::Counter::kIsRate);\n  state.counters[\"Allocs\"] =\n      benchmark::Counter(allocs, benchmark::Counter::kIsRate);\n}\n\nBENCHMARK(BM_copies_baseline)->Args({1000000, 20})->Threads(1)->UseRealTime();\nBENCHMARK(BM_copies_baseline)->Args({1000000, 20})->Threads(4)->UseRealTime();\nBENCHMARK(BM_copies_baseline)->Args({1000000, 20})->Threads(16)->UseRealTime();\nBENCHMARK(BM_copies_baseline)->Args({1000000, 20})->Threads(64)->UseRealTime();\nBENCHMARK(BM_copies_baseline)->Args({1000000, 20})->Threads(128)->UseRealTime();\nBENCHMARK(BM_copies_baseline)->Args({1000000, 20})->Threads(256)->UseRealTime();\n```\n\nI observe roughly 1.5-3x improvements.\n\nEnd to end application testing also sees significant improvements in the contended scenario.\n\nReviewed By: jianyuh, ngimel\n\nDifferential Revision: D32588784\n\nfbshipit-source-id: ee86c3b7ed4da6412dd3c89362f989f4b5d91736", "pr_number": "68906", "files_changed": ["aten/src/ATen/cuda/CachingHostAllocator.cpp", "aten/src/ATen/cuda/CachingHostAllocator.h"], "labels": ["fb-exported", "Merged", "cla signed", "ciflow/default"]}, "f7d598948a": {"title": "Remove native_functions.yaml dependency from TensorModeKernel.cu (#66913)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/66913\n\nTest Plan: Imported from OSS\n\nReviewed By: bdhirsh\n\nDifferential Revision: D31856102\n\nPulled By: dagitses\n\nfbshipit-source-id: 8888a1984adef09104a40ae683d091143cd1f4fa", "pr_number": "66913", "files_changed": ["aten/src/ATen/native/NonEmptyUtils.h", "aten/src/ATen/native/ReduceOpsUtils.h", "aten/src/ATen/native/cuda/TensorModeKernel.cpp", "aten/src/ATen/native/cuda/TensorModeKernel.cu", "aten/src/ATen/native/cuda/TensorModeKernel.h", "c10/core/DeviceArray.h", "caffe2/CMakeLists.txt"], "labels": ["open source", "Merged", "cla signed", "ciflow/default"]}, "174eea8a05": {"title": "Remove native_functions.yaml dependency from IndexKernel.{cpp,cu} (#66914)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/66914\n\nTest Plan: Imported from OSS\n\nReviewed By: zou3519\n\nDifferential Revision: D31856105\n\nPulled By: dagitses\n\nfbshipit-source-id: 8729783b68879b509ae6b66ce145de0af68aad8c", "pr_number": "66914", "files_changed": ["aten/src/ATen/native/IndexKernel.h", "aten/src/ATen/native/TensorAdvancedIndexing.cpp", "aten/src/ATen/native/TensorAdvancedIndexing.h", "aten/src/ATen/native/TensorTransformations.cpp", "aten/src/ATen/native/TensorTransformations.h", "aten/src/ATen/native/cpu/IndexKernel.cpp", "aten/src/ATen/native/cuda/IndexKernel.cpp", "aten/src/ATen/native/cuda/IndexKernel.cu", "aten/src/ATen/native/cuda/IndexKernel.h", "aten/src/ATen/native/cuda/KernelUtils.cuh", "caffe2/CMakeLists.txt"], "labels": ["open source", "Merged", "cla signed", "ciflow/default"]}, "b83e8d560b": {"title": "[LT] Sync LTC branch changes on torch/csrc/lazy/core (#69012)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/69012\n\nSome changes to torch/csrc/lazy/core were done on the\nlazy_tensor_staging branch (https://github.com/pytorch/pytorch/pull/68427).\nMerge those back into the trunk.\n\nTest Plan: Imported from OSS\n\nReviewed By: wconstab\n\nDifferential Revision: D32708696\n\nPulled By: desertfire\n\nfbshipit-source-id: e54b978f2bdb9c7db27880f60246fdf1e8b41019", "pr_number": "69012", "files_changed": ["torch/csrc/lazy/backend/backend_interface.h", "torch/csrc/lazy/core/hash.h", "torch/csrc/lazy/ts_backend/ts_lowering_context.h", "torch/csrc/lazy/ts_backend/ts_node_lowering.h"], "labels": ["Merged", "cla signed", "ciflow/default"]}, "6fea7499c2": {"title": "CompositeImplicitAutograd compliance testing (#65819)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65819\n\nRelated to #61669.\n\nFunctions registered as CompositeImplicitAutograd MUST work for most, if\nnot all, backends. This includes Tensor subclasses.\n\nTo achieve this, we (PyTorch) impose a set of constraints on how a\nCompositeImplicitAutograd function can be written.\n\nConcretely, this PR adds tests for all OpInfos that checks for\ncompliance. The things that get tested in this PR apply to composite\nops and are that:\n- the op does not change the metadata of a Tensor without performing\ndispatches\n- the op does not call set_ or resize_\n- the op does not directly access the data ptr\n\nThe mechanism for the test is to create a new __torch_dispatch__\nobject, CompositeCompliantTensor. For each operator, we wrap all inputs\nin CompositeCompliantTensor, turn on python mode for it,\nand send it through the operator.\n\nNon-CompositeImplicitAutograd operators will pass the test because they\nperform a dispatch to backend code. Here's how CompositeCompliantTensor\ncatches problems:\n\n- If it sees set_ or resize_ getting called, it will directly error\nout\n- After each operation, CompositeCompliantTensor checks to make sure\nthat its metadata is consistent with that of the thing it is wrapping.\nIf the CompositeImplicitAutograd op modifes the metadata directly\n(through e.g. the TensorImpl API) then the metadata will go out of sync.\n- If data_ptr gets called, that returns a nice error (because the\nstorage is meta).\n\nCompositeCompliantTensor is written in an interesting way. First off,\nif a view operation occurs (e.g. `B = A.view_op(...)`), then B.storage()\nmust alias A.storage() where B.storage() is CompositeCompliantTensor's\nstorage, NOT the storage of the tensor it is wrapping. This is an\ninvariant in autograd, see #62182 for details. To handle\nthis we replay the view on A's storage and set it as B's storage.\n\nSecondly, there are cases where the metadata is allowed to go out of\nsync. I believe this is only possible with in-place view functions, like\ntranspose_, t_, squeeze_, unsqueeze_. Those are special cased.\n\nFinally, I added a new section to aten/src/ATen/native/README.md about\nwhat it means to be CompositeImplicitAutograd Compliant\n\nTest Plan: - run tests\n\nReviewed By: ezyang, bdhirsh\n\nDifferential Revision: D31268369\n\nPulled By: zou3519\n\nfbshipit-source-id: 31634b1cbe1778ab30196013cfc376ef9bd2e8b1", "pr_number": "65819", "files_changed": ["aten/src/ATen/native/README.md", "test/test_ops.py", "torch/testing/_internal/common_methods_invocations.py", "torch/testing/_internal/composite_compliance.py"], "labels": ["Merged", "cla signed", "ciflow/default"]}, "d3de3546d9": {"title": "Revert D32099294: Split cuda: list cpp files that go in _cu library explicitly", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD32099294 (https://github.com/pytorch/pytorch/commit/b47ae9810c1a645f4942737ab4a58b2b1407e7bd)\n\nOriginal commit changeset: 8a3582944b6b\n\nfbshipit-source-id: eab63e6ba3db3e17f404292a3659823607627576", "pr_number": null, "files_changed": ["caffe2/CMakeLists.txt"], "labels": []}, "81246ed01c": {"title": "Markdown was linking to repo rather than pytorch.org website (#68937)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/68937\n\nReviewed By: samdow\n\nDifferential Revision: D32707264\n\nPulled By: soulitzer\n\nfbshipit-source-id: c534f008087def33784dde701130769e2058aa9f", "pr_number": "68937", "files_changed": ["CONTRIBUTING.md"], "labels": ["open source", "Merged", "cla signed", "ciflow/default"]}, "8f9f559453": {"title": "ammend tensors.rst and torch.rst for doc generation (#69030)", "body": "Summary:\n(This is my first contribution to PyTorch) Added missing operations to docs added in https://github.com/pytorch/pytorch/issues/64430. Please let me know if I've done anything wrong.\n\nFixes https://github.com/pytorch/pytorch/issues/68928\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/69030\n\nReviewed By: samdow\n\nDifferential Revision: D32706826\n\nPulled By: soulitzer\n\nfbshipit-source-id: edcc175a8f9bc69450a39059580c05edce699312", "pr_number": "69030", "files_changed": ["docs/source/tensors.rst", "docs/source/torch.rst"], "labels": ["open source", "Merged", "cla signed", "ciflow/default"]}, "ef7ed082ec": {"title": "[PyTorch] Remove StringView from RecordFunction implementation [2/2] (#68411)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/68411\n\nAvoids heap-allocating a std::string instance in before() each time even if it's not going to be used.\nghstack-source-id: 144287655\n\nTest Plan:\nRun //caffe2/caffe2/fb/high_perf_models/pytorch/benchmark_framework_overheads:cpp_benchmark before/after this diff with arguemnts --stressTestRecordFunction --op empty\n\nBefore: P467922606\nAfter: P467922626\n\nReviewed By: chaekit\n\nDifferential Revision: D32453846\n\nfbshipit-source-id: 18e1b482dbf5217add14cbaacd447de47cb5877b", "pr_number": "68411", "files_changed": ["aten/src/ATen/record_function.cpp", "aten/src/ATen/record_function.h"], "labels": ["Merged", "cla signed", "ciflow/default"]}, "f6f1b580f8": {"title": "Fix mypy in cpp_extension.py (#69101)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/69101\n\nTest Plan: Imported from OSS\n\nReviewed By: atalman, janeyx99\n\nDifferential Revision: D32730081\n\nPulled By: malfet\n\nfbshipit-source-id: 76ace65b51850b74b175a3c4688c05e107873e8d", "pr_number": "69101", "files_changed": ["torch/utils/cpp_extension.py"], "labels": ["Merged", "cla signed", "ciflow/default"]}, "e534c5efd7": {"title": "CMake: Include instead of copying cpu kernel files (#67656)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/67656\n\nCurrently, each cpu kernel file is copied into the build folder 3 times to give them different compilation flags. This changes it to instead generate 3 files that `#include` the original file. The biggest difference is that updating a copied file requires `cmake` to re-run, whereas include dependencies are natively handled by `ninja`.\n\nA side benefit is that included files show up directly in the build dependency graph, whereas `cmake` file copies don't.\n\nTest Plan: Imported from OSS\n\nReviewed By: dagitses\n\nDifferential Revision: D32566108\n\nPulled By: malfet\n\nfbshipit-source-id: ae75368fede37e7ca03be6ade3d4e4a63479440d", "pr_number": "67656", "files_changed": ["cmake/Codegen.cmake", "cmake/IncludeSource.cpp.in"], "labels": ["open source", "cla signed", "ciflow/default", "ciflow/all"]}, "219db3b4e1": {"title": "Add OpInfo for torch.linalg.tensorsolve (#68810)", "body": "Summary:\nThis PR adds an OpInfo entry for tensorsolve function.\nThe keyword argument is different from NumPy so a lambda function is needed to be passed to `ref=`.\nI had to change the dtypes for `test_reference_testing` because NumPy does computation internally using double for all linear algebra functions and maybe for some other functions. Using `torch.float64` and `torch.complex128` is more reliable for NumPy comparisons.\n\ncc mruberry\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/68810\n\nReviewed By: soulitzer\n\nDifferential Revision: D32696065\n\nPulled By: mruberry\n\nfbshipit-source-id: a4305065d3e7d0097503dc05938b3c4784e14996", "pr_number": "68810", "files_changed": ["test/test_ops.py", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["module: tests", "triaged", "open source", "cla signed", "ciflow/default"]}, "845a82b635": {"title": "Debug positive definite constraints (#68720)", "body": "Summary:\nWhile implementing https://github.com/pytorch/pytorch/issues/68644,\nduring the testing of 'torch.distributions.constraint.positive_definite', I found an error in the code: [location](https://github.com/pytorch/pytorch/blob/c7ecf1498d961415006c3710ac8d99166fe5d634/torch/distributions/constraints.py#L465-L468)\n```\nclass _PositiveDefinite(Constraint):\n    \"\"\"\n    Constrain to positive-definite matrices.\n    \"\"\"\n    event_dim = 2\n\n    def check(self, value):\n        # Assumes that the matrix or batch of matrices in value are symmetric\n        # info == 0 means no error, that is, it's SPD\n        return torch.linalg.cholesky_ex(value).info.eq(0).unsqueeze(0)\n```\n\nThe error is caused when I check the positive definiteness of\n`torch.cuda.DoubleTensor([[2., 0], [2., 2]])`\nBut it did not made a problem for\n`torch.DoubleTensor([[2., 0], [2., 2]])`\n\nYou may easily reproduce the error by following code:\n\n```\nPython 3.9.7 (default, Sep 16 2021, 13:09:58)\n[GCC 7.5.0] :: Anaconda, Inc. on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> import torch\n>>> const = torch.distributions.constraints.positive_definite\n>>> const.check(torch.cuda.DoubleTensor([[2., 0], [2., 2]]))\ntensor([False], device='cuda:0')\n>>> const.check(torch.DoubleTensor([[2., 0], [2., 2]]))\ntensor([True])\n```\nThe cause of error can be analyzed more if you give 'check_errors = True' as a additional argument for 'torch.linalg.cholesky_ex'.\nIt seem that it is caused by the recent changes in 'torch.linalg'.\nAnd, I suggest to modify the '_PositiveDefinite' class by using 'torch.linalg.eig' function like the below:\n\n```\nclass _PositiveDefinite(Constraint):\n    \"\"\"\n    Constrain to positive-definite matrices.\n    \"\"\"\n    event_dim = 2\n\n    def check(self, value):\n        return (torch.linalg.eig(value)[0].real > 0).all(dim=-1)\n```\n\nBy using above implementation, I get following result:\n```\nPython 3.9.7 (default, Sep 16 2021, 13:09:58)\n[GCC 7.5.0] :: Anaconda, Inc. on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> import torch\n>>> const = torch.distributions.constraints.positive_definite\n>>> const.check(torch.cuda.DoubleTensor([[2., 0.], [2., 2.]]))\ntensor(True, device='cuda:0')\n>>> const.check(torch.DoubleTensor([[2., 0.], [2., 2.]]))\ntensor(True)\n```\n\nFYI, I do not know what algorithm is used in 'torch.linalg.eig' and 'torch.linalg.cholesky_ex'. As far as I know, they have same time complexity generally, O(n^3). It seems that in case you used special algorithms or finer parallelization, time complexity of Cholesky decomposition may be reduced to approximately O(n^2.5). If there is a reason 'torch.distributions.constraints.positive_definite' used 'torch.linalg.cholesky_ex' rather than 'torch.linalg.eig' previously, I hope to know.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/68720\n\nReviewed By: samdow\n\nDifferential Revision: D32724391\n\nPulled By: neerajprad\n\nfbshipit-source-id: 32e2a04b2d5b5ddf57a3de50f995131d279ede49", "pr_number": "68720", "files_changed": ["test/distributions/test_constraints.py", "torch/distributions/constraints.py", "torch/distributions/multivariate_normal.py"], "labels": ["triaged", "open source", "cla signed", "ciflow/default"]}, "afff381824": {"title": "Automated submodule update: tensorpipe (#69089)", "body": "Summary:\nThis is an automated pull request to update the first-party submodule for [pytorch/tensorpipe](https://github.com/pytorch/tensorpipe).\n\nNew submodule commit: https://github.com/pytorch/tensorpipe/commit/ed4bbe52b7405d06dd5309cb9b8e647e033b9e00\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/69089\n\nTest Plan: Ensure that CI jobs succeed on GitHub before landing.\n\nReviewed By: lw\n\nDifferential Revision: D32725534\n\nfbshipit-source-id: 73b1e0f67c957ca0220cd47179dd4b350a98fd33", "pr_number": "69089", "files_changed": ["third_party/tensorpipe"], "labels": ["triaged", "open source", "cla signed", "ciflow/default"]}, "478069d6f2": {"title": "Remove duplicate .DS_Store in gitignore (#68981)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/68981\n\nReviewed By: samdow\n\nDifferential Revision: D32707039\n\nPulled By: soulitzer\n\nfbshipit-source-id: 346f0f3de583d995be34c252db4f9f26cd574ba8", "pr_number": "68981", "files_changed": [".gitignore"], "labels": ["open source", "cla signed", "ciflow/default"]}, "c08e95dd9c": {"title": "Introduce `IS_LINUX` and `IS_MACOS` global vars (#69093)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/69093\n\nTest Plan: Imported from OSS\n\nReviewed By: samdow\n\nDifferential Revision: D32730080\n\nPulled By: malfet\n\nfbshipit-source-id: aa3f218d09814b4edd96b01c7b57b85fd58c47fc", "pr_number": "69093", "files_changed": ["torch/utils/cpp_extension.py"], "labels": ["cla signed", "ciflow/default"]}, "deaf745aee": {"title": "Add kl divergence between normal and laplace distribution. (#68807)", "body": "Summary:\nFixes [https://github.com/pytorch/pytorch/issues/68746]\n![KL_normal_laplace](https://user-images.githubusercontent.com/35850237/143008244-f304cee1-9583-4de1-b0d0-5751ebdb8188.png)\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/68807\n\nReviewed By: H-Huang\n\nDifferential Revision: D32750391\n\nPulled By: neerajprad\n\nfbshipit-source-id: 129e6ef60d6e244d0d6b02b3944bfd5d8b06edcb", "pr_number": "68807", "files_changed": ["test/distributions/test_distributions.py", "torch/distributions/kl.py"], "labels": ["triaged", "open source", "cla signed", "ciflow/default"]}, "e6c435bf96": {"title": "[LTC] Upstream helpers for c10::Device <=> BackendDevice (#69064)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/69064\n\nThis commit upstreams helpers for converting a c10::Device to\nBackendDevice and vice versa.\n\nTest Plan: ./build/bin/test_lazy --gtest_filter=BackendDeviceTest.FromAten:BackendDeviceTest.ToAten\n\nReviewed By: wconstab\n\nDifferential Revision: D32732607\n\nPulled By: alanwaketan\n\nfbshipit-source-id: 0dd233d37a4a30fc4b22dba322ddd85d4cb3635b", "pr_number": "69064", "files_changed": ["test/cpp/lazy/test_backend_device.cpp", "tools/build_variables.bzl", "torch/csrc/lazy/backend/backend_device.cpp", "torch/csrc/lazy/backend/backend_device.h", "torch/csrc/lazy/backend/backend_interface.cpp", "torch/csrc/lazy/backend/backend_interface.h"], "labels": ["cla signed", "ciflow/default"]}, "263125a962": {"title": "Fix RAdam docstring on LR default value (#69186)", "body": "Summary:\nFixes #{issue number}\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/69186\n\nReviewed By: albanD\n\nDifferential Revision: D32759614\n\nPulled By: H-Huang\n\nfbshipit-source-id: b11819c50156a538cd6003e9cddde0390c853f67", "pr_number": "69186", "files_changed": ["torch/optim/radam.py"], "labels": ["open source", "cla signed", "ciflow/default"]}, "124bb6a19d": {"title": "RegisterDispatchKey.cpp: remove redundant code (#68983)", "body": "Summary:\nremove the line since line 10 has already included this header file\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/68983\n\nReviewed By: samdow\n\nDifferential Revision: D32706952\n\nPulled By: soulitzer\n\nfbshipit-source-id: 98746e12d8d04d64ee2e0449e4aec5153ac723d5", "pr_number": "68983", "files_changed": ["aten/src/ATen/templates/RegisterDispatchKey.cpp"], "labels": ["open source", "cla signed", "ciflow/default"]}, "929f2a750a": {"title": "Back out \"[CUDA Pinned Memory] Alternative implementation of pinned memory allocator focusing on multi-threaded scalability\" (#69191)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/69191\n\nReviewed By: xing-liu, yuchenhao\n\nDifferential Revision: D32748466\n\nfbshipit-source-id: 6abd3265e8a20270305da3f8be25114ad4d67fc2", "pr_number": "69191", "files_changed": ["aten/src/ATen/cuda/CachingHostAllocator.cpp", "aten/src/ATen/cuda/CachingHostAllocator.h"], "labels": ["fb-exported", "cla signed", "ciflow/default"]}, "cbe0a38d8c": {"title": "Back out \"[CUDA Pinned Memory] Event recording with non-blocking copies should track the storage context, not the tensor data pointer\" (#69193)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/69193\n\nReviewed By: xing-liu, yuchenhao\n\nDifferential Revision: D32748570\n\nfbshipit-source-id: bd73d7567f94c70daeace49d4081381b8adf2d77", "pr_number": "69193", "files_changed": ["aten/src/ATen/native/cuda/Copy.cu", "test/test_cuda.py"], "labels": ["fb-exported", "cla signed", "ciflow/default"]}, "397183f44c": {"title": "Add Lazy Tensor codegen infra (#69020)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/69020\n\nMerges the lazy tensor codegen infra which has already been used on lazy_tensor_staging.\n\nTest Plan: Test via lazy_tensor_staging branch\n\nReviewed By: alanwaketan, bdhirsh\n\nDifferential Revision: D32570613\n\nfbshipit-source-id: 2cd5698644398bda69669683f8de79fd3b6639b5", "pr_number": "69020", "files_changed": ["tools/codegen/api/lazy.py", "tools/codegen/dest/__init__.py", "tools/codegen/dest/lazy_ir.py", "tools/codegen/dest/lazy_ts_lowering.py", "tools/codegen/gen_backend_stubs.py", "tools/codegen/gen_lazy_tensor.py"], "labels": ["fb-exported", "cla signed", "ciflow/default"]}, "113684cf81": {"title": "Fix crash in `checkCustomClassType` if arg is null (#69259)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/69259\n\nOtherwise `checkCustomClassType(nullptr, new Type())` will crash\n\nTest Plan: Imported from OSS\n\nReviewed By: lw\n\nDifferential Revision: D32775297\n\nPulled By: malfet\n\nfbshipit-source-id: 54b10fd395d734c615dcaf85a5e599a445cee663", "pr_number": "69259", "files_changed": ["aten/src/ATen/core/ivalue.cpp"], "labels": ["cla signed", "ciflow/default", "ciflow/macos"]}, "78ab3cde4a": {"title": "Do not modify type map from getCustomClassTypeImpl() (#69261)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/69261\n\nAs this function is supposed to be called only once per type from\ncaching getCustomClassType template\n\nTest Plan: Imported from OSS\n\nReviewed By: suo, lw\n\nDifferential Revision: D32776564\n\nPulled By: malfet\n\nfbshipit-source-id: 218436657e6ad5ad0c87964857114d1e60c57140", "pr_number": "69261", "files_changed": ["aten/src/ATen/core/ivalue.h"], "labels": ["cla signed", "ciflow/default", "ciflow/macos"]}, "0de7a618a3": {"title": "functionalization: update is_aliased() logic (#68881)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/68881\n\nTest Plan: Imported from OSS\n\nReviewed By: zou3519\n\nDifferential Revision: D32647614\n\nPulled By: bdhirsh\n\nfbshipit-source-id: 6bec50d3e54419d1707d0b6c0c6729bcc1ced1f0", "pr_number": "68881", "files_changed": ["aten/src/ATen/FunctionalTensorWrapper.cpp", "aten/src/ATen/FunctionalTensorWrapper.h", "test/test_functionalization.py", "torch/csrc/autograd/python_torch_functions_manual.cpp"], "labels": ["cla signed", "ciflow/default"]}, "698c35e743": {"title": "Add functorch TLS to ATen/ThreadLocalState (#69181)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/69181\n\nfunctorch lives out-of-tree. However, it has some TLS that needs to be\npropagated. The solution for that is we store a pointer to the TLS\ninside pytorch/pytorch and extend FuncTorchTLSBase inside functorch to\ninclude whatever functorch needs.\n\nA previous solution used ThreadLocalDebugInfo. However, all\nPyTorch-managed threads (e.g. spawned by Autograd) all receive a\nshared_ptr that points to the same ThreadLocalDebugInfo. This leads to\nrace conditions if the multiple threads start modifying the TLS\nstored within ThreadLocalDebugInfo without using mutexes.\n\nTest Plan:\n- tested with functorch\n- The performance impact of this change when functorch is not used is\nnegligible because we end up manipulating nullptrs.\n\nReviewed By: albanD\n\nDifferential Revision: D32742312\n\nPulled By: zou3519\n\nfbshipit-source-id: 1a8439a4af06b3d3e50b9a2dbca98a0ba612062a", "pr_number": "69181", "files_changed": ["aten/src/ATen/FuncTorchTLS.cpp", "aten/src/ATen/FuncTorchTLS.h", "aten/src/ATen/ThreadLocalState.cpp", "aten/src/ATen/ThreadLocalState.h", "tools/build_variables.bzl"], "labels": ["cla signed", "ciflow/default"]}, "5c816520b3": {"title": "ns for fx: fix bug in graph matcher (#69238)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/69238\n\nThe NS for FX graph matcher was not properly taking into account\nseen_nodes, this allowed a node to be matched twice.\n\nTest Plan:\nFB-only testing on real model passes.\n\nIdeally we would have a test case to capture this, but hopefully we can land this soon to unblock production work.\n\nImported from OSS\n\nReviewed By: HDCharles\n\nDifferential Revision: D32765761\n\nfbshipit-source-id: ed3dff8fd981e399a649fcd406883b4d56cc712a", "pr_number": "69238", "files_changed": ["torch/ao/ns/fx/graph_matcher.py", "torch/ao/ns/fx/pattern_utils.py"], "labels": ["cla signed", "ciflow/default", "fx"]}, "84aa1ddedd": {"title": "[quant] Remove warning for quantized Tensor in `__dir__` (#69265)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/69265\n\nThis is used in tab completion, we should not put warning here\n\nTest Plan:\nci\n\nImported from OSS\n\nReviewed By: albanD\n\nDifferential Revision: D32778736\n\nfbshipit-source-id: f1bec5e09a8238ab41329ac2b64e6f3267799f6a", "pr_number": "69265", "files_changed": ["torch/_tensor.py"], "labels": ["cla signed", "ciflow/default"]}, "b22e4d4aea": {"title": "[PyTorch][SR] Add more to() tests & extend debug logging in testStaticRuntime (#67219)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/67219\n\nI found that these specific test cases were causing different failures when developing D31776259. I also found that it was difficult to debug testStaticRuntime failures, so I added more verbose logs gated behind -v 2.\nghstack-source-id: 144507287\n\nTest Plan: Used during development of D31776259\n\nReviewed By: hlu1\n\nDifferential Revision: D31847566\n\nfbshipit-source-id: ea9147fb246c345d18bbc8d7f3bfba48d3a0fab3", "pr_number": "67219", "files_changed": ["benchmarks/static_runtime/test_static_runtime.cc", "benchmarks/static_runtime/test_utils.cc"], "labels": ["cla signed", "ciflow/default"]}, "52219b1017": {"title": "Fix `ChainedScheduler.get_last_lr()` (#69112)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/68820\n\ncc vincentqb jbschlosser albanD\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/69112\n\nReviewed By: zou3519\n\nDifferential Revision: D32796626\n\nPulled By: albanD\n\nfbshipit-source-id: bde9d4e473527be4c0a7f21cb57f795a67a99eaa", "pr_number": "69112", "files_changed": ["test/test_optim.py", "torch/optim/lr_scheduler.py"], "labels": ["module: optimizer", "triaged", "open source", "cla signed", "ciflow/default"]}, "5b2586fe09": {"title": "[testing] Ignore expected_regex in assertRaisesRegex for non-native device (#68723)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/29719\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/68723\n\nReviewed By: zou3519\n\nDifferential Revision: D32797061\n\nPulled By: mruberry\n\nfbshipit-source-id: 3bcae6d3d62d180059dbe39be520b0e7f9aea19f", "pr_number": "68723", "files_changed": ["test/test_torch.py", "torch/testing/_internal/common_device_type.py", "torch/testing/_internal/common_utils.py"], "labels": ["open source", "cla signed", "ciflow/default"]}, "9f39a2de0a": {"title": "[fix] add range check for `k` kthvalue_cpu (#68863)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/68813\n\nLong-term it might make more sense to port it to structured\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/68863\n\nReviewed By: H-Huang\n\nDifferential Revision: D32749372\n\nPulled By: mruberry\n\nfbshipit-source-id: 85a1b2a31e922ff1df0d0f3f576ad219e652aa49", "pr_number": "68863", "files_changed": ["aten/src/ATen/native/Sorting.cpp", "aten/src/ATen/native/cuda/Sorting.cpp", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["triaged", "open source", "cla signed", "ciflow/default"]}, "33c3c539b6": {"title": "THPStorage: Prefer intrusive_ptr over owning raw pointers (#69248)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/69248\n\nReviewed By: zou3519\n\nDifferential Revision: D32771035\n\nPulled By: ngimel\n\nfbshipit-source-id: cf9bbcc5563ae9715ecf13631ba56c32240e59e3", "pr_number": "69248", "files_changed": ["c10/util/intrusive_ptr.h", "torch/csrc/generic/Storage.cpp", "torch/csrc/generic/Storage.h", "torch/csrc/generic/StorageMethods.cpp", "torch/csrc/generic/StorageSharing.cpp", "torch/csrc/generic/serialization.cpp", "torch/csrc/generic/serialization.h"], "labels": ["triaged", "open source", "cla signed", "ciflow/default"]}, "bede18b061": {"title": "Add support for C++ frontend wrapper on Linux (#69094)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/69094\n\nPartially addresses https://github.com/pytorch/pytorch/issues/68768\n\nTest Plan: Imported from OSS\n\nReviewed By: seemethere\n\nDifferential Revision: D32730079\n\nPulled By: malfet\n\nfbshipit-source-id: 854e4215ff66e087bdf354fed7a17e87f2649c87", "pr_number": "69094", "files_changed": ["test/test_utils.py", "torch/utils/cpp_extension.py"], "labels": ["cla signed", "ciflow/default"]}, "572c3e3118": {"title": "Fix some usages of CUDA_VERSION (#69092)", "body": "Summary:\nSee https://pytorch.slack.com/archives/G4Z791LL8/p1638229956006300\n\nI grepped c10, aten, and torch for CUDA_VERSION and checked the usages I saw.\nI can't guarantee I made a clean sweep. but this improves the status quo.\n\ncc ngimel\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/69092\n\nReviewed By: zou3519\n\nDifferential Revision: D32786919\n\nPulled By: ngimel\n\nfbshipit-source-id: 1d29827dca246f33118d81e136252ddb5bf3830f", "pr_number": "69092", "files_changed": ["aten/src/ATen/Dispatch.h", "aten/src/ATen/cuda/CUDABlas.cpp", "c10/util/complex.h"], "labels": ["module: cuda", "triaged", "open source", "cla signed", "ciflow/default"]}, "21919be96b": {"title": "CMake: Update precompiled header and fix support (#67851)", "body": "Summary:\nThis fixes the `USE_PRECOMPILED_HEADERS` cmake version check which was accidentally inverted, so it was always disabled.\n\nI've also made the precompiled header so it only includes headers used in 95% or more of code, weighted by compile time. This limits it to the standard library, `c10` and a limited subset of `ATen/core`. Crucially, the new pch doesn't depend on `native_functions.yaml` so won't cause as much unnecessary rebuilding.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/67851\n\nReviewed By: zou3519\n\nDifferential Revision: D32290902\n\nPulled By: dagitses\n\nfbshipit-source-id: dfc33330028c99b02ff40963926c1f1260d00d00", "pr_number": "67851", "files_changed": ["CMakeLists.txt", "aten/src/ATen/core/ATen_pch.h", "caffe2/CMakeLists.txt"], "labels": ["triaged", "open source", "cla signed", "ciflow/default"]}, "8f8524a447": {"title": "Expose is_metal_available in header (#68942)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/68942\n\nCurrently, `at::native::is_metal_available()` is implemented, but it's not exposed in the header, so nobody can use it. It's a useful function and I want to use it, so exposing it in the header.\n\nTest Plan: CI\n\nReviewed By: sodastsai, xta0\n\nDifferential Revision: D32675236\n\nfbshipit-source-id: b4e692db7d171dfb872d5c2233cc808d7131f2e9", "pr_number": "68942", "files_changed": ["aten/src/ATen/metal/Context.h"], "labels": ["fb-exported", "cla signed", "ciflow/default"]}, "a20b9f8d5c": {"title": "add HPU case for backend_to_string function (#69225)", "body": "Summary:\nChange-Id: If8ed7f1161343a2e494d8b964576e1ee193007f7\n\nFixes https://github.com/pytorch/pytorch/issues/65609\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/69225\n\nReviewed By: gchanan\n\nDifferential Revision: D32804545\n\nPulled By: wconstab\n\nfbshipit-source-id: bdf359bd779113153ebdecc515edba94e47e0ae4", "pr_number": "69225", "files_changed": ["torch/csrc/utils/tensor_types.cpp"], "labels": ["open source", "cla signed", "ciflow/default"]}, "088a4feb41": {"title": "Update the documentation for AMP with DataParallel (#69218)", "body": "Summary:\nFollowing https://github.com/pytorch/pytorch/issues/60540 and pull request https://github.com/pytorch/pytorch/issues/43102\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/69218\n\nReviewed By: gchanan\n\nDifferential Revision: D32803814\n\nPulled By: ngimel\n\nfbshipit-source-id: 06fdbbee2c7734153271be70ec4bc24263c8c367", "pr_number": "69218", "files_changed": ["docs/source/notes/amp_examples.rst"], "labels": ["triaged", "open source", "cla signed", "ciflow/default"]}, "a813ddf5ec": {"title": "CUDACachingAllocator: make an error message more accurate. (#69174)", "body": "Summary:\nThe `TORCH_CHECK` asserts for strictly-greater-than `kLargeBuffer`,\nbut the exception claims `>=`. Fix the error message to match the\ncode.\n\nHappy to open an issue if it's helpful; I was hopeful the trivial fix doesn't need a separate issue.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/69174\n\nReviewed By: zou3519\n\nDifferential Revision: D32760055\n\nPulled By: H-Huang\n\nfbshipit-source-id: 1a8ab68f36b326ed62d78afdcb198f4d6572d017", "pr_number": "69174", "files_changed": ["c10/cuda/CUDACachingAllocator.cpp"], "labels": ["open source", "cla signed", "ciflow/default"]}, "855365e9c4": {"title": "Clean up dead code (#69296)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/69296\n\nremove a commented block of code that was accidentally checked in\n\nTest Plan: no testable changes\n\nReviewed By: alanwaketan\n\nDifferential Revision: D32799197\n\nfbshipit-source-id: d3eb05cbafb0f5a4a3f41c17f66ca6d0c2fc60b7", "pr_number": "69296", "files_changed": ["tools/codegen/dest/lazy_ir.py"], "labels": ["fb-exported", "cla signed", "ciflow/default"]}, "a3ca4c83a6": {"title": "[PyTorch] Add torch::jit::toString(const Type&) (#66689)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66689\n\nLet's not take an extra refcount bump to stringify types.\nghstack-source-id: 144374720\n\nTest Plan: CI\n\nReviewed By: suo\n\nDifferential Revision: D31691526\n\nfbshipit-source-id: 673d632a83e6179c063530fdbc346c22d5f47d7c", "pr_number": "66689", "files_changed": ["aten/src/ATen/core/Dict_inl.h", "aten/src/ATen/core/List_inl.h", "aten/src/ATen/core/jit_type.h"], "labels": ["cla signed", "ciflow/default"]}, "da023611d7": {"title": "[CUDA graphs] Fixes make_graphed_callables example typos (#69379)", "body": "Summary:\ncc mcarilli\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/69379\n\nReviewed By: mruberry\n\nDifferential Revision: D32841260\n\nPulled By: ngimel\n\nfbshipit-source-id: a7d0b9db0578526907547b201eddd55827812b63", "pr_number": "69379", "files_changed": ["docs/source/notes/cuda.rst"], "labels": ["open source", "cla signed", "module: cuda graphs", "ciflow/default"]}, "9663e08674": {"title": "[Static Runtime] Fix a bug that aten::embedding_bag keeps cannot handle resized input tensors (#69219)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/69219\n\nThis change fixes a bug that `aten::embedding_bag` implementation does not adjust the size of a managed output tensor according to a given input after memory planning starts.\n\nTest Plan: Enhanced `StaticRuntime.EmbeddingBag` to trigger the existing bug that's fixed by this change.\n\nReviewed By: mikeiovine\n\nDifferential Revision: D32544399\n\nfbshipit-source-id: 0a9f1d453e96f0cfa8443c8d0b28bbc520e38b29", "pr_number": "69219", "files_changed": ["aten/src/ATen/native/EmbeddingBag.cpp", "benchmarks/static_runtime/test_static_runtime.cc"], "labels": ["fb-exported", "cla signed", "ciflow/default"]}, "0bbe21b172": {"title": "[LT] Upstream more util functions (#69098)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/69098\n\nAdd the following utils: helpers, ir_dump_util, and\ntensor_util. Some of the util functions may be better organized by\ngrouping into different files, but we can leave that for later.\n\nTest Plan: Imported from OSS\n\nReviewed By: alanwaketan\n\nDifferential Revision: D32758480\n\nPulled By: desertfire\n\nfbshipit-source-id: 2a0707879f0c49573380b4c8227a3c916c99bf9a", "pr_number": "69098", "files_changed": ["tools/build_variables.bzl", "torch/csrc/lazy/backend/backend_interface.cpp", "torch/csrc/lazy/core/hash.h", "torch/csrc/lazy/core/helpers.cpp", "torch/csrc/lazy/core/helpers.h", "torch/csrc/lazy/core/ir_dump_util.cpp", "torch/csrc/lazy/core/ir_dump_util.h", "torch/csrc/lazy/core/ir_util.cpp", "torch/csrc/lazy/core/ir_util.h", "torch/csrc/lazy/core/tensor_util.cpp", "torch/csrc/lazy/core/tensor_util.h"], "labels": ["cla signed", "ciflow/default"]}, "e8f4c9cc40": {"title": "[LT] Upstream LazyView and view ops IR Nodes (#69277)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/69277\n\nLazyView is the main class for tracking alias caused by view\nops. The corresponding IR classes for view ops are hand-written now, and\nwe can switch to code-gen them in future. For certain view ops, they\nhave a reverse IR class to perform inplace update in the backward\ndirection on a chain of alias ops.\n\nAs part of the future work, we will simplify the logic for LazyView once\nthe functionalization pass in core is ready to use.\n\nTest Plan: Imported from OSS\n\nReviewed By: wconstab\n\nDifferential Revision: D32820014\n\nPulled By: desertfire\n\nfbshipit-source-id: d9eb526cb23885f667e4815dc9dd291a7b7e4256", "pr_number": "69277", "files_changed": ["test/cpp/lazy/test_permutation_util.cpp", "tools/build_variables.bzl", "torch/csrc/lazy/core/internal_ops/ltc_ops.cpp", "torch/csrc/lazy/core/internal_ops/ltc_ops.h", "torch/csrc/lazy/core/lazy_view.cpp", "torch/csrc/lazy/core/lazy_view.h", "torch/csrc/lazy/core/permutation_util.h", "torch/csrc/lazy/core/view_ops/as_strided.cpp", "torch/csrc/lazy/core/view_ops/as_strided.h", "torch/csrc/lazy/core/view_ops/as_strided_view_update.cpp", "torch/csrc/lazy/core/view_ops/as_strided_view_update.h", "torch/csrc/lazy/core/view_ops/diagonal.cpp", "torch/csrc/lazy/core/view_ops/diagonal.h", "torch/csrc/lazy/core/view_ops/diagonal_view_update.cpp", "torch/csrc/lazy/core/view_ops/diagonal_view_update.h", "torch/csrc/lazy/core/view_ops/narrow.cpp", "torch/csrc/lazy/core/view_ops/narrow.h", "torch/csrc/lazy/core/view_ops/narrow_view_update.cpp", "torch/csrc/lazy/core/view_ops/narrow_view_update.h", "torch/csrc/lazy/core/view_ops/permute.cpp", "torch/csrc/lazy/core/view_ops/permute.h", "torch/csrc/lazy/core/view_ops/resize.cpp", "torch/csrc/lazy/core/view_ops/resize.h", "torch/csrc/lazy/core/view_ops/select.cpp", "torch/csrc/lazy/core/view_ops/select.h", "torch/csrc/lazy/core/view_ops/select_view_update.cpp", "torch/csrc/lazy/core/view_ops/select_view_update.h", "torch/csrc/lazy/core/view_ops/view.cpp", "torch/csrc/lazy/core/view_ops/view.h"], "labels": ["cla signed", "ciflow/default"]}, "b6f41bb848": {"title": "The Jiterator (#69439)", "body": "Summary:\nThis PR:\n\n- creates the \"jiterator\" pattern, allowing elementwise unary and binary kernels that don't accept scalars to be jit compiled when called\n- ports the gcd and i1 CUDA kernels to use the jiterator\n- extends elementwise binary systemic testing to be comparable to elementwise unary systemic testing\n- separates one test case from test_out in test_ops.py\n- updates more OpInfos to use expected failures instead of skips\n\nThe jiterator currently does not support half, bfloat16 or complex dtypes. It also (as mentioned above) doesn't support scalar inputs. In the future we expect to add support for those datatypes and scalars.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/69439\n\nReviewed By: ngimel\n\nDifferential Revision: D32874968\n\nPulled By: mruberry\n\nfbshipit-source-id: d44bb9cde4f602703e75400ec5a0b209f085e9b3", "pr_number": "69439", "files_changed": ["aten/src/ATen/core/Array.h", "aten/src/ATen/cudnn/Utils.h", "aten/src/ATen/native/cpu/BinaryOpsKernel.cpp", "aten/src/ATen/native/cuda/CUDALoops.cuh", "aten/src/ATen/native/cuda/GcdLcmKernel.cu", "aten/src/ATen/native/cuda/Loops.cuh", "aten/src/ATen/native/cuda/Math.cuh", "aten/src/ATen/native/cuda/MemoryAccess.cuh", "aten/src/ATen/native/cuda/UnarySpecialOpsKernel.cu", "aten/src/ATen/native/cuda/ZetaKernel.cu", "aten/src/ATen/native/cuda/jit_utils.cu", "aten/src/ATen/native/cuda/jit_utils.h", "test/test_binary_ufuncs.py", "test/test_ops.py", "test/test_unary_ufuncs.py", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["cla signed", "ciflow/default", "ciflow/all"]}, "78b7a419b2": {"title": "Enable native_dropout/backward for lazy (#69374)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/69374\n\nEnables existing native_dropout operator for use with lazy tensors.  Also adds aten interned strings so lazy tensor codegen can refer to the symbols in generated IR classes.\n\nTest Plan: CI for regressions of existing use cases, and manual tests of new Lazy Tensor functionality\n\nReviewed By: ngimel\n\nDifferential Revision: D32837301\n\nfbshipit-source-id: a372a24ec65367fb84ad2e97c7e38cae4ec703a6", "pr_number": "69374", "files_changed": ["aten/src/ATen/core/aten_interned_strings.h", "aten/src/ATen/native/Dropout.cpp"], "labels": ["fb-exported", "cla signed", "ciflow/default"]}, "a974699633": {"title": "Skips failing ROCm test (#69456)", "body": "Summary:\nROCm and CUDA type promotion are slightly divergent and need to be updated.\n\ncc jeffdaily sunway513 jithunnair-amd ROCmSupport KyleCZH\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/69456\n\nReviewed By: anjali411, janeyx99\n\nDifferential Revision: D32883895\n\nPulled By: mruberry\n\nfbshipit-source-id: 3b0ba8a9d092c2d7ff20d78da42d4a147b1db12d", "pr_number": "69456", "files_changed": ["test/test_binary_ufuncs.py"], "labels": ["module: rocm", "cla signed", "ciflow/default"]}, "bf01cd5228": {"title": "Move THC_sleep to ATen (#69038)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/69038\n\nTest Plan: Imported from OSS\n\nReviewed By: anjali411\n\nDifferential Revision: D32872479\n\nPulled By: ngimel\n\nfbshipit-source-id: 97c7592b16eee2ecc66c42507c358aa92cc8ee50", "pr_number": "69038", "files_changed": ["BUILD.bazel", "aten/src/ATen/cuda/Sleep.cu", "aten/src/ATen/cuda/Sleep.h", "aten/src/THC/CMakeLists.txt", "aten/src/THC/THC.h", "aten/src/THC/THCSleep.cu", "aten/src/THC/THCSleep.h", "torch/csrc/cuda/Module.cpp"], "labels": ["open source", "cla signed", "ciflow/default"]}, "38c576cfef": {"title": "Clean up CODEOWNERS for .github/ (#69395)", "body": "Summary:\nCleans up the CODEOWNERS file to reflect current team\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/69395\n\nTest Plan: yeah_sandcastle\n\nReviewed By: anjali411\n\nDifferential Revision: D32885237\n\nPulled By: seemethere\n\nfbshipit-source-id: a465f2cd0e27d5e53f5af5769d1cad47ec5348e7", "pr_number": "69395", "files_changed": ["CODEOWNERS"], "labels": ["cla signed", "ciflow/none"]}, "4d81175a07": {"title": "add VSX dispatch for fft_fill_with_conjugate_symmetry_stub (#68914)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/68057.\n\nAs discussed in https://github.com/pytorch/pytorch/issues/68057 adding change to provide the missing dispatch for VSX.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/68914\n\nReviewed By: seemethere\n\nDifferential Revision: D32696773\n\nPulled By: malfet\n\nfbshipit-source-id: f1b70ab85bf9fb1c0119cc70d6125b8801d95669", "pr_number": "68914", "files_changed": ["aten/src/ATen/native/mkl/SpectralOps.cpp"], "labels": ["open source", "cla signed", "ciflow/default"]}, "59e98b66ac": {"title": "Revert D32704467: [Autograd/Checkpoint] Checkpoint implementation without reentrant autograd", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD32704467 (https://github.com/pytorch/pytorch/commit/e032dae32904c6e90353e90051785514d6c5a7d9)\n\nOriginal commit changeset: 6eea1cce6b93\n\nfbshipit-source-id: 1a788c1fd57cee46bba82e216e6162d078359cc2", "pr_number": null, "files_changed": ["test/test_autograd.py", "torch/utils/checkpoint.py"], "labels": []}, "3edf1b6cee": {"title": "[PyTorch] Avoid no-op shared_ptr dtor when constructing tuple (#69337)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/69337\n\nSee note in code.\nghstack-source-id: 144657751\n\nTest Plan:\nRan PyTorchFeatureConversionBenchmark 5x before/after:\n\n```\nswolchok@devbig032 ~/f/fbcode> for x in (seq 5); sudo scripts/bertrand/noise/denoise.sh /tmp/pytorch_feature_conversion_benchmark.Dec2CacheTupleTypes ; end                                                                                                                                                                                              (pytorch-ort-bert)\n============================================================================\nsigrid/lib/features/tests/PyTorchFeatureConversionBenchmark.cpprelative  time/iter  iters/s\n============================================================================\nPyTorchFeatureConversionDenseBenchmark                       2.39us  418.75K\nPyTorchFeatureConversionIdListBenchmark                      3.59us  278.91K\nPyTorchFeatureConversionIdScoreListBenchmark                 5.01us  199.51K\n============================================================================\n============================================================================\nsigrid/lib/features/tests/PyTorchFeatureConversionBenchmark.cpprelative  time/iter  iters/s\n============================================================================\nPyTorchFeatureConversionDenseBenchmark                       2.42us  413.80K\nPyTorchFeatureConversionIdListBenchmark                      3.56us  280.60K\nPyTorchFeatureConversionIdScoreListBenchmark                 5.05us  198.15K\n============================================================================\n============================================================================\nsigrid/lib/features/tests/PyTorchFeatureConversionBenchmark.cpprelative  time/iter  iters/s\n============================================================================\nPyTorchFeatureConversionDenseBenchmark                       2.41us  414.25K\nPyTorchFeatureConversionIdListBenchmark                      3.55us  281.59K\nPyTorchFeatureConversionIdScoreListBenchmark                 5.02us  199.09K\n============================================================================\n============================================================================\nsigrid/lib/features/tests/PyTorchFeatureConversionBenchmark.cpprelative  time/iter  iters/s\n============================================================================\nPyTorchFeatureConversionDenseBenchmark                       2.39us  417.68K\nPyTorchFeatureConversionIdListBenchmark                      3.55us  281.65K\nPyTorchFeatureConversionIdScoreListBenchmark                 5.05us  198.06K\n============================================================================\n============================================================================\nsigrid/lib/features/tests/PyTorchFeatureConversionBenchmark.cpprelative  time/iter  iters/s\n============================================================================\nPyTorchFeatureConversionDenseBenchmark                       2.39us  417.54K\nPyTorchFeatureConversionIdListBenchmark                      3.56us  281.03K\nPyTorchFeatureConversionIdScoreListBenchmark                 5.05us  198.13K\n============================================================================\nswolchok@devbig032 ~/f/fbcode> for x in (seq 5); sudo scripts/bertrand/noise/denoise.sh /tmp/pytorch_feature_conversion_benchmark.Dec2TupleConstruction ; end                                                                                                                                                                                            (pytorch-ort-bert)\n============================================================================\nsigrid/lib/features/tests/PyTorchFeatureConversionBenchmark.cpprelative  time/iter  iters/s\n============================================================================\nPyTorchFeatureConversionDenseBenchmark                       2.38us  420.38K\nPyTorchFeatureConversionIdListBenchmark                      3.53us  282.90K\nPyTorchFeatureConversionIdScoreListBenchmark                 4.99us  200.41K\n============================================================================\n============================================================================\nsigrid/lib/features/tests/PyTorchFeatureConversionBenchmark.cpprelative  time/iter  iters/s\n============================================================================\nPyTorchFeatureConversionDenseBenchmark                       2.37us  421.54K\nPyTorchFeatureConversionIdListBenchmark                      3.54us  282.27K\nPyTorchFeatureConversionIdScoreListBenchmark                 4.99us  200.28K\n============================================================================\n============================================================================\nsigrid/lib/features/tests/PyTorchFeatureConversionBenchmark.cpprelative  time/iter  iters/s\n============================================================================\nPyTorchFeatureConversionDenseBenchmark                       2.38us  420.99K\nPyTorchFeatureConversionIdListBenchmark                      3.56us  280.56K\nPyTorchFeatureConversionIdScoreListBenchmark                 5.08us  196.91K\n============================================================================\n============================================================================\nsigrid/lib/features/tests/PyTorchFeatureConversionBenchmark.cpprelative  time/iter  iters/s\n============================================================================\nPyTorchFeatureConversionDenseBenchmark                       2.37us  421.48K\nPyTorchFeatureConversionIdListBenchmark                      3.54us  282.87K\nPyTorchFeatureConversionIdScoreListBenchmark                 5.00us  199.88K\n============================================================================\n============================================================================\nsigrid/lib/features/tests/PyTorchFeatureConversionBenchmark.cpprelative  time/iter  iters/s\n============================================================================\nPyTorchFeatureConversionDenseBenchmark                       2.38us  419.69K\nPyTorchFeatureConversionIdListBenchmark                      3.56us  280.68K\nPyTorchFeatureConversionIdScoreListBenchmark                 4.97us  201.23K\n============================================================================\n```\n\nLooks like maybe around 1% faster?\n\nReviewed By: hlu1\n\nDifferential Revision: D32817592\n\nfbshipit-source-id: 4b015dc993b26a92e45a3673e14fde32105a34fa", "pr_number": "69337", "files_changed": ["aten/src/ATen/core/ivalue_inl.h"], "labels": ["cla signed", "ciflow/default"]}, "1a202b0c39": {"title": "Docs: Fix broken code syntax in autograd.rst (#69362)", "body": "Summary:\nThe backticks around `nn.Parameters` were not rendered correctly because the word was enclosed in an italics block.\nSpotted the issue on https://pytorch.org/docs/stable/notes/autograd.html#locally-disable-grad-doc.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/69362\n\nReviewed By: zou3519\n\nDifferential Revision: D32924093\n\nPulled By: albanD\n\nfbshipit-source-id: 5a310ac3f3d13a5116f7aa911817b9452eee711d", "pr_number": "69362", "files_changed": ["docs/source/notes/autograd.rst"], "labels": ["triaged", "open source", "cla signed", "ciflow/default"]}, "358e908162": {"title": "Add Union type to TorchScript Language Ref (#69514)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/69514\n\nReviewed By: tugsbayasgalan\n\nDifferential Revision: D32909371\n\nPulled By: gmagogsfm\n\nfbshipit-source-id: af1c3040cd59ee913dc576cf8a8c759313f1e07f", "pr_number": "69514", "files_changed": ["docs/source/jit_language_reference_v2.rst"], "labels": ["cla signed", "ciflow/default"]}, "6df7b75186": {"title": "skip ORT tensor in TensorIterator because it doesn't have storage (#68705)", "body": "Summary:\nORT Tensors are similar to XLA tensors which doesn't have storage. So extend the condition to ORT tensors.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/68705\n\nReviewed By: zou3519\n\nDifferential Revision: D32921378\n\nPulled By: albanD\n\nfbshipit-source-id: 3bda9bba2ddd95cb561a4d1cff463de652256708", "pr_number": "68705", "files_changed": ["aten/src/ATen/TensorIterator.cpp"], "labels": ["triaged", "open source", "cla signed", "ciflow/default"]}, "829b49b867": {"title": "Output UnionType str rep with () instead of [] (#69502)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/69502\n\nTest Plan: Imported from OSS\n\nReviewed By: pbelevich\n\nDifferential Revision: D32902781\n\nPulled By: tugsbayasgalan\n\nfbshipit-source-id: 67a73b209575437477cdbd3eb8f685019709e99c", "pr_number": "69502", "files_changed": ["aten/src/ATen/core/jit_type.h", "aten/src/ATen/core/type.cpp", "test/jit/test_union.py"], "labels": ["cla signed", "ciflow/default"]}, "9a7732e852": {"title": "CMake: Support dynamic codegen outputs (#68246)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/68246\n\nCurrently the codegen produces a list of output files at CMake\nconfiguration time and the build system has no way of knowing if the\noutputs change. So if that happens, you basically need to delete the\nbuild folder and re-run from scratch.\n\nInstead, this generates the output list every time the code generation\nis run and changes the output to be a `.cmake` file that gets included\nin the main cmake configuration step. That means the build system\nknows to re-run cmake automatically if a new output is added. So, for\nexample you could change the number of shards that `Operators.cpp` is\nsplit into and it all just works transparently to the user.\n\nTest Plan: Imported from OSS\n\nReviewed By: zou3519\n\nDifferential Revision: D32596268\n\nPulled By: albanD\n\nfbshipit-source-id: 15e0896aeaead90aed64b9c8fda70cf28fef13a2", "pr_number": "68246", "files_changed": ["cmake/Codegen.cmake", "tools/codegen/gen.py", "tools/codegen/utils.py"], "labels": ["open source", "cla signed", "ciflow/default", "ciflow/all"]}, "3456c2cbc8": {"title": "Allow build_android.sh to forward Vulkan args (#69332)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/69332\n\n ---\n\n## Context\n\nThe `build_android.sh` script currently does not forward Vulkan configuration options, which makes it impossible to control them when running `build_pytorch_android.sh`.\n\n## Changes\n\nSlightly change the script to allow Vulkan configuration options to propagate from `build_pytorch_android.sh` to `build_android.sh`\n\nTest Plan: Imported from OSS\n\nReviewed By: beback4u\n\nDifferential Revision: D32840908\n\nPulled By: SS-JIA\n\nfbshipit-source-id: e55d89c93c996b92b743cf047f5a285bb516bbc4", "pr_number": "69332", "files_changed": ["scripts/build_android.sh"], "labels": ["cla signed", "ciflow/default"]}, "049debd97d": {"title": "[Reland][Autograd/Checkpoint] Checkpoint implementation without reentrant autograd (#69508)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/69508\n\nOriginal Phabricator Diff: D32704467 (https://github.com/pytorch/pytorch/commit/e032dae32904c6e90353e90051785514d6c5a7d9)\n\nReland, fix is to not test traditional checkpoint when input does not require grad as that is unsupported as documented.\n\nOriginal PR body:\n\nResubmission of https://github.com/pytorch/pytorch/pull/62964 with the\nsuggestions and tests discussed in\nhttps://github.com/pytorch/pytorch/issues/65537.\n\nAdds a `use_reentrant=False` flag to `checkpoint` function. When\n`use_reentrant=True` is specified, a checkpointing implementation that uses\nSavedVariableHooks instead of re-entrant autograd is used. This makes it more\ncomposable with things such as `autograd.grad` as well as DDP (still need to\nadd thorough distributed testing).\n\nAs discussed in https://github.com/pytorch/pytorch/issues/65537, the tests that we need to add are:\n\n- [x] Gradient hooks are called once\n- [x] works when input does require grads but Tensor that require grads are captures (like first layer in a nn)\n- [x] works for functions with arbitrary input/output objects\n- [x] distributed tests (next PR)\n\nNote that this is only for `torch.utils.checkpoint`, if this approach overall looks good, we will do something similar for `checkpoint_sequential`.\nghstack-source-id: 144948501\n\nTest Plan: CI\n\nReviewed By: zhaojuanmao\n\nDifferential Revision: D32902634\n\nfbshipit-source-id: 2ee87006e5045e5471ff80c36a07fbecc2bea3fe", "pr_number": "69508", "files_changed": ["test/test_autograd.py", "torch/utils/checkpoint.py"], "labels": ["cla signed", "ci/master", "ciflow/default"]}, "8a975c0106": {"title": "[LT] Sync with the lazy_tensor_staging branch (#69527)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/69527\n\n- Add missing TORCH_API in class/struct declarations;\n- Fix internal op declarations in ltc_ops;\n- Update lazy_ts_lowering.py\n\nTest Plan: Imported from OSS\n\nReviewed By: alanwaketan\n\nDifferential Revision: D32918929\n\nPulled By: desertfire\n\nfbshipit-source-id: e956d51aff5ef593fdf4cd5ad2a38e38788913d8", "pr_number": "69527", "files_changed": ["tools/build_variables.bzl", "tools/codegen/dest/lazy_ts_lowering.py", "torch/csrc/lazy/core/internal_ops/ltc_ops.cpp", "torch/csrc/lazy/core/internal_ops/ltc_ops.h", "torch/csrc/lazy/core/lazy_view.h", "torch/csrc/lazy/core/view_ops/as_strided.h", "torch/csrc/lazy/core/view_ops/as_strided_view_update.h", "torch/csrc/lazy/core/view_ops/diagonal.h", "torch/csrc/lazy/core/view_ops/diagonal_view_update.h", "torch/csrc/lazy/core/view_ops/narrow.h", "torch/csrc/lazy/core/view_ops/narrow_view_update.h", "torch/csrc/lazy/core/view_ops/permute.h", "torch/csrc/lazy/core/view_ops/resize.h", "torch/csrc/lazy/core/view_ops/select.h", "torch/csrc/lazy/core/view_ops/select_view_update.h", "torch/csrc/lazy/core/view_ops/view.cpp", "torch/csrc/lazy/core/view_ops/view.h"], "labels": ["cla signed", "ciflow/default"]}, "2d38d37f5f": {"title": "use irange for loops (#69533)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/69533\n\nModified loops in files under fbsource/fbcode/caffe2/ from the format\n```\nfor(TYPE var=x0;var<x_max;x++)\n```\nto the format\n```\nfor(const auto var: irange(xmax))\n```\n\nThis was achieved by running r-barnes's loop upgrader script (D28874212) with some modification to exclude all files under /torch/jit and a number of reversions or unused variable suppression warnings added by hand.\n\nTest Plan: Sandcastle\n\nReviewed By: malfet\n\nDifferential Revision: D32837942\n\nfbshipit-source-id: 8663037a38ade8f81bd5e983a614d197ea11f0d1", "pr_number": "69533", "files_changed": ["caffe2/sgd/adagrad_op.h", "caffe2/sgd/adam_op.h", "caffe2/sgd/learning_rate_adaption_op.h", "caffe2/sgd/learning_rate_op.h", "caffe2/sgd/momentum_sgd_op.h", "caffe2/sgd/rowwise_adagrad_fused.h", "caffe2/sgd/rowwise_counter.h", "caffe2/sgd/storm_op.h", "caffe2/sgd/wngrad_op.h", "caffe2/sgd/yellowfin_op.h", "caffe2/transforms/pattern_net_transform.h", "caffe2/utils/proto_utils.h", "caffe2/utils/threadpool/WorkersPool.h", "caffe2/video/video_input_op.h"], "labels": ["fb-exported", "cla signed", "ciflow/default"]}, "fc8404b5bc": {"title": "histc: Avoid dispatch in parallel region (#68520)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/68520\n\nRef #56794\n\nThis changes the code from allocating 1 tensor per thread inside the\nparallel region, to allocating one larger tensor outside the parallel\nregion and manually viewing each thread's slice of the histogram.\n\nTest Plan: Imported from OSS\n\nReviewed By: zou3519\n\nDifferential Revision: D32929365\n\nPulled By: ngimel\n\nfbshipit-source-id: e28da2736e849a0282b70f34d11526d3355d5bd5", "pr_number": "68520", "files_changed": ["aten/src/ATen/native/cpu/HistogramKernel.cpp"], "labels": ["open source", "cla signed", "ciflow/default"]}, "8b20dde932": {"title": "add python dispatch test back to CI and fix typo in test (#69565)", "body": "Summary:\nThe error message was changed following a PR comment. And since the test doesn't run on CI, I forgot to update the test to catch the new error message.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/69565\n\nReviewed By: mrshenli\n\nDifferential Revision: D32932982\n\nPulled By: albanD\n\nfbshipit-source-id: a1da72b0ca735e72b481bc944039233094f1c422", "pr_number": "69565", "files_changed": ["test/run_test.py", "test/test_python_dispatch.py"], "labels": ["cla signed", "ciflow/default"]}, "ee60b5ddf3": {"title": "Improve efficiency of shape hash by not using tostring (#69496)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/69496\n\ntostring is expensive, and this is equivalent and faster\n\nTest Plan: covered by lazy tensor unit tests\n\nReviewed By: desertfire, alanwaketan\n\nDifferential Revision: D32901050\n\nfbshipit-source-id: 34080f415db5fd5d3817f7f2533f062a6ec07d21", "pr_number": "69496", "files_changed": ["torch/csrc/lazy/core/shape.cpp", "torch/csrc/lazy/core/shape.h", "torch/csrc/lazy/ts_backend/ts_node.cpp"], "labels": ["fb-exported", "cla signed", "ciflow/default"]}, "30bb4e0071": {"title": "Add nvidia-smi memory and utilization as native Python API (#69104)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/69104\n\nAdd nvidia-smi memory and utilization as native Python API\n\nTest Plan:\ntesting the function returns the appropriate value.\nUnit tests to come.\n\nReviewed By: malfet\n\nDifferential Revision: D32711562\n\nfbshipit-source-id: 01e676203299f8fde4f3ed4065f68b497e62a789", "pr_number": "69104", "files_changed": ["docs/source/cuda.rst", "torch/cuda/__init__.py"], "labels": ["fb-exported", "cla signed", "ciflow/default"]}, "2808563e69": {"title": "Forward fix for failing master (#69625)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/69625\n\nTest Plan: Imported from OSS\n\nReviewed By: albanD\n\nDifferential Revision: D32959635\n\nPulled By: anjali411\n\nfbshipit-source-id: 4d811c6a05deb991cb2886dd65b3f6059555b395", "pr_number": "69625", "files_changed": ["aten/src/ATen/native/MathBitFallThroughLists.h"], "labels": ["cla signed", "ciflow/default"]}, "b10381f42d": {"title": "Port smooth_l1_loss to structured kernels (#67404)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/67404\n\nPort smooth_l1_loss to structured kernels.\n\nBrian Hirsh authored the part of adding build_borrowing_binary_op_coerce_to_scalar to TensorIterator.\n\nTest Plan: This commit shouldn't change the behavior. So, CI.\n\nReviewed By: bdhirsh, ngimel\n\nDifferential Revision: D31981147\n\nPulled By: alanwaketan\n\nfbshipit-source-id: a779bb76c848eed8b725dc0e1d56b97a3bd9c158", "pr_number": "67404", "files_changed": ["aten/src/ATen/native/BinaryOps.h", "aten/src/ATen/native/Loss.cpp", "aten/src/ATen/native/cpu/BinaryOpsKernel.cpp", "aten/src/ATen/native/cuda/BinaryMiscOpsKernels.cu", "aten/src/ATen/native/native_functions.yaml"], "labels": ["cla signed", "ciflow/default"]}, "3b27304d20": {"title": "Fix typos in ATen README (#69170)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/69170\n\nReviewed By: mrshenli\n\nDifferential Revision: D32957504\n\nPulled By: H-Huang\n\nfbshipit-source-id: d8e613b67a864f95e45b2d45398ee71efde0c567", "pr_number": "69170", "files_changed": ["aten/src/ATen/native/README.md"], "labels": ["open source", "cla signed", "ciflow/default"]}, "7956a405ef": {"title": "Make make_dual also return namedtuple when level less than zero (#68628)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/68628\n\nTest Plan: Imported from OSS\n\nReviewed By: jbschlosser\n\nDifferential Revision: D32899681\n\nPulled By: soulitzer\n\nfbshipit-source-id: 61ed09f4038e19817978a521e9571fdc482b424b", "pr_number": "68628", "files_changed": ["torch/autograd/forward_ad.py"], "labels": ["cla signed", "ciflow/default"]}, "b61c532f96": {"title": "Make make_dual redispatch (#68630)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/68630\n\nConstraints:\n1) (functorch) if all the inputs to an op have requires_grad=False and don't have tangents, then their VariableType\n    kernel should be a no-op i.e., behave like a redispatch. This is due to functorch's DynamicLayerStack\n   having the autograd key by default (which is so that transformations like vmap) still work with autograd\n2) (inference mode) inference tensors in inference mode will call straight into the kernel, we should still do something sensible\n    inside even if we normally wouldn't redispatch into it.\n3) ~Should support potential application of interposition below autograd: `nn.Parameter` is a example of subclassing where the subclass\n    is not preserved when an operation is performed. There is an exception though: we want calling `make_dual` on a\n    `nn.Parameter` to preserve its parameterness.~\n4) Should avoid calls to shallow_copy_and_detach to avoid spurious calls into `__python_dispatch__`.\n\nThis PR:\n- does not redispatch to `make_dual` from its `ADInplaceOrView` kernel to satisfy (1)\n- calls into `alias` from the kernel in the native namespace so that behavior is consistent with other views in inference mode to satisfy (2)\n- discussion of (3). We still wouldn't be able to directly override `make_dual` below autograd. In this PR, instead of not redispatching at all, we choose to redispatch into `at::alias` so that one can override `make_dual`. The side effect is that one would not be able to distinguish calls between the two, which can be problematic (though a straightforward but hacky solution would be to create a new `at::alias_for_make_dual` that would allow users to distinguish) the two. This isn't ideal but seems to be the simplest way to satisfy (3). We don't pursue that hacky solution here.\n- (4) is satisfied because we remove calls to `shallow_copy_and_detach`\n\n<details>\n<summary> A potentially less hacky but more involved solution? (WIP) </summary>\n\nRealizing that make_dual is more like requires_grad, perhaps it shouldn't be autograd explicit? Make make_dual a composite or python-only construct. i.e., it would be a view on the primal followed by something to the effect of primal.set_fw_grad(tangent).\n\nAdditional constraints:\n5) make_dual needs to be backward-differentiable (I can't think of any applications yet becuase\n   technically as a high-order function, jvp's input is the tangent only, \"detach\" is not applied on\n   the tangent, so one would still be able to propagate gradients through it).\n6) set_fw_grad needs to raise an error if there is a layout mismatch and base is a forward-differnentiable view\n\nPossible plan\n- (6) implies that a plain view would not suffice. We need a `detach`-like operation to ensure that set_fw_grad\n  knows the view is not forward differentiable.\n- (5) implies that is this (new) `detach` would need to be backward differentiable (API TBD).\n- (3) is no longer relevant because make_dual is no longer autograd explicit, but perhaps this new detach should behave like the current one? There is a lot of logic to replicate for detach, so this may be hard.\n- (1) is satisfied if we use current detach logic, i.e., , and (4) is trivial.\n\nI'm not convinced that this is the right solution either, because in the end does (3) still work?\n\n </details>\n\nTest Plan: Imported from OSS\n\nReviewed By: jbschlosser\n\nDifferential Revision: D32899679\n\nPulled By: soulitzer\n\nfbshipit-source-id: 98e13ae954e14e1e68dbd03eb5ab3300d5ed2c5e", "pr_number": "68630", "files_changed": ["aten/src/ATen/native/AutogradComposite.cpp", "aten/src/ATen/native/VariableMethodStubs.cpp", "test/test_autograd.py", "torch/csrc/autograd/VariableTypeManual.cpp"], "labels": ["cla signed", "ciflow/default"]}, "ecf9c82f24": {"title": "Reduce binary size of TensorCompare.cu (#68835)", "body": "Summary:\nThis PR does several things\n1) eliminates `where` instantiations for deprecated `byte` condition dtype, and casts `condition` to `bool` in this case. This is a perf penalty for people using deprecated calls\n2) Makes `clamp_{min/max}.Tensor` overload reuse `clamp_{min/max}.Scalar` kernels if limit argument is cpu scalar, instead of instantiating `gpu_kernel_with_scalars`\n3) Unifies all clamp_scalar kernels to use a single kernel with lambda picking the correct operation. I've verified that it doesn't degrade kernel performance.\n4) Eliminates redundant TensorIterator construction that `clamp` structured kernel was doing when only `min` or `max` was specified\n\nThis reduces the cubin size for TensorCompare.cu on V100 from 15751920 bytes to 7691120 bytes, with corresponding reduction in compile time.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/68835\n\nReviewed By: mruberry\n\nDifferential Revision: D32839241\n\nPulled By: ngimel\n\nfbshipit-source-id: 0acde5af10a767264afbdb24684b137c5544b8d9", "pr_number": "68835", "files_changed": ["aten/src/ATen/native/TensorCompare.cpp", "aten/src/ATen/native/TensorCompare.h", "aten/src/ATen/native/cpu/TensorCompareKernel.cpp", "aten/src/ATen/native/cuda/TensorCompare.cu"], "labels": ["cla signed", "ciflow/default"]}, "dc87cf5fe1": {"title": "Fixes mem_get_info when querying on a device other than the current device (#69640)", "body": "Summary:\nAlso fixes the documentation failing to appear and adds a test to validate that op works with multiple devices properly.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/69640\n\nReviewed By: ngimel\n\nDifferential Revision: D32965391\n\nPulled By: mruberry\n\nfbshipit-source-id: 4fe502809b353464da8edf62d92ca9863804f08e", "pr_number": "69640", "files_changed": ["docs/source/cuda.rst", "test/test_cuda.py", "torch/csrc/cuda/shared/cudart.cpp"], "labels": ["cla signed", "ciflow/default"]}, "d2917f705a": {"title": "Fix errors in `common_utils.py` (#69578)", "body": "Summary:\nThis fixes the following error:\n```python\nTraceback (most recent call last):\n  File \"/home/gaoxiang/pytorch-ucc2/test/distributed/test_distributed_spawn.py\", line 40, in <module>\n    run_tests()\n  File \"/home/gaoxiang/.local/lib/python3.9/site-packages/torch/testing/_internal/common_utils.py\", line 618, in run_tests\n    ['--import-slow-tests'] if IMPORT_SLOW_TESTS else List[str]([]))\n  File \"/usr/lib/python3.9/typing.py\", line 680, in __call__\n    raise TypeError(f\"Type {self._name} cannot be instantiated; \"\nTypeError: Type List cannot be instantiated; use list() instead\nTraceback (most recent call last):\n  File \"/home/gaoxiang/pytorch-ucc2/test/run_test.py\", line 1058, in <module>\n    main()\n  File \"/home/gaoxiang/pytorch-ucc2/test/run_test.py\", line 1036, in main\n    raise RuntimeError(err_message)\nRuntimeError: distributed/test_distributed_spawn failed!\n```\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/69578\n\nReviewed By: mrshenli\n\nDifferential Revision: D32963113\n\nPulled By: malfet\n\nfbshipit-source-id: b064e230c5e572e890b4ac66ebdda2707b8c12d7", "pr_number": "69578", "files_changed": ["torch/testing/_internal/common_utils.py"], "labels": ["open source", "cla signed", "ciflow/default"]}, "6de9f0fc94": {"title": "OpInfo: Allow sample_inputs_func to be any iterable (#69256)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/69256\n\nCloses #52486\n\nTest Plan: Imported from OSS\n\nReviewed By: mrshenli\n\nDifferential Revision: D32942008\n\nPulled By: mruberry\n\nfbshipit-source-id: f5b01b0298c0160b0bec6e86e2b6db8cfe746206", "pr_number": "69256", "files_changed": ["test/jit/test_dtype_analysis.py", "test/test_ops.py", "test/test_sparse.py", "test/test_sparse_csr.py", "torch/testing/_internal/common_methods_invocations.py", "torch/testing/_internal/common_utils.py"], "labels": ["open source", "cla signed", "ciflow/default", "ciflow/all"]}, "0ccb1dcdbb": {"title": "Fix inference_mode decorator (#68617)", "body": "Summary:\nThis fixes the case when `torch.inference_mode` is called with `mode=False` (disabled). When used as a decorator, it ignored the argument and enabled inference mode anyway.\n\n`_DecoratorContextManager` is changed so that a new instance is a copy instead of a new instance with default parameters.\n\nI also added more tests to cover this case.\n\nCurrent behaviour:\n\n```python\n>>> import torch\n>>> x = torch.ones(1, 2, 3, requires_grad=True)\n>>> torch.inference_mode(mode=False)\n... def func(x):\n...     return x * x\n...\n>>> out = func(x)\n>>> out.requires_grad\nFalse\n```\n\nNew behaviour (fixed):\n\n```python\n>>> import torch\n>>> x = torch.ones(1, 2, 3, requires_grad=True)\n>>> torch.inference_mode(mode=False)\n... def func(x):\n...     return x * x\n...\n>>> out = func(x)\n>>> out.requires_grad\nTrue\n```\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/68617\n\nReviewed By: mrshenli\n\nDifferential Revision: D32958434\n\nPulled By: albanD\n\nfbshipit-source-id: 133c69970ef8bffb9fc9ab5142dedcffc4c32945", "pr_number": "68617", "files_changed": ["test/test_autograd.py", "torch/autograd/grad_mode.py"], "labels": ["triaged", "open source", "cla signed", "ciflow/default"]}, "9ad05f2c3a": {"title": "Upgrade oneDNN to v2.3.3 and package oneDNN Graph API together (#63748)", "body": "Summary:\nThis PR upgrades oneDNN to [v2.3.3](https://github.com/oneapi-src/oneDNN/releases/tag/v2.3.3) and includes [Graph API preview release](https://github.com/oneapi-src/oneDNN/releases/tag/graph-v0.2) in one package.\n\n- oneDNN will be located at `pytorch/third_party/ideep/mkl-dnn/third_party/oneDNN`\n- The version of oneDNN will be [v2.3.3](https://github.com/oneapi-src/oneDNN/releases/tag/v2.3.3)\n  The main changes on CPU:\n\n  - v2.3\n    - Extended primitive cache to improve primitive descriptor creation performance.\n    - Improved primitive cache performance in multithreaded configurations.\n    - Introduced initial optimizations for bfloat16 compute functionality for future Intel Xeon Scalable processor (code name Sapphire Rapids).\n    - Improved performance of binary primitive and binary post-op for cases with broadcast and mixed source and destination formats.\n    - Improved performance of reduction primitive\n    - Improved performance of depthwise convolution primitive with NHWC activations for training cases\n  - v2.3.1\n    -  Improved int8 GEMM performance for processors with Intel AVX2 and Intel DL Boost support\n    - Fixed integer overflow for inner product implementation on CPUs\n    - Fixed out of bounds access in GEMM implementation for Intel SSE 4.1\n  - v2.3.2\n    - Fixed performance regression in fp32 inner product primitive for processors with Intel AVX512 support\n  - v2.3.3\n    - Reverted check for memory descriptor stride validity for unit dimensions\n    - Fixed memory leak in CPU GEMM implementation\n\n  More changes can be found in https://github.com/oneapi-src/oneDNN/releases.\n- The Graph API provides flexible API for aggressive fusion, and the preview2 supports fusion for FP32 inference.  See the [Graph API release branch](https://github.com/oneapi-src/oneDNN/tree/dev-graph-preview2) and [spec](https://spec.oneapi.io/onednn-graph/latest/introduction.html) for more details. A separate PR will be submitted to integrate the oneDNN Graph API to Torchscript graph.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/63748\n\nReviewed By: albanD\n\nDifferential Revision: D32153889\n\nPulled By: malfet\n\nfbshipit-source-id: 536071168ffe312d452f75d54f34c336ca3778c1", "pr_number": "63748", "files_changed": ["cmake/Modules/FindMKLDNN.cmake", "third_party/ideep", "third_party/mkl-dnn.BUILD"], "labels": ["triaged", "open source", "cla signed", "ciflow/default", "intel priority"]}, "41e1ab0785": {"title": "Introduce isTensorSubclassLike; add special cases to backwards formulas (#69534)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/69534\n\nSomething is TensorSubclassLike if it is a Tensor subclass or if it has\nthe same problems as Tensor subclasses. Today that just includes Tensor\nSubclasses and meta tensors but may include other things in the future.\n\nSome of our backwards formulas are incompatible with TensorSubclassLike\nobjects. For example, calling .data_ptr() is a problem because many\nTensorSubclassLike objects don't have storage. Another problem is\nin-place operations: performing `regular_tensor.inplace_(tensor_subclass)`\nis a problem.\n\nThis PR adds special cases to the backward formulas for torch.max and\ntorch.clamp to handle this. The backward formulas for torch.max and\ntorch.clamp are not dispatcher operations so they cannot be overridden\nand we hesitate to make them dispatcher operations for FC/BC concerns\nand performance overhead concerns.\n\nFurthermore, the old concept of \"is this inplace operation vmap\ncompatible?\" can be subsumed by the general \"is this inplace operation\ntensor-subclass compatible\" question, so I replaced all instances of\nisInplaceVmapCompatible and replaced it with the isTensorSubclassLike\nchecks.\n\nTest Plan\n- I tested the changes using functorch.\n- It's possible to write a test for these in core (one has to make\na custom tensor subclass and then send it through the operation and then\ninvoke autograd), but I wanted to push the work to doing some\ngeneric testing for backward formulas\n(https://github.com/pytorch/pytorch/issues/69530) instead of doing some\none-off things now.\n\nTest Plan: Imported from OSS\n\nReviewed By: mrshenli\n\nDifferential Revision: D32967727\n\nPulled By: zou3519\n\nfbshipit-source-id: 30fda1a7581da4c55179b7a3ca05069150bbe2dc", "pr_number": "69534", "files_changed": ["aten/src/ATen/TensorSubclassLikeUtils.h", "aten/src/ATen/native/Resize.cpp", "torch/csrc/autograd/FunctionsManual.cpp"], "labels": ["cla signed", "ciflow/default"]}, "be757addfa": {"title": "Do not use `std::labs` (#69704)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/69704\n\nInstead, compute size diff inside the if statement\n\nTest Plan: Imported from OSS\n\nReviewed By: zou3519, seemethere\n\nDifferential Revision: D32997004\n\nPulled By: malfet\n\nfbshipit-source-id: a23819240bfe8278a11ebc6bae1e856de162f082", "pr_number": "69704", "files_changed": ["aten/src/ATen/TensorNames.cpp"], "labels": ["cla signed", "ciflow/default"]}, "9962bfb3c9": {"title": "Remove THTensor (#69040)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/69040\n\nTest Plan: Imported from OSS\n\nReviewed By: anjali411\n\nDifferential Revision: D32872478\n\nPulled By: ngimel\n\nfbshipit-source-id: f93e16509d64308d91e374744410a6a811e7f4e3", "pr_number": "69040", "files_changed": ["BUILD.bazel", "aten/src/TH/CMakeLists.txt", "aten/src/TH/TH.h", "aten/src/TH/THTensor.cpp", "aten/src/TH/THTensor.h", "aten/src/TH/THTensor.hpp", "aten/src/TH/generic/THTensor.cpp", "aten/src/TH/generic/THTensor.h", "tools/build_variables.bzl", "torch/csrc/PythonTypes.h", "torch/csrc/THP.h", "torch/csrc/Types.h", "torch/csrc/cuda/Storage.cpp", "torch/csrc/cuda/override_macros.h", "torch/csrc/cuda/undef_macros.h", "torch/csrc/utils.cpp"], "labels": ["open source", "cla signed", "ciflow/default"]}, "7ea5926130": {"title": "Make blend operations clang-Wall clean (#69705)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/69705\n\nTest Plan: Imported from OSS\n\nReviewed By: r-barnes\n\nDifferential Revision: D32997007\n\nPulled By: malfet\n\nfbshipit-source-id: cbadc44e1e7373800e94b7b2fd2711530854978c", "pr_number": "69705", "files_changed": ["aten/src/ATen/cpu/vec/vec256/vec256_complex_double.h", "aten/src/ATen/cpu/vec/vec256/vec256_complex_float.h", "aten/src/ATen/cpu/vec/vec512/vec512_complex_float.h"], "labels": ["cla signed", "ciflow/default"]}, "3219f6a487": {"title": "Make vec512 bfloat16 map function clang-Wall clean (#69707)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/69707\n\n`const` modifier for `__m512` return value doesn't make much sense\n\nTest Plan: Imported from OSS\n\nReviewed By: r-barnes\n\nDifferential Revision: D32997008\n\nPulled By: malfet\n\nfbshipit-source-id: fb98659713fe2a23cc702252c0655106687f0dbf", "pr_number": "69707", "files_changed": ["aten/src/ATen/cpu/vec/vec512/vec512_bfloat16.h"], "labels": ["cla signed", "ciflow/default"]}, "1d269e8c15": {"title": "[PyTorch] Simple refcount bump fixes in standardizeVectorForUnion & callees (#66695)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66695\n\nMore extra reference counting in this path.\nghstack-source-id: 145125484\n\nTest Plan: CI\n\nReviewed By: suo\n\nDifferential Revision: D31692197\n\nfbshipit-source-id: 126b6c72efbef9410d4c2e61179b6b67459afc23", "pr_number": "66695", "files_changed": ["aten/src/ATen/core/type.cpp"], "labels": ["cla signed", "ciflow/default"]}, "d026057bb3": {"title": "[PyTorch] Update SmallVector from LLVM (#69110)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/69110\n\nI pasted the current LLVM code, reapplied the modifications listed in the code comments, caught a few more in the diff/build process. The trivially copyable detection is different now; if gcc builds fail, will try reverting to C10_IS_TRIVIALLY_COPYABLE or copying what LLVM is doing.\n\nThe motivation for this change is that, as noted in an existing comment, C10_IS_TRIVIALLY_COPYABLE did the wrong thing for std::unique_ptr, which caused problems with D32454856 / #68412.\n\nghstack-source-id: 145327773\n\nTest Plan: CI\n\nReviewed By: bhosmer, mruberry\n\nDifferential Revision: D32733017\n\nfbshipit-source-id: 9452ab90328e3fdf457aad23a26f2f6835b0bd3d", "pr_number": "69110", "files_changed": ["c10/macros/Macros.h", "c10/test/util/SmallVectorTest.cpp", "c10/util/ArrayRef.h", "c10/util/SmallVector.cpp", "c10/util/SmallVector.h"], "labels": ["cla signed", "ciflow/default", "ciflow/all"]}, "e305e4d4d8": {"title": "Suppress common warnings when building by clang (#69710)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/69710\n\nNamely no range-loop-analysis (that detect when loop variable can not be const reference\n\nTest Plan: Imported from OSS\n\nReviewed By: r-barnes\n\nDifferential Revision: D32997003\n\nPulled By: malfet\n\nfbshipit-source-id: dba0e7875e5b667e2cc394c70dd75e2403265918", "pr_number": "69710", "files_changed": ["CMakeLists.txt", "cmake/public/utils.cmake", "torch/CMakeLists.txt"], "labels": ["cla signed", "ciflow/default"]}, "59deee8308": {"title": "Make c10 tests compilable with -Werror (#69711)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/69711\n\nTest Plan: Imported from OSS\n\nReviewed By: r-barnes\n\nDifferential Revision: D32997005\n\nPulled By: malfet\n\nfbshipit-source-id: 369194051ece9d213b48584ca84e5d76b3794dae", "pr_number": "69711", "files_changed": ["c10/test/core/impl/SizesAndStrides_test.cpp", "c10/test/util/intrusive_ptr_test.cpp"], "labels": ["cla signed", "ciflow/default"]}, "19fecc63e4": {"title": "[PyTorch][kineto] Remove heap-allocated vectors in saveExtraArgs (#69737)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/69737\n\nWe can use stack allocation instead.\nghstack-source-id: 145312454\n\nTest Plan: Ran internal framework overhead benchmark with --stressTestKinto --kinetoAddFlops, but difference was minimal. Still good to fix.\n\nReviewed By: chowarfb\n\nDifferential Revision: D33007329\n\nfbshipit-source-id: e096312fef5b729cf12580be152c9418683745b8", "pr_number": "69737", "files_changed": ["torch/csrc/autograd/profiler_utils.cpp"], "labels": ["cla signed", "ciflow/default"]}, "3906f8247a": {"title": "clear predict_net field from PredictorExporterMeta stored in the exporter to save memory (#68485)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/68485\n\nIn OSS, the only change is that we make the predict_net field of PredictorExporterMeta nullable.\n\nTest Plan: sandcastle, let CI run\n\nReviewed By: boryiingsu\n\nDifferential Revision: D32467138\n\nfbshipit-source-id: 81bd5fca695462f6a186bcfa927073874cc9c26a", "pr_number": "68485", "files_changed": ["caffe2/python/predictor/predictor_exporter.py"], "labels": ["fb-exported", "cla signed", "ciflow/default"]}, "f565167fbd": {"title": "Revert D32606547: torch/monitor: add C++ events and handlers", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD32606547 (https://github.com/pytorch/pytorch/commit/e61fc1c03b64e61ca4f5bbe278db7ee2cf35e8ff)\n\nOriginal commit changeset: a00d0364092d\n\nOriginal Phabricator Diff: D32606547 (https://github.com/pytorch/pytorch/commit/e61fc1c03b64e61ca4f5bbe278db7ee2cf35e8ff)\n\nfbshipit-source-id: fbaf2cc06ad4bec606e8a9c6f591d65c04e6fa56", "pr_number": null, "files_changed": ["test/cpp/monitor/test_counters.cpp", "test/cpp/monitor/test_events.cpp", "tools/build_variables.bzl", "torch/csrc/monitor/counters.cpp", "torch/csrc/monitor/counters.h", "torch/csrc/monitor/events.cpp", "torch/csrc/monitor/events.h"], "labels": []}, "db32daf4b2": {"title": "Do not test batched forward grad for inplace ops (#69558)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/69558\n\nCurrently we skip batched forward grad checks completely for certain views that also have inplace variants. This PR allow us to decouple the check.\n\nAlternative: just skip the batched forward checks for inplace ops entirely. I'm okay with this because it was surprising to me these checks are being run in the first place.\n\nTest Plan: Imported from OSS\n\nReviewed By: albanD\n\nDifferential Revision: D33020599\n\nPulled By: soulitzer\n\nfbshipit-source-id: f8012aadc0e775f80da0ab62b2c11f6645bb1f51", "pr_number": "69558", "files_changed": ["test/test_ops.py", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["cla signed", "ciflow/default"]}, "baf92f9d5a": {"title": "Fix copy_ forward AD to handle broadcasting (#69592)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/69592\n\nCurrently, forward AD function for`copy_` (in `VariableTypeManual`) does not handle the broadcasting case. ~EDIT: but that is not a design decision, not a bug. In this PR, we make that clear as a comment.~\n\nNote: `broadcast_to` does not have a batching rule in core, so the ops that rely on `copy_` to broadcast will still fail batched forward grad computation.\n\nTest Plan: Imported from OSS\n\nReviewed By: albanD\n\nDifferential Revision: D33020603\n\nPulled By: soulitzer\n\nfbshipit-source-id: 09cb702bffc74061964a9c05cfef5121f8164814", "pr_number": "69592", "files_changed": ["test/test_autograd.py", "torch/csrc/autograd/VariableTypeManual.cpp"], "labels": ["cla signed", "ciflow/default"]}, "b08d64202a": {"title": "Remove THGeneral (#69041)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/69041\n\n`TH_CONCAT_{N}` is still being used by THP so I've moved that into\nit's own header but all the compiled code is gone.\n\nTest Plan: Imported from OSS\n\nReviewed By: anjali411\n\nDifferential Revision: D32872477\n\nPulled By: ngimel\n\nfbshipit-source-id: 06c82d8f96dbcee0715be407c61dfc7d7e8be47a", "pr_number": "69041", "files_changed": ["BUILD.bazel", "CONTRIBUTING.md", "aten/src/ATen/CMakeLists.txt", "aten/src/TH/CMakeLists.txt", "aten/src/TH/TH.h", "aten/src/TH/THGeneral.cpp", "aten/src/TH/THGeneral.h.in", "aten/src/TH/THHalf.h", "c10/util/Exception.h", "tools/build_variables.bzl", "torch/csrc/Generator.cpp", "torch/csrc/Module.cpp", "torch/csrc/Storage.cpp", "torch/csrc/Storage.h", "torch/csrc/THConcat.h", "torch/csrc/THP.h", "torch/csrc/Types.h", "torch/csrc/autograd/engine.cpp", "torch/csrc/cuda/Module.cpp", "torch/csrc/cuda/restore_macros.h", "torch/csrc/generic/StorageMethods.cpp", "torch/csrc/utils.h", "torch/csrc/utils/byte_order.cpp", "torch/csrc/utils/byte_order.h"], "labels": ["open source", "cla signed", "ciflow/default"]}, "b1ef56d646": {"title": "[quant][docs] quantized model save/load instructions (#69789)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/69789\n\nAdd details on how to save and load quantized models without hitting errors\n\nTest Plan:\nCI autogenerated docs\n\nImported from OSS\n\nReviewed By: jerryzh168\n\nDifferential Revision: D33030991\n\nfbshipit-source-id: 8ec4610ae6d5bcbdd3c5e3bb725f2b06af960d52", "pr_number": "69789", "files_changed": ["docs/source/quantization.rst"], "labels": ["cla signed", "ciflow/default", "ciflow/docs"]}, "6078e12ad6": {"title": "Add forward AD support for as_strided (#68629)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/68629\n\nTest Plan: Imported from OSS\n\nReviewed By: albanD\n\nDifferential Revision: D32899680\n\nPulled By: soulitzer\n\nfbshipit-source-id: b80ba4483c06108938923f17dc67278b854515ef", "pr_number": "68629", "files_changed": ["test/test_view_ops.py"], "labels": ["cla signed", "ciflow/default"]}, "4829dcea09": {"title": "Codegen: Generate seperate headers per operator (#68247)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/68247\n\nThis splits `Functions.h`, `Operators.h`, `NativeFunctions.h` and\n`NativeMetaFunctions.h` into seperate headers per operator base name.\nWith `at::sum` as an example, we can include:\n```cpp\n<ATen/core/sum.h>         // Like Functions.h\n<ATen/core/sum_ops.h>     // Like Operators.h\n<ATen/core/sum_native.h>  // Like NativeFunctions.h\n<ATen/core/sum_meta.h>    // Like NativeMetaFunctions.h\n```\n\nThe umbrella headers are still being generated, but all they do is\ninclude from the `ATen/ops' folder.\n\nFurther, `TensorBody.h` now only includes the operators that have\nmethod variants. Which means files that only include `Tensor.h` don't\nneed to be rebuilt when you modify function-only operators. Currently\nthere are about 680 operators that don't have method variants, so this\nis potentially a significant win for incremental builds.\n\nTest Plan: Imported from OSS\n\nReviewed By: mrshenli\n\nDifferential Revision: D32596272\n\nPulled By: albanD\n\nfbshipit-source-id: 447671b2b6adc1364f66ed9717c896dae25fa272", "pr_number": "68247", "files_changed": ["BUILD.bazel", "CMakeLists.txt", "aten.bzl", "aten/src/ATen/CMakeLists.txt", "aten/src/ATen/core/CheckMemoryFormat.h", "aten/src/ATen/templates/Function.h", "aten/src/ATen/templates/Functions.h", "aten/src/ATen/templates/MethodOperators.h", "aten/src/ATen/templates/NativeFunction.h", "aten/src/ATen/templates/NativeFunctions.h", "aten/src/ATen/templates/NativeMetaFunction.h", "aten/src/ATen/templates/NativeMetaFunctions.h", "aten/src/ATen/templates/Operator.h", "aten/src/ATen/templates/Operators.cpp", "aten/src/ATen/templates/Operators.h", "aten/src/ATen/templates/RedispatchFunctions.h", "aten/src/ATen/templates/TensorBody.h", "cmake/Codegen.cmake", "setup.py", "tools/codegen/gen.py", "tools/linter/clang_tidy/generate_build_files.py", "torch/CMakeLists.txt"], "labels": ["open source", "module: codegen", "cla signed", "ciflow/default"]}, "fa615b332d": {"title": "added set_printoptions examples (#68324)", "body": "Summary:\nAdded examples for `torch.set_printoptions`\n\n```\n>>> torch.set_printoptions(precision=2)\n>>> torch.tensor([1.12345])\ntensor([1.12])\n>>> torch.set_printoptions(threshold=5)\n>>> torch.arange(10)\ntensor([0, 1, 2, ..., 7, 8, 9])\n```\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/68324\n\nReviewed By: ngimel\n\nDifferential Revision: D33063869\n\nPulled By: anjali411\n\nfbshipit-source-id: 24db99df1419f96ba8ae2b5217cb039b288b630a", "pr_number": "68324", "files_changed": ["torch/_tensor_str.py"], "labels": ["triaged", "open source", "cla signed", "ciflow/default"]}, "fdcb78df38": {"title": "`print` fix in `lr_scheduler` (#68338)", "body": "Summary:\n`{:5d}` fails for `CosineAnnealingWarmRestarts` which has float `epoch`\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/68338\n\nReviewed By: jbschlosser\n\nDifferential Revision: D33063970\n\nPulled By: albanD\n\nfbshipit-source-id: 992e987f8d5f6f8f5067924df4671e9725b6d884", "pr_number": "68338", "files_changed": ["torch/optim/lr_scheduler.py"], "labels": ["triaged", "open source", "cla signed", "ciflow/default"]}, "07767569c9": {"title": "Properly import LooseVersion (#69904)", "body": "Summary:\nThis fixes regression introduced by https://github.com/pytorch/pytorch/pull/57040\n\nSomehow importing `distutils` from `setuptool` caused import of\n`distutils.versions`, which is not a documented dependency and got\nchange with the release of\n[setuptools-59.6.0](https://github.com/pypa/setuptools/tree/v59.6.0)\nWe should not rely on that, as\n`import distutils` never re-imports `distutils.version`, which one can\nsee by observing\nhttps://github.com/python/cpython/blob/3.9/Lib/distutils/__init__.py\nor by running:\n```\n% python3 -c \"import distutils;print(distutils.__version__, dir(distutils))\"\n3.7.5 ['__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', '__version__', 'sys']\n% python3 -c \"from setuptools import distutils;print(distutils.__version__, dir(distutils))\"\n3.7.5 ['__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', '__version__', 'archive_util', 'ccompiler', 'cmd', 'config', 'core', 'debug', 'dep_util', 'dir_util', 'dist', 'errors', 'extension', 'fancy_getopt', 'file_util', 'filelist', 'log', 'spawn', 'sys', 'sysconfig', 'util', 'version']\n```\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/69904\n\nReviewed By: albanD, atalman, janeyx99\n\nDifferential Revision: D33094453\n\nPulled By: malfet\n\nfbshipit-source-id: aaf1adb7c6f293c4e376ccff21c64cd6ba625e97", "pr_number": "69904", "files_changed": ["test/run_test.py", "tools/setup_helpers/cmake.py", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["cla signed", "ciflow/default"]}, "2c9dd886af": {"title": "Modify torch.movedim to handle scalar as no-op (#69537)", "body": "Summary:\n`torch.movedim` directly handle the case of a scalar tensor (0-dim) in input as a no-op by returning a view of the input tensor (after all the usual checks for the other parameters)\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/69537\n\nTest Plan:\nThis code now works fine and res1 is a view of tensor\n```\nimport torch\n\ntensor = torch.rand(torch.Size([]))\nres1 = torch.movedim(tensor, 0, 0)\n```\n\nFixes https://github.com/pytorch/pytorch/issues/69432\n\nReviewed By: jbschlosser\n\nDifferential Revision: D33020014\n\nPulled By: albanD\n\nfbshipit-source-id: b3b2d380d70158bd3b3d6b40c073377104e09007", "pr_number": "69537", "files_changed": ["aten/src/ATen/native/TensorShape.cpp"], "labels": ["triaged", "open source", "module: viewing and reshaping", "cla signed", "ciflow/default"]}, "8acd0a8b2f": {"title": "Allow row sizes to support int64/size_t. (#69303)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/69303\n\nPull Request resolved: https://github.com/pytorch/FBGEMM/pull/792\n\nFollow up to D32715453 (https://github.com/pytorch/pytorch/commit/e60fd1065926f7f25173dc08f3b2f2091d0af9c2), allowing row size to be 64-bit.\n\nTest Plan:\nbuck test mode/opt -c fbcode.caffe2_gpu_type=v100,a100 //deeplearning/fbgemm/fbgemm_gpu:quantize_ops_test\n   buck test mode/opt -c fbcode.caffe2_gpu_type=none //deeplearning/fbgemm/fbgemm_gpu:quantize_ops_test\n   buck test mode/opt //caffe2/test:\n\nReviewed By: jspark1105, jianyuh\n\nDifferential Revision: D32768838\n\nfbshipit-source-id: 9e2b01d8d23e71f8333820e725379c3fc1c0711a", "pr_number": "69303", "files_changed": ["caffe2/perfkernels/fused_nbit_rowwise_conversion.cc", "caffe2/perfkernels/fused_nbit_rowwise_conversion.h"], "labels": ["fb-exported", "cla signed", "ciflow/default"]}, "1188d89a1d": {"title": "TestMathBits: Call functions with original sample input values (#68947)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/68947\n\n`_test_math_view` currently calls the operator with different values\nthan those specified in the `SampleInput`. This is undesirable as it\ncould break mathematical properties required by the operator. Instead,\nthis calls `math_op_view(math_op_physical(sample.input))` to get a\nview that represents the same value as the original input.\n\n`test_neg_view` already did this by returning `torch._neg_view(-x)`\nfrom `math_op_view` but this moves the handling into `_test_math_view`\nto make it apply to all view op tests.\n\nTest Plan: Imported from OSS\n\nReviewed By: jbschlosser\n\nDifferential Revision: D33064327\n\nPulled By: anjali411\n\nfbshipit-source-id: 4d87e0c04fc39b95f8dc30dcabda0d554d16a1d8", "pr_number": "68947", "files_changed": ["test/test_ops.py", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["open source", "cla signed", "ciflow/default"]}, "fef9981998": {"title": "Update run_test.py (#69920)", "body": "Summary:\nDo not compare LooseVersion against string\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/69920\n\nReviewed By: atalman\n\nDifferential Revision: D33101166\n\nPulled By: malfet\n\nfbshipit-source-id: a2df9e01d17663262718f11e580c8b009764f7b5", "pr_number": "69920", "files_changed": ["test/run_test.py"], "labels": ["cla signed", "ciflow/win"]}, "269e92669a": {"title": "[c2] Remove unused private fields (#69709)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/69709\n\nFix logical bug in `caffe2/ideep/operators/conv_op.cc`, which\ncontained an always false statement (fusion_type_ == X && fusion_type_ == Y ) statement\n\nTest Plan: Imported from OSS\n\nReviewed By: r-barnes\n\nDifferential Revision: D32997006\n\nPulled By: malfet\n\nfbshipit-source-id: 23e4db1b17cf8a77eae6a8691847ffa484d4736c", "pr_number": "69709", "files_changed": ["caffe2/ideep/operators/conv_op.cc", "caffe2/ideep/operators/conv_transpose_unpool_base_op.h", "caffe2/ideep/operators/local_response_normalization_op.cc", "caffe2/operators/quantized/int8_channel_shuffle_op.h", "caffe2/operators/quantized/int8_leaky_relu_op.h", "caffe2/operators/quantized/int8_relu_op.h", "caffe2/operators/quantized/int8_sigmoid_op.h", "caffe2/operators/quantized/int8_softmax_op.h", "caffe2/operators/self_binning_histogram_op.h", "caffe2/sgd/learning_rate_functors.h"], "labels": ["cla signed", "ciflow/default"]}, "d4f8313497": {"title": "Add low level torch.profiler.kineto_profile base class (#63302)", "body": "Summary:\nRefactor torch.profiler.profile by separate it into one low level class and one high level wrapper.\n\nThe PR include the following change:\n1. separate class torch.profiler.profile into two separated class: kineto_profiler and torch.profiler.profile.\n2. The former class has the low-level functionality exposed in C++ level like: prepare_profiler, start_profiler, stop_profiler.\n3. The original logics in torch.profiler.profile including export_chrome_trace, export_stacks, key_averages, events, add_metadata are all moved into kineto_profiler since they are all exposed by the torch.autograd.profiler.\n4. The new torch.profiler.profile is fully back-compatible with original class since it inherit from torch.profiler.kineto_profiler. Its only responsibility in new implementation is the maintenance of the finite state machine of ProfilerAction.\n\nWith the refactoring, the responsibility boundary is clear and the new logic is simple to understand.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/63302\n\nReviewed By: albanD\n\nDifferential Revision: D33006442\n\nPulled By: robieta\n\nfbshipit-source-id: 30d7c9f5c101638703f1243fb2fcc6ced47fb690", "pr_number": "63302", "files_changed": ["docs/source/profiler.rst", "torch/profiler/__init__.py", "torch/profiler/profiler.py"], "labels": ["triaged", "open source", "cla signed", "ciflow/default", "msft-collab"]}, "24ee1d13f6": {"title": "Another attempt to fix version comparison check (#69939)", "body": "Summary:\nFixes #{issue number}\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/69939\n\nReviewed By: atalman\n\nDifferential Revision: D33108135\n\nPulled By: malfet\n\nfbshipit-source-id: cadadfe5b04c4378f149136f8e1f8e8d6266775c", "pr_number": "69939", "files_changed": ["test/run_test.py"], "labels": ["cla signed", "ciflow/win"]}, "0ef523633f": {"title": "Revert D32498570: make codegen'd device guards not cuda-specific. Allow them to be used in external codegen", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD32498570 (https://github.com/pytorch/pytorch/commit/2e7a91c45feb18151b3797ef1e88e5e839749f6d)\n\nOriginal commit changeset: 0ce6a5614417\n\nOriginal Phabricator Diff: D32498570 (https://github.com/pytorch/pytorch/commit/2e7a91c45feb18151b3797ef1e88e5e839749f6d)\n\nfbshipit-source-id: 7c64ce1b5e51a680b4aeae8721e0c9e15c793289", "pr_number": null, "files_changed": ["tools/codegen/dest/register_dispatch_key.py", "tools/codegen/gen.py", "tools/codegen/gen_backend_stubs.py", "tools/codegen/model.py", "tools/test/test_gen_backend_stubs.py"], "labels": []}, "f6cad53443": {"title": "Revert D32498569: allow external backend codegen to toggle whether to generate out= and inplace kernels", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD32498569 (https://github.com/pytorch/pytorch/commit/aa0cf68c1767bea886a3caaea49ac197e0ff64cf)\n\nOriginal commit changeset: ebd932d042b9\n\nOriginal Phabricator Diff: D32498569 (https://github.com/pytorch/pytorch/commit/aa0cf68c1767bea886a3caaea49ac197e0ff64cf)\n\nfbshipit-source-id: 21a393fa339510d926512a7983d33ece327b743d", "pr_number": null, "files_changed": ["test/run_test.py", "test/test_gen_backend_stubs.py", "tools/codegen/gen_backend_stubs.py", "tools/test/test_gen_backend_stubs.py"], "labels": []}, "33363cea64": {"title": "Revert D32498572: allow external backend codegen to be used without autograd kernels", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD32498572 (https://github.com/pytorch/pytorch/commit/b83b6f74241de2c80d323c0d8a17904c98623dcb)\n\nOriginal commit changeset: 3e7159c633f6\n\nOriginal Phabricator Diff: D32498572 (https://github.com/pytorch/pytorch/commit/b83b6f74241de2c80d323c0d8a17904c98623dcb)\n\nfbshipit-source-id: f93fa444c95a2423eef5975a2ecdb96f14e0c535", "pr_number": null, "files_changed": ["tools/codegen/gen_backend_stubs.py", "tools/codegen/gen_lazy_tensor.py"], "labels": []}, "ebc35a7ead": {"title": "[JIT] Enable freezing for sparse COO tensors (#69614)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/69614\n\nPreviously sparse COO tensors were ignored during freezing, because\n`tryInsertConstant` would fail during `freeze_module.cpp`, and because\nhashes weren't implemented for COO tensor IValues.\n\nTest Plan: Imported from OSS\n\nReviewed By: mrshenli\n\nDifferential Revision: D32954620\n\nPulled By: davidberard98\n\nfbshipit-source-id: a91f97fdfc2152b417f43a6948100c94970c0831", "pr_number": "69614", "files_changed": ["aten/src/ATen/core/ivalue.h", "aten/src/ATen/test/ivalue_test.cpp", "test/jit/test_sparse.py", "test/test_jit.py"], "labels": ["oncall: jit", "cla signed", "ciflow/default"]}, "d71b8e1a8d": {"title": "More distutils.version.LooseVersion changes (#69947)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/69947\n\nReviewed By: seemethere\n\nDifferential Revision: D33111996\n\nPulled By: malfet\n\nfbshipit-source-id: e7d2cc4ed3e39452e809965e360b05f0b409ec0d", "pr_number": "69947", "files_changed": ["torch/testing/_internal/common_cuda.py", "torch/utils/tensorboard/__init__.py"], "labels": ["cla signed", "ciflow/default"]}, "29914f55bf": {"title": "Skip print_test_stats checks for tests that use repeat_test_for_types (#69872)", "body": "Summary:\nOnce https://github.com/pytorch/pytorch/issues/69865 is fixed, this change should be undone.\n\nThis will avoid print_test_stats errors in CI, such as https://github.com/pytorch/pytorch/runs/4501145212?check_suite_focus=true (HUD view https://hud.pytorch.org/commit/pytorch/pytorch/fc37e5b3ed0e51a0dff39da6e3a1a3ce16fe2756)\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/69872\n\nReviewed By: dagitses, suo\n\nDifferential Revision: D33094446\n\nPulled By: janeyx99\n\nfbshipit-source-id: 7378556d75ea94dd407a2bf9dda37b15c57014f7", "pr_number": "69872", "files_changed": ["tools/stats/print_test_stats.py"], "labels": ["cla signed", "ciflow/default"]}, "aeedd89d4e": {"title": "[PyTorch] RecordFunction: use SmallVector for ObserverContextList (#68412)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/68412\n\nThese lists have the same size as CallbackHandles, so they should be the same container type.\nghstack-source-id: 145668416\n\nTest Plan:\nRun same command as previous diff.\n\nBefore: see previous diff, average about 0.46us\nAfter: P467928077, average about 0.43us\n\nReviewed By: chaekit\n\nDifferential Revision: D32454856\n\nfbshipit-source-id: 3a3ff4d381d99f51ef868d4dec4db7c411b5ea56", "pr_number": "68412", "files_changed": ["aten/src/ATen/record_function.h"], "labels": ["cla signed", "ciflow/default"]}, "c6bcfb152d": {"title": "[PyTorch][easy] Move GlobalRecordFunctionCallbacks{,Entry} to cpp file (#68483)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/68483\n\nDoesn't need to be in the header.\nghstack-source-id: 145668417\n\nTest Plan: CI\n\nReviewed By: chaekit\n\nDifferential Revision: D32477113\n\nfbshipit-source-id: 30e7796413e3220e4051544559f9110ab745022d", "pr_number": "68483", "files_changed": ["aten/src/ATen/record_function.cpp", "aten/src/ATen/record_function.h"], "labels": ["cla signed", "ciflow/default"]}, "587f8d9924": {"title": "OperatorEntry: Avoid unnecessarily templated code (#67986)", "body": "Summary:\n`assertSignatureIsCorrect` is instantiated at minimum once per unique operator signature yet its core logic is independent of the type. So, it makes sense to have a light-weight template that does nothing but call into the non-templated function with the correct `CppSignature` object.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/67986\n\nReviewed By: jbschlosser\n\nDifferential Revision: D33108600\n\nPulled By: swolchok\n\nfbshipit-source-id: 7594524d3156ff2422e6edcdffcb263dc67ea346", "pr_number": "67986", "files_changed": ["aten/src/ATen/core/dispatch/OperatorEntry.cpp", "aten/src/ATen/core/dispatch/OperatorEntry.h"], "labels": ["triaged", "open source", "cla signed", "ciflow/default"]}, "bab61be43b": {"title": "Codegen: Add root_name property to NativeFunction{,sGroup} (#68687)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/68687\n\nThis adds `NativeFunction.root_name` which is the canonical name\nfor the operator group. i.e. the BaseOperatorName without inplace or\ndouble-underscores. In the previous PR I referred to this as\n`base_name` but confusingly `BaseOperatorName` does potentially\ninclude inplace or double-underscores.\n\nI also add the property to `NativeFunctionsGroup` so that grouped\nfunctions with type `Union[NativeFunction, NativeFunctionsGroup]`\ncan have the property queried without needing `isinstance` checks.\n\nTest Plan: Imported from OSS\n\nReviewed By: jbschlosser\n\nDifferential Revision: D32596271\n\nPulled By: albanD\n\nfbshipit-source-id: 8b6dad806ec8d796dcd70fc664604670d668cae7", "pr_number": "68687", "files_changed": ["tools/codegen/gen.py", "tools/codegen/model.py"], "labels": ["open source", "cla signed", "ciflow/default"]}, "303d60b8da": {"title": "Add TORCH_ASSERT_ONLY_METHOD_OPERATORS macro (#68688)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/68688\n\nThis adds a new macro `TORCH_ASSERT_ONLY_METHOD_OPERATORS` which\nallows `Tensor.h` to be included, but not headers which pull in all\nother operators. So, a file that defines this macro needs to use the\nfine-grained headers to include only the operators being used.\n\nTest Plan: Imported from OSS\n\nReviewed By: jbschlosser\n\nDifferential Revision: D32596267\n\nPulled By: albanD\n\nfbshipit-source-id: 6fc2ce3d2b0f52ac6d81b3f063193ce26e0d75a3", "pr_number": "68688", "files_changed": ["aten/src/ATen/templates/DispatchKeyFunctions_inl.h", "aten/src/ATen/templates/Functions.h", "aten/src/ATen/templates/NativeFunctions.h", "aten/src/ATen/templates/Operators.h", "aten/src/ATen/templates/RedispatchFunctions.h"], "labels": ["open source", "cla signed", "ciflow/default"]}, "6ba18ba87e": {"title": "Codegen: Generate static dispatch headers per operator (#68714)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/68714\n\nThis splits the static dispatch headers (e.g. `CPUFunctions.h`)\ninto per operators headers (e.g. `ops/empty_cpu_dispatch.h`) which is\nneeded for when `Tensor.h` is compiled with static dispatch enabled.\n\nThere are also several places in ATen where the static dispatch\nheaders are used as an optimization even in dynamic dispatch builds.\n\nTest Plan: Imported from OSS\n\nReviewed By: jbschlosser\n\nDifferential Revision: D32596265\n\nPulled By: albanD\n\nfbshipit-source-id: 287783ef4e35c7601e9d2714ddbc8d4a5b1fb9e5", "pr_number": "68714", "files_changed": ["aten/src/ATen/templates/DispatchKeyFunction.h", "aten/src/ATen/templates/DispatchKeyFunctions_inl.h", "aten/src/ATen/templates/Functions.h", "tools/codegen/gen.py"], "labels": ["open source", "cla signed", "ciflow/default"]}, "9c7c1b769a": {"title": "Functionalization: Only include headers for required ops (#68690)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/68690\n\nRegisterFunctionalization.cpp is a shared file, so only including the\nrequired operators means a single operator change only requires 1\nshard to be rebuilt instead of all of them.\n\nTest Plan: Imported from OSS\n\nReviewed By: jbschlosser\n\nDifferential Revision: D32596275\n\nPulled By: albanD\n\nfbshipit-source-id: 8b56f48872156b96fbc0a16b542b8bab76b73fd4", "pr_number": "68690", "files_changed": ["aten/src/ATen/templates/RegisterFunctionalization.cpp", "tools/codegen/gen.py", "tools/codegen/gen_functionalization_type.py"], "labels": ["open source", "cla signed", "ciflow/default"]}, "38cfacd817": {"title": "Tensor: Define operators override functions in TensorBody.h (#68697)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/68697\n\nCurrently, if you include `Tensor.h` but not `TensorOperators.h` then\nusing overloaded operators will compile but fail at link time.\nInstead, this defines the member functions in `TensorBody.h` and\nleaves `TensorOperators.h` as only the free functions.\n\nTest Plan: Imported from OSS\n\nReviewed By: jbschlosser\n\nDifferential Revision: D32596269\n\nPulled By: albanD\n\nfbshipit-source-id: 5ce39334dc3d505865268f5049b1e25bb90af44a", "pr_number": "68697", "files_changed": ["aten/src/ATen/TensorOperators.h", "aten/src/ATen/templates/TensorBody.h"], "labels": ["open source", "cla signed", "ciflow/default"]}, "b28a4100ff": {"title": "scripts: Fix manylinux2014 promotion to pypi (#70003)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/70003\n\nSigned-off-by: Eli Uriegas <eliuriegas@fb.com>\n\nTest Plan: Imported from OSS\n\nReviewed By: jbschlosser, janeyx99\n\nDifferential Revision: D33143730\n\nPulled By: seemethere\n\nfbshipit-source-id: 83a46047fbfe4709e841fbfcaa75e434ff325be5", "pr_number": "70003", "files_changed": ["scripts/release/promote/wheel_to_pypi.sh"], "labels": ["cla signed", "ciflow/default"]}, "65ab63310b": {"title": "[PyTorch] use div instead of mul when calculating sampling probability (#70001)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/70001\n\nmultiplying inversion of `kLowProb` instead of division which uses less expensive `mul` instead of `idv`\n\nTest Plan:\nBefore\n{F682076291}\n\nAfter\n{F682076323}\n\nReviewed By: robieta\n\nDifferential Revision: D32608440\n\nfbshipit-source-id: 7851317a0f7e33813f2bd7a152e5e7f4b5c361b4", "pr_number": "70001", "files_changed": ["aten/src/ATen/record_function.cpp"], "labels": ["fb-exported", "cla signed", "ciflow/default"]}, "32ffad17a9": {"title": "[PyTorch][Easy] make GlobalRecordFunctionCallbacks smallvector (#70002)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/70002\n\ncallbacks are limited to 4. no reason for it to be `std::vector`\n\nTest Plan: CI\n\nReviewed By: aaronenyeshi\n\nDifferential Revision: D32611294\n\nfbshipit-source-id: 21823248abe40d461579b9b68d53c8c0de2a133d", "pr_number": "70002", "files_changed": ["aten/src/ATen/record_function.cpp"], "labels": ["fb-exported", "cla signed", "ciflow/default"]}, "98c0fb8b42": {"title": "[sparsity] More descriptive error message for missing parameters (#69895)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/69895\n\nsparse.Linear has an error message that doesn't tell the user how to resolve the issue. This adds more info.\nghstack-source-id: 145603212\n\nTest Plan: Not needed -- string change only\n\nReviewed By: jerryzh168\n\nDifferential Revision: D33039278\n\nfbshipit-source-id: b5f7f5d257142eb3e7ad73f7c005755253a329d7", "pr_number": "69895", "files_changed": ["torch/ao/nn/sparse/quantized/linear.py"], "labels": ["cla signed", "ciflow/default"]}, "73a6c36f1b": {"title": "Add more details to the known limitations section of torchhub docs (#69970)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/69970\n\nThis is a follow up to https://github.com/pytorch/hub/issues/243\n\nTest Plan: Imported from OSS\n\nReviewed By: jbschlosser\n\nDifferential Revision: D33124060\n\nPulled By: NicolasHug\n\nfbshipit-source-id: 298fe14b39a1aff3e0b029044c9a0db8bc82336a", "pr_number": "69970", "files_changed": ["docs/source/hub.rst"], "labels": ["cla signed", "ciflow/default"]}, "e6a4988b2d": {"title": "[LTC] Upstream utils in computation_client (#69621)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/69621\n\nUpstream the following utils\n- metrics.h\n- multi_wait.h\n- thread_pool.h\n- unique.h\n\nTest Plan: Imported from OSS\n\nReviewed By: wconstab, VitalyFedyunin\n\nDifferential Revision: D32957629\n\nPulled By: desertfire\n\nfbshipit-source-id: 5f2fb57493856556099b7cda7560a568d1f9ed97", "pr_number": "69621", "files_changed": ["tools/build_variables.bzl", "tools/codegen/dest/lazy_ir.py", "tools/codegen/gen_lazy_tensor.py", "torch/csrc/lazy/core/config.cpp", "torch/csrc/lazy/core/config.h", "torch/csrc/lazy/core/metrics.cpp", "torch/csrc/lazy/core/metrics.h", "torch/csrc/lazy/core/multi_wait.cpp", "torch/csrc/lazy/core/multi_wait.h", "torch/csrc/lazy/core/thread_pool.cpp", "torch/csrc/lazy/core/thread_pool.h", "torch/csrc/lazy/core/unique.h"], "labels": ["cla signed", "ciflow/default"]}, "28243769f9": {"title": "[LTC] Upstream several internal ops (#69716)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/69716\n\nTo prepare for the landing of LazyTensor and LazyGraphExecutor,\n- arithmetic_ir_ops.h\n- cast.h\n- device_data.h\n- expand.h\n- generic.h\n- scalar.h\n\nTest Plan: Imported from OSS\n\nReviewed By: wconstab\n\nDifferential Revision: D32999410\n\nPulled By: desertfire\n\nfbshipit-source-id: 31559dd7a1e525591ae9e2d7f915ee864437c11f", "pr_number": "69716", "files_changed": ["tools/build_variables.bzl", "torch/csrc/lazy/core/metrics.cpp", "torch/csrc/lazy/ts_backend/ops/arithmetic_ir_ops.cpp", "torch/csrc/lazy/ts_backend/ops/arithmetic_ir_ops.h", "torch/csrc/lazy/ts_backend/ops/cast.cpp", "torch/csrc/lazy/ts_backend/ops/cast.h", "torch/csrc/lazy/ts_backend/ops/device_data.cpp", "torch/csrc/lazy/ts_backend/ops/device_data.h", "torch/csrc/lazy/ts_backend/ops/expand.cpp", "torch/csrc/lazy/ts_backend/ops/expand.h", "torch/csrc/lazy/ts_backend/ops/generic.cpp", "torch/csrc/lazy/ts_backend/ops/generic.h", "torch/csrc/lazy/ts_backend/ops/scalar.cpp", "torch/csrc/lazy/ts_backend/ops/scalar.h"], "labels": ["cla signed", "ciflow/default"]}, "fe7b6446d5": {"title": "[LTC] Upstream LazyTensor and LazyGraphExecutor (#69815)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/69815\n\nTest Plan: Imported from OSS\n\nReviewed By: dagitses, jbschlosser\n\nDifferential Revision: D33059774\n\nPulled By: desertfire\n\nfbshipit-source-id: dd1e3e5f4fd3181517eebd2742f6a5b7b6fb9a7d", "pr_number": "69815", "files_changed": ["tools/build_variables.bzl", "torch/csrc/lazy/core/config.cpp", "torch/csrc/lazy/core/config.h", "torch/csrc/lazy/core/lazy_graph_executor.cpp", "torch/csrc/lazy/core/lazy_graph_executor.h", "torch/csrc/lazy/core/tensor.cpp", "torch/csrc/lazy/core/tensor.h"], "labels": ["cla signed", "ciflow/default"]}, "a6a1c709ff": {"title": "Fixed libtorch at::Tensor::print() linking error (#69615)", "body": "Summary:\nThere was a declaration of function at::Tensor::print() in TensorBody.h,  left there during the refactoring of Tensor and TensorBase (d701357d921ef167d42c125e65b6f7da6be3ad0f). Removing it from TensorBody.h resolve the issue.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/69615\n\nTest Plan:\ncode below now compile and works fine (print `[CPUFloatType [3, 4, 5, 5, 5]] `)\n```\n#include <torch/torch.h>\n\nint main()\n{\n    torch::Tensor tensor = torch::randn({3, 4, 5, 5, 5});\n    tensor.print();\n}\n```\n\nFixes https://github.com/pytorch/pytorch/issues/69515\n\nReviewed By: ngimel\n\nDifferential Revision: D33020361\n\nPulled By: albanD\n\nfbshipit-source-id: 190f253fb4101a4205aede3574b6e8acd19e54a1", "pr_number": "69615", "files_changed": ["aten/src/ATen/templates/TensorBody.h"], "labels": ["triaged", "open source", "better-engineering", "cla signed", "ci/master", "ciflow/default"]}, "c80b5b8c8f": {"title": "Revert D33102715: Back out \"Revert D32606547: torch/monitor: add C++ events and handlers\"", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD33102715 (https://github.com/pytorch/pytorch/commit/eb374de3f538c19b3bc88d257101e39d3eb05e7d)\n\nOriginal commit changeset: 3816ff01c578\n\nOriginal Phabricator Diff: D33102715 (https://github.com/pytorch/pytorch/commit/eb374de3f538c19b3bc88d257101e39d3eb05e7d)\n\nfbshipit-source-id: e262b6d8c80a05f3a67e024fedfbadefdbfe6e29", "pr_number": null, "files_changed": ["test/cpp/monitor/test_counters.cpp", "test/cpp/monitor/test_events.cpp", "tools/build_variables.bzl", "torch/csrc/monitor/counters.cpp", "torch/csrc/monitor/counters.h", "torch/csrc/monitor/events.cpp", "torch/csrc/monitor/events.h"], "labels": []}, "ebc66bfeea": {"title": "[Profiler] Pull helper methods into dedicated file. (And start `torch/csrc/profiler` folder. (#69255)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/69255\n\nOne thing that I've found as I optimize profier is that there's a lot of intermingled code, where the kineto profiler relies on the legacy (autograd) profiler for generic operations. This made optimization hard because I had to manage too many complex dependencies. (Exaserbated by the USE_KINETO #ifdef's sprinkled around.) This PR is the first of several to restructure the profiler(s) so the later optimizations go in easier.\n\nTest Plan: Unit tests\n\nReviewed By: aaronenyeshi\n\nDifferential Revision: D32671972\n\nfbshipit-source-id: efa83b40dde4216f368f2a5fa707360031a85707", "pr_number": "69255", "files_changed": ["setup.py", "tools/build_variables.bzl", "torch/csrc/autograd/init.cpp", "torch/csrc/autograd/profiler_kineto.cpp", "torch/csrc/autograd/profiler_kineto.h", "torch/csrc/autograd/profiler_legacy.cpp", "torch/csrc/autograd/profiler_legacy.h", "torch/csrc/autograd/profiler_utils.h", "torch/csrc/profiler/util.cpp", "torch/csrc/profiler/util.h"], "labels": ["fb-exported", "cla signed", "ciflow/default"]}, "7f7966a888": {"title": "[Docs] Fix the syntax of documentation (#69958)", "body": "Summary:\nFixes the syntax of documentation in the file torch/nn/utils/clip_grad.py\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/69958\n\nReviewed By: mruberry\n\nDifferential Revision: D33160612\n\nPulled By: albanD\n\nfbshipit-source-id: 2dc199fee345bb4c75632900bc6f73a1ab8192a6", "pr_number": "69958", "files_changed": ["torch/nn/utils/clip_grad.py"], "labels": ["open source", "cla signed", "ciflow/default"]}, "c9e898fef8": {"title": "delete TH (#69929)", "body": "Summary:\nMove TH<C>GenerateByteType includes into torch/csrc (the only place they are used), and we can remove TH folder altogether!\nThe only thing left in THC are includes left for bc compatibility.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/69929\n\nReviewed By: mruberry\n\nDifferential Revision: D33133013\n\nPulled By: ngimel\n\nfbshipit-source-id: 78c87cf93d2d641631b0f71051ace318bf4ec3c1", "pr_number": "69929", "files_changed": ["aten/CMakeLists.txt", "aten/src/ATen/native/cpu/README.md", "aten/src/TH/CMakeLists.txt", "aten/src/TH/README.md", "aten/src/TH/THGenerateByteType.h", "aten/src/THC/CMakeLists.txt", "aten/src/THC/THCGenerateByteType.h", "torch/csrc/Storage.cpp", "torch/csrc/Storage.h", "torch/csrc/THCGenerateByteType.h", "torch/csrc/THGenerateByteType.h", "torch/csrc/cuda/Storage.cpp", "torch/csrc/cuda/Storage.h", "torch/csrc/cuda/serialization.cpp", "torch/csrc/cuda/serialization.h", "torch/csrc/cuda/utils.h", "torch/csrc/serialization.cpp", "torch/csrc/serialization.h", "torch/csrc/utils.h"], "labels": ["cla signed", "ciflow/default"]}, "9ff8c49ed9": {"title": "Enable cpu scalar arguments for jiterator (#69861)", "body": "Summary:\nCreates analog of `gpu_kernel_with_scalars` for jiterator kernels\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/69861\n\nReviewed By: mruberry\n\nDifferential Revision: D33134013\n\nPulled By: ngimel\n\nfbshipit-source-id: fd2412e8d6432e15d5721e95a194d29fa70ad92c", "pr_number": "69861", "files_changed": ["aten/src/ATen/native/Math.h", "aten/src/ATen/native/cuda/CUDALoops.cuh", "aten/src/ATen/native/cuda/Loops.cuh", "aten/src/ATen/native/cuda/ZetaKernel.cu", "aten/src/ATen/native/cuda/jit_utils.cu", "aten/src/ATen/native/cuda/jit_utils.h", "test/test_binary_ufuncs.py"], "labels": ["cla signed", "ciflow/default"]}, "96fe82ac3c": {"title": "HANDLE_TH_ERRORS: Move exception translation out of line (#69974)", "body": "Summary:\nI've noticed that the `HANDLE_TH_ERRORS` macros are actually very expensive in terms of compile time.  Moving the bulk of the catch statements out of line using a lippincott function significantly improves compile times and object file binary sizes. For just the generated autograd bindings, this halves serial build time from 8 minutes to 4 and binary size is more than halved for most files with the biggest difference being `python_variable_methods.cpp` which went from 126 MB to 43 MB.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/69974\n\nReviewed By: mruberry\n\nDifferential Revision: D33160899\n\nPulled By: albanD\n\nfbshipit-source-id: fc35fa86f69ffe5a0752557be30b438c8564e998", "pr_number": "69974", "files_changed": ["torch/csrc/Exceptions.cpp", "torch/csrc/Exceptions.h"], "labels": ["open source", "cla signed", "ciflow/default"]}, "b4c4a015d6": {"title": "Revert D33163841: Revert D33102715: Back out \"Revert D32606547: torch/monitor: add C++ events and handlers\"", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD33163841\n\nOriginal commit changeset: e262b6d8c80a\n\nOriginal Phabricator Diff: D33102715 (https://github.com/pytorch/pytorch/commit/eb374de3f538c19b3bc88d257101e39d3eb05e7d)\n\nfbshipit-source-id: 644216036a238a458f0a2198460b36d24fb035f8", "pr_number": null, "files_changed": ["test/cpp/monitor/test_counters.cpp", "test/cpp/monitor/test_events.cpp", "tools/build_variables.bzl", "torch/csrc/monitor/counters.cpp", "torch/csrc/monitor/counters.h", "torch/csrc/monitor/events.cpp", "torch/csrc/monitor/events.h"], "labels": []}, "5f3f327a9d": {"title": "update `SequentialLR` signature (#69817)", "body": "Summary:\n- ~optimizer isn't required for `SequentialLR` since it's already present in the schedulers. Trying to match the signature of it with `ChainedScheduler`.~\n- ~`verbose` isn't really used anywhere so removed it.~\n\nupdated missing docs and added a small check\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/69817\n\nReviewed By: ngimel\n\nDifferential Revision: D33069589\n\nPulled By: albanD\n\nfbshipit-source-id: f015105a35a2ca39fe94c70acdfd55cdf5601419", "pr_number": "69817", "files_changed": ["torch/optim/lr_scheduler.py", "torch/optim/lr_scheduler.pyi"], "labels": ["open source", "cla signed", "ciflow/default"]}, "39f65fee47": {"title": "[jit] Split ClassType into a separate header. (#68036)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/68036\n\nIn Edge cases we want to separately include class_type.h because in the future we want to stop depending on the rest of the JIT types declared inside jit_type.h\nghstack-source-id: 145818699\n\nTest Plan: no behavior change.\n\nReviewed By: qihqi, gmagogsfm\n\nDifferential Revision: D32264618\n\nfbshipit-source-id: 53dc187772e3dde88ff978b87252c31f3641860b", "pr_number": "68036", "files_changed": ["aten/src/ATen/core/class_type.cpp", "aten/src/ATen/core/class_type.h", "aten/src/ATen/core/jit_type.h", "aten/src/ATen/core/jit_type_base.h", "aten/src/ATen/core/type.cpp", "tools/build_variables.bzl"], "labels": ["cla signed", "ciflow/default"]}, "fa582045fc": {"title": "Fix lint/mypy violations (#70059)", "body": "Summary:\nIntroduced by https://github.com/pytorch/pytorch/pull/69194\n\nFixes #{issue number}\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/70059\n\nReviewed By: suo, cccclai\n\nDifferential Revision: D33170748\n\nPulled By: malfet\n\nfbshipit-source-id: a2e42f37d04c21a735f6474e42eb6670d2a0c3b9", "pr_number": "70059", "files_changed": ["tools/codegen/operator_versions/gen_mobile_upgraders.py"], "labels": ["cla signed", "ciflow/default"]}, "de296d526f": {"title": "move torch.testing from prototype to beta (#69668)", "body": "Summary:\ncc brianjo mruberry\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/69668\n\nReviewed By: albanD\n\nDifferential Revision: D33028213\n\nPulled By: mruberry\n\nfbshipit-source-id: 3316b887d4c322cc1262feee651464da4124a6de", "pr_number": "69668", "files_changed": ["docs/source/testing.rst"], "labels": ["module: docs", "open source", "cla signed", "module: testing", "ciflow/default"]}, "950957f857": {"title": "Fix jit tests assuming sample_inputs is a list (#69975)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/69975\n\ncc mruberry\n\nTest Plan: Imported from OSS\n\nReviewed By: ejguan\n\nDifferential Revision: D33172952\n\nPulled By: mruberry\n\nfbshipit-source-id: 1f8bb49179f7fbd0fec5e7344e8c213484518e27", "pr_number": "69975", "files_changed": ["test/jit/test_dtype_analysis.py"], "labels": ["oncall: jit", "module: tests", "open source", "cla signed", "ciflow/default"]}, "de992c6b21": {"title": "Specify ij indexing when cartesian_prod calls meshgrid (#68753)", "body": "Summary:\nCurrently, `cartesian_prod` calls `meshgrid` without passing an indexing parameter. This causes a warning to be shown when running the `cartesian_prod` example from the docs. This PR simply passes the default value for this indexing parameter instead.\n\nFixes https://github.com/pytorch/pytorch/issues/68741\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/68753\n\nReviewed By: kimishpatel\n\nDifferential Revision: D33173011\n\nPulled By: mruberry\n\nfbshipit-source-id: 667185ec85bd62bda177bc5768d36f56cfc8b9ab", "pr_number": "68753", "files_changed": ["aten/src/ATen/native/Itertools.cpp"], "labels": ["triaged", "open source", "cla signed", "ciflow/default"]}, "0cfff65395": {"title": "Apply contiguous on inputs of cdist backward (#70016)", "body": "Summary:\nDescription:\n- Apply contiguous on inputs of cdist backward\n- Added a test\n\nFixes https://github.com/pytorch/pytorch/issues/69997\n\ncc ezyang albanD zou3519 gqchen pearu nikitaved soulitzer Lezcano Varal7\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/70016\n\nReviewed By: ejguan\n\nDifferential Revision: D33187946\n\nPulled By: albanD\n\nfbshipit-source-id: 645306aa043b2f84c4c2df0306fabfc224d746b6", "pr_number": "70016", "files_changed": ["aten/src/ATen/native/Distance.cpp", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["module: autograd", "open source", "cla signed", "ciflow/default"]}, "60eb1e53b2": {"title": "Sparse CSR CPU: Add block sparse support for MKL path (#68710)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/68710\n\nThis PR adds support for block sparse (BSR) matrices for functions that\nuse Inspector-Executor MKL Sparse API. At the moment of this PR it's:\n* torch.addmm\n* torch.addmv\n* torch.triangular_solve (once https://github.com/pytorch/pytorch/pull/62180 is merged)\n\ncc nikitaved pearu cpuhrsch IvanYashchuk\n\nTest Plan: Imported from OSS\n\nReviewed By: ZolotukhinM\n\nDifferential Revision: D33179486\n\nPulled By: cpuhrsch\n\nfbshipit-source-id: e1dec0dccdbfed8b280be16b8c11fc9e770d50ae", "pr_number": "68710", "files_changed": ["aten/src/ATen/mkl/SparseBlas.cpp", "aten/src/ATen/mkl/SparseBlas.h", "aten/src/ATen/mkl/SparseDescriptors.h", "aten/src/ATen/native/mkl/SparseBlasImpl.cpp", "test/test_sparse_csr.py"], "labels": ["module: sparse", "module: mkl", "open source", "cla signed", "ciflow/default", "ciflow/win", "ciflow/cpu"]}, "a6b7521428": {"title": "always use max cmake when cmake3 and cmake are all existed (#69355)", "body": "Summary:\nFor Pytorch source build when using Ninja generator, it requires **CMake >=3.13**,  Pytorch always checks **cmake3 >= 3.10** first, so when **3.13> cmake3 >= 3.10** and then PyTorch will use cmake3, there will report an error: ```Using the Ninja generator requires CMake version 3.13 or greater```  even the **CMake >=3.13** .\n\nFor example: for my centos machine, the system CMake3 is ```3.12```,  and my conda env's CMake is ```3.19.6```,  there will have a build error which PyTorch choose CMake 3, I can update CMake3 or create an alias or a symlink to solve this problem, but the more reasonable way is that ```_get_cmake_command ``` always return the newest CMake executable (unless explicitly overridden with a same CMAKE_PATH environment variable).\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/69355\n\nReviewed By: jbschlosser\n\nDifferential Revision: D33062274\n\nPulled By: malfet\n\nfbshipit-source-id: c6c77ce1374e6090a498be227032af1e1a82d418", "pr_number": "69355", "files_changed": ["tools/setup_helpers/cmake.py"], "labels": ["triaged", "open source", "cla signed", "ciflow/default"]}, "38e026c14d": {"title": "Add tanh_backward to AT symbols (#70071)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/70071\n\nThis commit adds tanh_backward to aten_interned_strings.h as an AT symbol.\n\nTest Plan: CI.\n\nReviewed By: mruberry\n\nDifferential Revision: D33173370\n\nPulled By: alanwaketan\n\nfbshipit-source-id: e20ed2a807156ce772b7c1e3f434fa895116f4c3", "pr_number": "70071", "files_changed": ["aten/src/ATen/core/aten_interned_strings.h"], "labels": ["cla signed", "ciflow/default"]}, "70ed4f3ffc": {"title": "Try dropping Torch from typeshed_internal (#69926)", "body": "Summary:\nRemoves the internal typeshed for PyTorch and replaces it with PyTorch's own type annotations.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/69926\n\nGenerated files are in P471601595, P471601643, P471601662\n\nBased on an example in D26410012\n\nTest Plan: Sandcastle\n\nReviewed By: malfet, pradeep90\n\nDifferential Revision: D32292834\n\nfbshipit-source-id: 5223f514cbdccd02c08ef0a027a48d92cdebed2c", "pr_number": "69926", "files_changed": ["tools/codegen/utils.py", "tools/pyi/gen_pyi.py"], "labels": ["fb-exported", "cla signed", "ciflow/default"]}, "9fb199bc12": {"title": "Add convolution_backward to aten_interned_strings.h (#70112)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/70112\n\nTest Plan: Imported from OSS\n\nReviewed By: albanD\n\nDifferential Revision: D33188664\n\nPulled By: jbschlosser\n\nfbshipit-source-id: 20e565c2fef4c1c3c087ba9b36320b7e539e467e", "pr_number": "70112", "files_changed": ["aten/src/ATen/core/aten_interned_strings.h"], "labels": ["cla signed", "ciflow/default"]}, "9d3a6fa623": {"title": "[quant][bc-breaking] Remove QConfigDynamic from quantization api (#69875)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/69875\n\natt\n\nTest Plan:\nci + regression tets:\n```\npython test/test_quantization.py TestPostTrainingStatic\npython test/test_quantization.py TestPostTrainingDynamic\npython test/test_quantization.py TestQuantizeFx\n```\n\nImported from OSS\n\nReviewed By: vkuzo\n\nDifferential Revision: D33079096\n\nfbshipit-source-id: 1e73bb27c518eba62b60f3a8c4b532dddc8367cf", "pr_number": "69875", "files_changed": ["docs/source/quantization-support.rst"], "labels": ["cla signed", "ciflow/default"]}, "84b7832010": {"title": "Updates CUDA memory leak check to verify against driver API and print more diagnostic information (#69556)", "body": "Summary:\nPer title\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/69556\n\nReviewed By: mrshenli\n\nDifferential Revision: D32954770\n\nPulled By: mruberry\n\nfbshipit-source-id: a6c2ae6f704422c178569980ca4b9c72c4272f55", "pr_number": "69556", "files_changed": ["mypy.ini", "test/test_autograd.py", "test/test_cuda.py", "torch/testing/_internal/common_utils.py"], "labels": ["cla signed", "ciflow/default"]}, "06d0536dad": {"title": "Low precision support for jiterator (#70157)", "body": "Summary:\nThis adds support for bfloat16 and fp16 types for jiterator by adding at::Half and at::BFloat16 classes to the jiterator code template. The only methods defined in those classes are construction from float and implicit conversion to float. Mathematical operations on them never need to be defined, because jiterator is written in a way to implicitly upcast the inputs to the functor, so all math has to be performed on float only (e.g. compute part of the kernel would always be written as\n```\n        out[j] = i0<float>(arg0[j]);\n```\nIt also adds support for casting to complex outputs, by adding a similar templated class c10::complex<T>. Originally I planned to only support float -> complex complex for it, but to compile fetch_and_cast function we also need complex -> float conversion. We can avoid it by compiling fetch_and_cast for a different subset of types, but I'm not doing it in this PR. Thus, technically, we can compile a kernel that would accept complex inputs and produce wrong results, but we are guarding against it by static asserting that none of the functor datatype are complex, and runtime-checking that none of the inputs are complex.\nAdding bfloat16, half and complex support allows us to remove special handling for type promotion tests for gcd.\ni0 (that supports half and bfloat16 inputs) is moved to use jiterator.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/70157\n\nReviewed By: mruberry\n\nDifferential Revision: D33221645\n\nPulled By: ngimel\n\nfbshipit-source-id: 9cfe8aba3498a0604c4ea62c217292ea06c826b1", "pr_number": "70157", "files_changed": ["aten/src/ATen/native/cuda/Loops.cuh", "aten/src/ATen/native/cuda/UnarySpecialOpsKernel.cu", "aten/src/ATen/native/cuda/jit_utils.cu", "test/test_binary_ufuncs.py", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["cla signed", "ciflow/default"]}, "7ea86dfdb1": {"title": "[Profiler] Factor common logic into `torch/csrc/profiler/api.h` (#69459)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/69459\n\nThis change breaks the dependency between the kineto and legacy profiler; instead of `profiler_kineto.h` including `profiler_legacy.h`, they both include `profiler/api.h`. As part of this refactor, I injected some intermediate classes to keep legacy behavior from leaking into the kineto profiler:\n\n1) ProfilerThreadLocalState has become ProfilerThreadLocalStateBase which just handles config and callback handle. Legacy and Kineto profilers inherit this and implement their own very disjoint set of logic.\n\n2) CUDAStubs is a pure virtual class to make the interface more readable, and the \"always fail\" behavior has been moved to a `DefaultCUDAStubs` class in `api.cpp`.\n\nTest Plan: Ran the overhead ubenchmark.\n\nReviewed By: aaronenyeshi\n\nDifferential Revision: D32678163\n\nfbshipit-source-id: 9b733283e4eae2614db68147de81b72f6094ce6c", "pr_number": "69459", "files_changed": ["tools/build_variables.bzl", "torch/csrc/autograd/profiler_cuda.cpp", "torch/csrc/autograd/profiler_kineto.cpp", "torch/csrc/autograd/profiler_kineto.h", "torch/csrc/autograd/profiler_legacy.cpp", "torch/csrc/autograd/profiler_legacy.h", "torch/csrc/profiler/api.cpp", "torch/csrc/profiler/api.h", "torch/csrc/profiler/cuda.cpp"], "labels": ["oncall: distributed", "oncall: jit", "fb-exported", "cla signed", "ciflow/default"]}, "978089c381": {"title": "Prevent divide-by-zero errors in Timer (#70050)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/66503\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/70050\n\nReviewed By: mruberry\n\nDifferential Revision: D33168868\n\nPulled By: robieta\n\nfbshipit-source-id: 7d0ece9e888f6c69a9e0ced581c92d3259fb3540", "pr_number": "70050", "files_changed": ["torch/utils/benchmark/utils/timer.py"], "labels": ["cla signed", "ciflow/default"]}, "bcb6076099": {"title": "Sparse CSR tensors: storage access should throw (#70072)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/70072\n\nLike (sparse COO tensors), sparse CSR tensors don't really have an actual storage() that can be accessed, so sparsetensor->storage() should throw.\n\ncc nikitaved pearu cpuhrsch\n\nTest Plan: Imported from OSS\n\nReviewed By: mruberry\n\nDifferential Revision: D33181309\n\nPulled By: davidberard98\n\nfbshipit-source-id: 8f1dc4da03073d807e5acee2ac47caeffb94b16c", "pr_number": "70072", "files_changed": ["aten/src/ATen/SparseCsrTensorImpl.cpp"], "labels": ["module: sparse", "cla signed", "ciflow/default"]}, "423ce416d8": {"title": "Prune osx-arm64 binaries from nightly channel (#70132)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/70043\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/70132\n\nReviewed By: janeyx99\n\nDifferential Revision: D33195431\n\nPulled By: malfet\n\nfbshipit-source-id: 4579a6788255a6df306862c3e959ae7a9ddd4e45", "pr_number": "70132", "files_changed": ["scripts/release/anaconda-prune/prune.sh"], "labels": ["cla signed", "ciflow/default"]}, "fcaecd718a": {"title": "Write flaky tests to rockset (#70136)", "body": "Summary:\nTry using Rockset as backend for data instead of RDS\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/70136\n\nReviewed By: suo\n\nDifferential Revision: D33242148\n\nPulled By: janeyx99\n\nfbshipit-source-id: 8935ceb43717fff4922b634165030cca7e934968", "pr_number": "70136", "files_changed": ["tools/stats/print_test_stats.py"], "labels": ["cla signed", "ciflow/default"]}, "e6d9bb8d57": {"title": "reduce the number of instantiations for bernoulli tensor tensor kernel (#70169)", "body": "Summary:\nReduces the binary size of DistributionBernoulli.cu 12282600 -> 3946792\nTensor-tensor bernoulli kernels are rarely used, we limit dispatches to double probability type for double `self` tensor, and `float` probability type for everything else. This would be a minor perf hit if probability tensor is of the different dtype, but given how rarely these kernels are used (and how rarely the probability tensor is not float) this is not a problem.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/70169\n\nReviewed By: jbschlosser\n\nDifferential Revision: D33237890\n\nPulled By: ngimel\n\nfbshipit-source-id: 185c4b97aba0fb6ae159d572dd5bbb13cf676bb4", "pr_number": "70169", "files_changed": ["aten/src/ATen/native/cuda/DistributionTemplates.h"], "labels": ["cla signed", "ciflow/default"]}, "f17e76b0f2": {"title": "Expand description of bias_sizes arg for convolution_backward (#70195)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/70195\n\nTest Plan: Imported from OSS\n\nReviewed By: zou3519\n\nDifferential Revision: D33240155\n\nPulled By: jbschlosser\n\nfbshipit-source-id: c4f907d6e33e4d1eeb1b5228f1152307c8b27729", "pr_number": "70195", "files_changed": ["aten/src/ATen/native/Convolution.cpp"], "labels": ["cla signed", "ciflow/default"]}, "6623c4838e": {"title": "Handle the corner case when min == max in L2 search (#70207)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/70207\n\nIn corner case when min == max, adjust_hist_to_include_zero() function used in L2 search will cause additional_nbins = -2147483648 and initialize bins_f with negative size.\n\nTest Plan:\nBefore fix:\nf315187213\n\nAfter fix:\nf315471862\n\nReviewed By: jspark1105\n\nDifferential Revision: D33227717\n\nfbshipit-source-id: 7e8a455e51a0703a3a9c5eb7595d9b4d43966001", "pr_number": "70207", "files_changed": ["caffe2/quantization/server/dnnlowp.cc"], "labels": ["fb-exported", "cla signed", "ciflow/default"]}, "8e763cd735": {"title": "Add explicit OperatorHandle destructor (#70033)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/70032\n\nWindows build of PyTorch doesn't produce the `c10::OperatorHandle::~OperatorHandle(void)` symbol in any of its `*.lib` files. This fix is to explicitly define it in Dispatcher.cpp, so downstream consumers wanting to dllimport can find it.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/70033\n\nReviewed By: jbschlosser\n\nDifferential Revision: D33240599\n\nPulled By: bdhirsh\n\nfbshipit-source-id: 56cc5963043bd5caac30e42c3501a4f48d086b36", "pr_number": "70033", "files_changed": ["aten/src/ATen/core/dispatch/Dispatcher.cpp", "aten/src/ATen/core/dispatch/Dispatcher.h"], "labels": ["triaged", "open source", "cla signed", "ciflow/default"]}, "5e222d08a1": {"title": "Revert \"Revert D32498572: allow external backend codegen to be used without autograd kernels\" (#69949)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/69949\n\nThis reverts commit 33363cea64fd4be16975c32cf57e9eb123af371d.\n\nTest Plan: Imported from OSS\n\nReviewed By: H-Huang\n\nDifferential Revision: D33113544\n\nPulled By: bdhirsh\n\nfbshipit-source-id: e219f10d52776498c9ad273e97bca3e3406cf702", "pr_number": "69949", "files_changed": ["tools/codegen/gen_backend_stubs.py", "tools/codegen/gen_lazy_tensor.py"], "labels": ["cla signed", "ciflow/default"]}, "304efd8e9a": {"title": "Change TH_BLAS_MKL into AT_MKL_ENABLED() (#70219)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/70219\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/69419\n\nTest Plan: Imported from OSS\n\nReviewed By: malfet\n\nDifferential Revision: D33246758\n\nPulled By: ngimel\n\nfbshipit-source-id: aedef4c9ef97b6aa9f574313c94f774b77df2748", "pr_number": "70219", "files_changed": ["aten/src/ATen/ParallelCommon.cpp", "aten/src/ATen/ParallelNative.cpp", "aten/src/ATen/ParallelNativeTBB.cpp", "aten/src/ATen/ParallelOpenMP.cpp", "aten/src/ATen/test/test_parallel.cpp", "third_party/cpuinfo.BUILD", "third_party/mkl-dnn.BUILD", "third_party/sleef.BUILD"], "labels": ["fb-exported", "cla signed", "ciflow/default"]}, "c4a6c7a436": {"title": "fix cpu binary size increase for clamp (#70168)", "body": "Summary:\nPer title\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/70168\n\nReviewed By: bdhirsh\n\nDifferential Revision: D33229811\n\nPulled By: ngimel\n\nfbshipit-source-id: 3509da766fa327f4103fdcf880d368f64c111496", "pr_number": "70168", "files_changed": ["aten/src/ATen/native/TensorCompare.cpp", "aten/src/ATen/native/TensorCompare.h", "aten/src/ATen/native/cpu/TensorCompareKernel.cpp", "aten/src/ATen/native/cuda/TensorCompare.cu"], "labels": ["cla signed", "ciflow/default"]}, "c321d4c1ca": {"title": "[Operator Versioning] Split the upgrader test to a separate file and cover mobile part (#70090)", "body": "Summary:\n1. Split the test `test_save_load.py` to two files. Basically move the operator versioning related changes to `test_save_load_for_op_versions.py`.\n2. Add mobile module related test to `test_save_load_for_op_versions.py`\n\nHow to run:\n```\nbuck test mode/opt //caffe2/test:jit\nor\npython test/test_jit.py TestSaveLoadForOpVersion\n```\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/70090\n\nghstack-source-id: 146103547\n\nTest Plan:\n```\nbuck test mode/opt //caffe2/test:jit\npython test/test_jit.py TestSaveLoadForOpVersion\n```\n\nReviewed By: tugsbayasgalan\n\nDifferential Revision: D33180767\n\nfbshipit-source-id: dd31e313c81e90b598ea9dd5ad04a853c017f994", "pr_number": "70090", "files_changed": ["test/jit/test_save_load.py", "test/jit/test_save_load_for_op_version.py", "test/test_jit.py"], "labels": ["oncall: jit", "cla signed", "ciflow/default"]}, "23902fb895": {"title": "Fixed typo in torch check for cdist (#70178)", "body": "Summary:\nDescription:\n- Fixed typo in torch check for cdist\n\ncc zou3519\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/70178\n\nReviewed By: bdhirsh\n\nDifferential Revision: D33236027\n\nPulled By: zou3519\n\nfbshipit-source-id: e87a982c0dc5fe576db8f2afc4b2010924f047c0", "pr_number": "70178", "files_changed": ["aten/src/ATen/native/Distance.cpp"], "labels": ["open source", "cla signed", "ciflow/default"]}, "21c6de9fdc": {"title": "Extend autograd functional benchmarking to run vectorized tasks (#67045)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/67045\n\nTo run: `python benchmarks/functional_autograd_benchmark/functional_autograd_benchmark.py --gpu -1 --model-filter=ppl    _robust_reg --num-iter 100`\n\n```\nResults for model ppl_robust_reg on task vjp: 0.0012262486852705479s (var: 2.2107682351446556e-10)\nResults for model ppl_robust_reg on task vhp: 0.002099371049553156s (var: 6.906406557760647e-10)\nResults for model ppl_robust_reg on task jvp: 0.001860950025729835s (var: 1.1251884146634694e-10)\nResults for model ppl_robust_reg on task hvp: 0.003481731517240405s (var: 2.2713633751614282e-10)\nResults for model ppl_robust_reg on task jacobian: 0.0012128615053370595s (var: 1.3687526667638394e-09)\nResults for model ppl_robust_reg on task hessian: 0.009885427542030811s (var: 9.366265096844018e-09)\nResults for model ppl_robust_reg on task hessian_fwdrev: 0.005268776323646307s (var: 2.4293791422991262e-09)\nResults for model ppl_robust_reg on task hessian_revrev: 0.002561321249231696s (var: 7.557877101938004e-10)\nResults for model ppl_robust_reg on task jacfwd: 0.002619938924908638s (var: 5.109343503839625e-10)\nResults for model ppl_robust_reg on task jacrev: 0.0013469004770740867s (var: 3.1857563254078514e-09)\n```\nNotes:\n - We go through batched fallback for both\n - ppl_robust_reg takes 3 tensor inputs and returns a single scalar output\n   - this means that jacobian is equivalent to doing vjp and vmap would not help us\n   - we expect jacfwd to be slower than jacrev\n\nTest Plan: Imported from OSS\n\nReviewed By: malfet\n\nDifferential Revision: D33265947\n\nPulled By: soulitzer\n\nfbshipit-source-id: 14f537a1376dea7e5afbe0c8e97f94731479b018", "pr_number": "67045", "files_changed": ["benchmarks/functional_autograd_benchmark/functional_autograd_benchmark.py"], "labels": ["cla signed", "ciflow/default"]}, "6217fee96b": {"title": "Revert D33246843: [pytorch][PR] Implementation of Wishart distribution", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD33246843 (https://github.com/pytorch/pytorch/commit/a217a62e73fd30b658743af8a69966f90327f018)\n\nOriginal commit changeset: 825fcddf4785\n\nOriginal Phabricator Diff: D33246843 (https://github.com/pytorch/pytorch/commit/a217a62e73fd30b658743af8a69966f90327f018)\n\nfbshipit-source-id: 2c8063e8d10e9d3ac20fa44673e6011ed1160753", "pr_number": null, "files_changed": ["docs/source/distributions.rst", "test/distributions/test_distributions.py", "torch/distributions/__init__.py", "torch/distributions/wishart.py"], "labels": []}, "e02d836cb2": {"title": "[LTC] Upstream LTCTensorImpl (#70062)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/70062\n\nThis commit upstreams LTCTensorImpl from the lazy_tensor_staging branch.\nIt inherits from c10::TensorImpl and thus manages the lifetime/storage\nof LazyTensor.\n\nTest Plan: ./build/bin/test_lazy --gtest_filter=LazyTensorImplTest.*\n\nReviewed By: desertfire\n\nDifferential Revision: D33171186\n\nPulled By: alanwaketan\n\nfbshipit-source-id: 6af9f91cc7c7e997f120cb89a7bcd6785c03ace0", "pr_number": "70062", "files_changed": ["test/cpp/lazy/CMakeLists.txt", "test/cpp/lazy/test_tensor_impl.cpp", "tools/build_variables.bzl", "torch/csrc/lazy/backend/backend_interface.cpp", "torch/csrc/lazy/backend/backend_interface.h", "torch/csrc/lazy/core/tensor_impl.cpp", "torch/csrc/lazy/core/tensor_impl.h"], "labels": ["cla signed", "ciflow/default"]}, "29f1ccc8f0": {"title": "Fix some Composite Compliance problems with binary_cross_entropy backward (#70198)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/70198\n\nThis PR fixes composite compliance problems with:\n- binary_cross_entropy's backward formula\n- binary_cross_entropy_with_logits's backward formula\n- binary_cross_entropy's double backward formula\n\nIt does so by adding checks for areAnyTensorSubclassLike.\n\nTest Plan:\n- I tested everything with functorch.\n- We are going to do https://github.com/pytorch/pytorch/issues/69530 in\nthe future so we have a way of testing this in core. I need the\nbinary_cross_entropy ones for something right now and didn't want to\nwait until we come up with a solution for #69530.\n\nReviewed By: Chillee\n\nDifferential Revision: D33246995\n\nPulled By: zou3519\n\nfbshipit-source-id: 310ed3196b937d01b189870b86a6c5f77f9258b4", "pr_number": "70198", "files_changed": ["torch/csrc/autograd/FunctionsManual.cpp"], "labels": ["cla signed", "ciflow/default"]}, "4db3a8fc0a": {"title": "[nn] TransformerEncoderLayer: no-batch-dim (#69291)", "body": "Summary:\nReference: https://github.com/pytorch/pytorch/issues/60585\nTODO:\n* [ ] Update docs?\n* [x] Generic reference function?\n\ncc albanD mruberry jbschlosser walterddr kshitij12345\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/69291\n\nReviewed By: davidberard98\n\nDifferential Revision: D33278970\n\nPulled By: jbschlosser\n\nfbshipit-source-id: 8dd5b6d7c0099fa38aa037c186778b10834bdee4", "pr_number": "69291", "files_changed": ["torch/testing/_internal/common_modules.py"], "labels": ["module: nn", "triaged", "open source", "cla signed", "ciflow/default"]}, "56969bf88a": {"title": "make inverse call linalg_inv (#70276)", "body": "Summary:\n`linalg.inv` and `inverse` are aliases according to documentation, yet their implementation is somewhat diverged. This makes `inverse` call into `linalg_inv`.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/70276\n\nReviewed By: malfet\n\nDifferential Revision: D33271847\n\nPulled By: ngimel\n\nfbshipit-source-id: cf018ddd2c1cee29026dd5f546f03f3a1d3cf362", "pr_number": "70276", "files_changed": ["aten/src/ATen/native/BatchLinearAlgebra.cpp", "aten/src/ATen/native/cuda/BatchLinearAlgebra.cpp", "aten/src/ATen/native/cuda/BatchLinearAlgebraLib.cpp", "aten/src/ATen/native/native_functions.yaml", "test/backward_compatibility/check_backward_compatibility.py"], "labels": ["cla signed", "ciflow/default"]}, "c34aa715fa": {"title": "AT_MKL_SEQUENTIAL and build changes (#70259)", "body": "Summary:\nRe-land of  https://github.com/pytorch/pytorch/pull/69419\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/70259\n\nTest Plan: Imported from OSS\n\nReviewed By: malfet\n\nDifferential Revision: D33246757\n\nPulled By: ngimel\n\nfbshipit-source-id: 738f8558d4cad6752be14108f9931ec3514f6682", "pr_number": "70259", "files_changed": ["BUILD.bazel", "aten/src/ATen/Config.h.in", "aten/src/ATen/ParallelOpenMP.cpp", "cmake/Dependencies.cmake"], "labels": ["fb-exported", "cla signed", "ciflow/default"]}, "7cdfd86a72": {"title": "TestMathBits: test with neg and conj bit set (#68948)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/68948\n\nThe case where both the negative and conjugate bits are set\nisn't tested currently despite being handled explicitly by `copy`.\nIn theory this shouldn't matter because neg_bit is only used for real\nvalues, but it does mean the code in copy is untested. So, this just\nruns it with a single sample as a sanity check.\n\nTest Plan: Imported from OSS\n\nReviewed By: jbschlosser\n\nDifferential Revision: D33064371\n\nPulled By: anjali411\n\nfbshipit-source-id: e90c65e311507c4fc618ff74fecc4929599c4fa3", "pr_number": "68948", "files_changed": ["test/test_ops.py", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["open source", "cla signed", "ciflow/default"]}, "2e94a0d282": {"title": "Remove backward ops for NNPACK spatial convolution (#70305)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/70305\n\nTest Plan: Imported from OSS\n\nReviewed By: zou3519\n\nDifferential Revision: D33279223\n\nPulled By: jbschlosser\n\nfbshipit-source-id: f263012b3edaa87ce5430ffd6204a5453360d5dd", "pr_number": "70305", "files_changed": ["aten/src/ATen/native/NNPACK.cpp", "aten/src/ATen/native/native_functions.yaml", "test/backward_compatibility/check_backward_compatibility.py"], "labels": ["cla signed", "ciflow/default"]}, "385c12852e": {"title": "[LTC] Upstream LazyTensor <=> at::Tensor utils (#70066)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/70066\n\nThis commit upstreams utils to convert at::Tensors into LazyTensors and\nvice versa.\n\nTest Plan:\nCovered by test_ptltc on the lazy_tensor_staging branch since TorchScript\nBackend hasn't merged yet.\n\nReviewed By: desertfire\n\nDifferential Revision: D33171590\n\nPulled By: alanwaketan\n\nfbshipit-source-id: b297ff5fc8ca1a02d30e16ad2249985310e836a9", "pr_number": "70066", "files_changed": ["torch/csrc/lazy/core/tensor.cpp", "torch/csrc/lazy/core/tensor.h"], "labels": ["cla signed", "ciflow/default"]}, "23ab6ce723": {"title": "Revert D33141011: extract //c10/macros into its own package", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD33141011 (https://github.com/pytorch/pytorch/commit/8f4c724bb6fe91998cef28f6a16dff79b9d927ad)\n\nOriginal commit changeset: caa97448f922\n\nOriginal Phabricator Diff: D33141011 (https://github.com/pytorch/pytorch/commit/8f4c724bb6fe91998cef28f6a16dff79b9d927ad)\n\nfbshipit-source-id: 79423ed51f9a43ecf1f716a739c74949b66fadb4", "pr_number": null, "files_changed": ["c10/BUILD.bazel", "c10/macros/BUILD.bazel"], "labels": []}, "795af1578c": {"title": "Revert D33172665: [LTC] Upstream utils to extract BackendDevice from at::Tensor", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD33172665 (https://github.com/pytorch/pytorch/commit/121d06799954b0c0ba24f084f2d78b1a81a4e4ec)\n\nOriginal commit changeset: b334ee358ea7\n\nOriginal Phabricator Diff: D33172665 (https://github.com/pytorch/pytorch/commit/121d06799954b0c0ba24f084f2d78b1a81a4e4ec)\n\nfbshipit-source-id: 8bff43cddfc5d30483ec5cea8eff037aab9d1cfa", "pr_number": null, "files_changed": ["test/cpp/lazy/test_backend_device.cpp", "torch/csrc/lazy/backend/backend_device.cpp", "torch/csrc/lazy/backend/backend_device.h"], "labels": []}, "5a9ea9e386": {"title": "Automated submodule update: tensorpipe (#70438)", "body": "Summary:\nThis is an automated pull request to update the first-party submodule for [pytorch/tensorpipe](https://github.com/pytorch/tensorpipe).\n\nNew submodule commit: https://github.com/pytorch/tensorpipe/commit/52791a2fd214b2a9dc5759d36725909c1daa7f2e\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/70438\n\nTest Plan: Ensure that CI jobs succeed on GitHub before landing.\n\nReviewed By: zertosh\n\nDifferential Revision: D33331758\n\nfbshipit-source-id: 1e811ddc30e9afa440523c6cb5c4e893eb560978", "pr_number": "70438", "files_changed": ["third_party/tensorpipe"], "labels": ["open source", "cla signed", "ciflow/default"]}, "15f14ce0dc": {"title": "fix typo in adam docs (#70387)", "body": "Summary:\nFix the typo in [adam docs in master branch](https://pytorch.org/docs/master/generated/torch.optim.Adam.html#torch.optim.Adam)\n\n![image](https://user-images.githubusercontent.com/41060790/147345284-37e180d1-fd06-4a62-9c79-2d17b8aa5cd3.png)\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/70387\n\nReviewed By: H-Huang\n\nDifferential Revision: D33309283\n\nPulled By: albanD\n\nfbshipit-source-id: d20c5d8f2498ac64013f71e202a6b50dcc069f2b", "pr_number": "70387", "files_changed": ["torch/optim/adam.py"], "labels": ["open source", "cla signed", "ciflow/default"]}, "c732a26e59": {"title": "Add macro to register CPU kernel for all arch types (#70332)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/70332\n\nIdea to avoid recompilations: what if we introduce a new macro REGISTER_ALL_CPU_DISPATCH that registers the same kernel across all CPU arch types? We'd call this from native/Convolution*.cpp and wouldn't need to move any logic underneath the native/cpu dir. That would simplify these PRs quite a bit and would also avoid the recompilation. Wdyt about this approach?\n\nTest Plan: Imported from OSS\n\nReviewed By: bdhirsh\n\nDifferential Revision: D33301403\n\nPulled By: jbschlosser\n\nfbshipit-source-id: d7cc163d4fe23c35c93e512d1f0a8af8c9897933", "pr_number": "70332", "files_changed": ["aten/src/ATen/native/DispatchStub.h"], "labels": ["cla signed", "ciflow/default"]}, "14f4b91f6e": {"title": "Add Nondeterministic Tol to gradient test in test_modules (#69402)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/69402\n\nTest Plan: Imported from OSS\n\nReviewed By: soulitzer\n\nDifferential Revision: D33285781\n\nPulled By: mikaylagawarecki\n\nfbshipit-source-id: f1ab43173d4f558adc943a8acefc13c34cfa5cfa", "pr_number": "69402", "files_changed": ["test/test_modules.py", "torch/testing/_internal/common_modules.py"], "labels": ["cla signed", "ciflow/default"]}, "fb78a31916": {"title": "Add testing across mem_formats to ModuleInfos (#69317)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/69317\n\nTest Plan: Imported from OSS\n\nReviewed By: jbschlosser\n\nDifferential Revision: D33285780\n\nPulled By: mikaylagawarecki\n\nfbshipit-source-id: 1d19293e640e5581351a9c74892dcac4bcdd3f1d", "pr_number": "69317", "files_changed": ["test/test_modules.py", "torch/testing/_internal/common_modules.py"], "labels": ["cla signed", "ciflow/default", "with-ssh"]}, "18dd5cdba5": {"title": "[Operator Versioning][Test] Use hypothesis for better test input data and broader coverage (#70263)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/70263\n\nLeverage the hypothesis library as it's more systematic way for testing. To write a test, it needs two parts:\n\n1. A function that looks like a normal test in your test framework of choice but with some additional arguments\n2. A given decorator that specifies how to provide those arguments.\nghstack-source-id: 146344955\n\nTest Plan:\n```\n\nbuck test mode/opt //caffe2/test:jit\npython test/test_jit.py TestSaveLoadForOpVersion\n\n```\n\nReviewed By: iseeyuan\n\nDifferential Revision: D33244389\n\nfbshipit-source-id: c93d23f3d9575ebcb4e927a8caee42f4c3a6939d", "pr_number": "70263", "files_changed": ["test/jit/test_save_load_for_op_version.py"], "labels": ["oncall: jit", "cla signed", "ciflow/default"]}, "35251a5528": {"title": "[PyTorch] Add Enum to IValue Deepcopy (#69937)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/69937\n\nThis enables ```export_torch_mobile_model``` compatibility with Enum IValues\n\nTest Plan: ModuleAPITest.DeepCopyEnum\n\nReviewed By: gmagogsfm\n\nDifferential Revision: D33104681\n\nfbshipit-source-id: ca2a6d259c312487fe38dd1bed33ab6b7910bc2a", "pr_number": "69937", "files_changed": ["aten/src/ATen/core/ivalue.cpp", "test/cpp/jit/test_module_api.cpp"], "labels": ["oncall: jit", "fb-exported", "cla signed", "ciflow/default"]}, "14d3d29b16": {"title": "make ProcessException pickleable (#70118)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/70116\n\nHappy to add tests if you let me know the best place to put them.\n\ncc VitalyFedyunin\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/70118\n\nReviewed By: malfet\n\nDifferential Revision: D33255899\n\nPulled By: ejguan\n\nfbshipit-source-id: 41d495374182eb28bb8bb421e890eca3bddc077b", "pr_number": "70118", "files_changed": ["test/test_multiprocessing_spawn.py", "torch/multiprocessing/spawn.py"], "labels": ["module: multiprocessing", "triaged", "open source", "cla signed", "ciflow/default"]}, "bc40fb5639": {"title": "[Reinstate] Wishart distribution (#70377)", "body": "Summary:\nImplement https://github.com/pytorch/pytorch/issues/68050\nReopened merged and reverted PR https://github.com/pytorch/pytorch/issues/68588 worked with neerajprad\ncc neerajprad\n\nSorry for the confusion.\n\nTODO:\n\n- [x] Unit Test\n- [x] Documentation\n- [x] Change constraint of matrix variables with 'torch.distributions.constraints.symmetric' if it is reviewed and merged. Debug positive definite constraints https://github.com/pytorch/pytorch/issues/68720\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/70377\n\nReviewed By: mikaylagawarecki\n\nDifferential Revision: D33355132\n\nPulled By: neerajprad\n\nfbshipit-source-id: e968c0d9a3061fb2855564b96074235e46a57b6c", "pr_number": "70377", "files_changed": ["docs/source/distributions.rst", "test/distributions/test_distributions.py", "torch/distributions/__init__.py", "torch/distributions/wishart.py"], "labels": ["triaged", "open source", "cla signed", "ciflow/default"]}, "779f41a78a": {"title": "[quant] Add a e2e test for standalone module + custom backend_config_dict (#70152)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/70152\n\nThis is to demonstrate our backend_config_dict works for one of\nour internal use cases\n\nTest Plan:\npython test/fx2trt/test_quant_trt.py\n\nImported from OSS\n\nReviewed By: vkuzo, raghuramank100\n\nDifferential Revision: D33205161\n\nfbshipit-source-id: dca8570816baaf85a79f2be75378d46c3af0e454", "pr_number": "70152", "files_changed": ["test/fx2trt/test_quant_trt.py"], "labels": ["cla signed", "ciflow/default"]}, "1e67570f3a": {"title": "Drop omp simd from batch_permutation_op.cc (#70579)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/70579\n\nFixes\n```\n     36 stderr: caffe2/caffe2/operators/batch_permutation_op.cc:25:1: error: loop not vectorized: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering [-Werror,-Wpass-failed=transform-warning]\n      3 caffe2/caffe2/operators/batch_permutation_op.cc:25:1: error: loop not vectorized: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering [-Werror,-Wpass-failed=transform-warning]\n```\n\nTest Plan: Sandcastle\n\nReviewed By: meyering\n\nDifferential Revision: D33378925\n\nfbshipit-source-id: 5ae3bfb8fadfa91a13ff0dcf5fae2ce7864ea90e", "pr_number": "70579", "files_changed": ["caffe2/operators/batch_permutation_op.cc"], "labels": ["fb-exported", "cla signed", "ciflow/default"]}, "bc3246453b": {"title": "Added explicit build command for Windows and clarification on obtaining (#70190)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/70190\n\nC++ build tools to readme.md\n\nTest Plan: Imported from OSS\n\nReviewed By: jbschlosser\n\nDifferential Revision: D33245438\n\nPulled By: ikriv\n\nfbshipit-source-id: ef863d68926bd7416d0e10d24197d19392c124de", "pr_number": "70190", "files_changed": ["README.md"], "labels": ["cla signed", "ciflow/default"]}, "f64906f470": {"title": "ibm z14/15 SIMD support (#66407)", "body": "Summary:\nhttps://github.com/pytorch/pytorch/issues/66406\nimplemented z arch 14/15 vector SIMD additions.\nso far besides bfloat all other types have their SIMD implementation.\n\nit has 99% coverage and currently passing the local test.\nit is concise and the main SIMD file is only one header file\nit's using template metaprogramming, mostly. but still, there are a few macrosses left with the intention not to modify PyTorch much\nSleef supports z15\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66407\n\nReviewed By: mrshenli\n\nDifferential Revision: D33370163\n\nPulled By: malfet\n\nfbshipit-source-id: 0e5a57f31b22a718cd2a9ac59753fb468cdda140", "pr_number": "66407", "files_changed": ["aten/src/ATen/Version.cpp", "aten/src/ATen/cpu/vec/intrinsics.h", "aten/src/ATen/cpu/vec/vec256/vec256.h", "aten/src/ATen/cpu/vec/vec256/zarch/vec256_zarch.h", "aten/src/ATen/cpu/vec/vec_base.h", "aten/src/ATen/native/BatchLinearAlgebraKernel.cpp", "aten/src/ATen/native/DispatchStub.cpp", "aten/src/ATen/native/DispatchStub.h", "aten/src/ATen/native/SegmentReduce.cpp", "aten/src/ATen/native/cpu/ReduceOpsKernel.cpp", "aten/src/ATen/native/mkl/SpectralOps.cpp", "aten/src/ATen/test/vec_test_all_types.cpp", "aten/src/ATen/test/vec_test_all_types.h", "cmake/Codegen.cmake", "cmake/Dependencies.cmake", "cmake/Modules/FindZVECTOR.cmake"], "labels": ["triaged", "module: vectorization", "open source", "cla signed", "ciflow/default"]}, "4d08db0cb2": {"title": "Flaky tests reporting: use GITHUB_RUN_ID instead of concatenated value (#70604)", "body": "Summary:\nI did not realize the WORKFLOW_ID variable in our GHA scripts concatenated RUN_ID and RUN_NUMBER.\n\nFor flaky tests collection, we should be only using RUN_ID, which makes it easier for us to write queries on the data\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/70604\n\nReviewed By: suo\n\nDifferential Revision: D33409503\n\nPulled By: janeyx99\n\nfbshipit-source-id: 932405989dc1a406dfe9da9a7f513ca127c8d436", "pr_number": "70604", "files_changed": ["tools/stats/print_test_stats.py"], "labels": ["cla signed", "ciflow/default"]}, "bb5b4cceb6": {"title": "Revert \"Revert D32498569: allow external backend codegen to toggle whether to generate out= and inplace kernels\" (#69950)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/69950\n\nThis reverts commit f6cad53443704dfe5a20cc62bee14d91e3bffcaa.\n\nTest Plan: Imported from OSS\n\nReviewed By: albanD\n\nDifferential Revision: D33113545\n\nPulled By: bdhirsh\n\nfbshipit-source-id: d6590294662588d36c09662dea65919ad4e1e288", "pr_number": "69950", "files_changed": ["test/run_test.py", "test/test_gen_backend_stubs.py", "tools/codegen/gen_backend_stubs.py", "tools/test/test_gen_backend_stubs.py"], "labels": ["cla signed", "ciflow/default"]}, "7b8c43cd7c": {"title": "Revert \"Revert D32498570: make codegen'd device guards not cuda-specific. Allow them to be used in external codegen\" (#69951)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/69951\n\nThis reverts commit 0ef523633fddf2d63e97d5028b00af10ff344561.\n\nTest Plan: Imported from OSS\n\nReviewed By: albanD\n\nDifferential Revision: D33113543\n\nPulled By: bdhirsh\n\nfbshipit-source-id: b28073ee0870b413ea9f617f27671ae5c6f3c696", "pr_number": "69951", "files_changed": ["tools/codegen/dest/register_dispatch_key.py", "tools/codegen/gen.py", "tools/codegen/gen_backend_stubs.py", "tools/codegen/model.py", "tools/test/test_gen_backend_stubs.py"], "labels": ["cla signed", "ciflow/default"]}, "7e58b1dd7b": {"title": "Sets device guard in _cudnn_impl functions (#70406)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/70404\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/70406\n\nReviewed By: mruberry\n\nDifferential Revision: D33407972\n\nPulled By: ngimel\n\nfbshipit-source-id: 6bf97602ea13f8eaaff95d9f412a2eeaa0e6ba10", "pr_number": "70406", "files_changed": ["aten/src/ATen/native/cudnn/RNN.cpp"], "labels": ["cla signed", "ciflow/default"]}, "a60adc7f8a": {"title": "fractional_max_pool2d_backward: port to structured kernel (#68245)", "body": "Summary:\nPorted to structured kernel the fractional_max_pool2d_backward.\n\nRef https://github.com/pytorch/pytorch/issues/55070\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/68245\n\nReviewed By: jbschlosser\n\nDifferential Revision: D33405521\n\nPulled By: bdhirsh\n\nfbshipit-source-id: 4930e870d4025485317208df751bc3721ecdb7eb", "pr_number": "68245", "files_changed": ["aten/src/ATen/native/FractionalMaxPool2d.cpp", "aten/src/ATen/native/cuda/FractionalMaxPool2d.cu", "aten/src/ATen/native/native_functions.yaml"], "labels": ["triaged", "open source", "cla signed", "ciflow/default"]}, "95a1952633": {"title": "add SparseXPU to dispatch key set autogradother_backends (#70443)", "body": "Summary:\nAccording to dispatch table computation logic, if no kernel\nregister to a certain dispatch key, will use CompositeExplicitAutograd\nbackend kernel, so we need add sparseXPU key to the alias key pool.\n\nSigned-off-by: Ma, Jing1 <jing1.ma@intel.com>\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/70443\n\nReviewed By: jbschlosser\n\nDifferential Revision: D33406004\n\nPulled By: bdhirsh\n\nfbshipit-source-id: 009037739c818676901b10465632d3fef5ba14f2", "pr_number": "70443", "files_changed": ["c10/core/DispatchKeySet.h"], "labels": ["triaged", "open source", "cla signed", "ciflow/default"]}, "f8f96d4858": {"title": "Copy: Re-use existing neg and conj kernel implementations (#68949)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/68949\n\nThis reuses the existing `neg_kernel` and `conj_kernel`\nimplementations for copy, saving some binary size and compile time.\n\nTest Plan: Imported from OSS\n\nReviewed By: ngimel\n\nDifferential Revision: D33064390\n\nPulled By: anjali411\n\nfbshipit-source-id: eb0ee94ed3db44ae828ea078ba616365f97a7ff5", "pr_number": "68949", "files_changed": ["aten/src/ATen/native/Copy.h", "aten/src/ATen/native/cpu/CopyKernel.cpp", "aten/src/ATen/native/cpu/UnaryOpsKernel.cpp", "aten/src/ATen/native/cuda/Copy.cu", "aten/src/ATen/native/cuda/DistributionTemplates.h", "aten/src/ATen/native/cuda/TriangularOps.cu"], "labels": ["open source", "cla signed", "ciflow/default"]}, "b16b444828": {"title": "don't unsqueeze every stack arg if possible (#70288)", "body": "Summary:\nFixes T98738497\nUse `cat` and `view` if possible, instead of unsqueezing every arg. Helps perf when there are a lot of small arguments to `stack`.\nBenchmark:\n```\nimport torch\nfrom torch.utils.benchmark import Timer\n\ninputs =  [torch.randn([1, 128]) for _ in range(500)]\nout = torch.empty(1,500,128)\ndef stack_cat(inputs):\n    cat_result = torch.concat(inputs, dim=1)\n    return cat_result.view( [1, 500, 128])\n\ntimer_stack = Timer(stmt=\"torch.stack(inputs, dim=1)\", globals=globals())\ntimer_cat = Timer(stmt=\"stack_cat(inputs)\", globals=globals())\nprint(\"stack \", timer_stack.blocked_autorange().median)\nprint(\"cat \", timer_cat.blocked_autorange().median)\n```\nBefore:\n```\nstack  0.00023390522226691247\ncat  7.437262553721667e-05\n```\nAfter\n```\nstack  7.397504318505526e-05\ncat  7.37407322973013e-05\n```\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/70288\n\nReviewed By: robieta, mruberry\n\nDifferential Revision: D33289789\n\nPulled By: ngimel\n\nfbshipit-source-id: b57dcb8ec66e767f552c08deeba330f31ae6c3d0", "pr_number": "70288", "files_changed": ["aten/src/ATen/native/TensorShape.cpp", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["cla signed", "ciflow/default"]}, "2292520bdc": {"title": "Fix genSparseCSRTensor: generate non-trivial values for uint8 dtype. (#70580)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/70580\n\ncc nikitaved pearu cpuhrsch\n\nTest Plan: Imported from OSS\n\nReviewed By: jbschlosser\n\nDifferential Revision: D33413597\n\nPulled By: cpuhrsch\n\nfbshipit-source-id: 313b08e1bd96ffb8d5c7a0fda9384502325e5d08", "pr_number": "70580", "files_changed": ["torch/testing/_internal/common_utils.py"], "labels": ["module: sparse", "open source", "cla signed", "module: testing", "ciflow/default"]}, "12653be434": {"title": "[PyTorch] Optimize no input NVTX collection (#70133)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/70133\n\nwe were creating `sstream` + string concats via `getNvtxStr` even when there were no inputs and wasting precious time. this diff avoids `stringstream` when there is no input to squeeze performance. 60% reduction in overhead\n\nTest Plan:\nBefore\n```\nI1214 22:48:07.964118 2971180 bench.cpp:154] Mean 0.970494\nI1214 22:48:07.964139 2971180 bench.cpp:155] Median 0.969054\nI1214 22:48:07.964144 2971180 bench.cpp:156] Min 0.962247\nI1214 22:48:07.964148 2971180 bench.cpp:157] stddev 0.00774841\nI1214 22:48:07.964154 2971180 bench.cpp:158] stddev / mean 0.00798398\n```\n\nAfter\n```\nI1214 22:59:00.039872 3437853 bench.cpp:154] Mean 0.384333\nI1214 22:59:00.039896 3437853 bench.cpp:155] Median 0.384886\nI1214 22:59:00.039899 3437853 bench.cpp:156] Min 0.370235\nI1214 22:59:00.039902 3437853 bench.cpp:157] stddev 0.00435907\nI1214 22:59:00.039907 3437853 bench.cpp:158] stddev / mean 0.0113419\n```\n\nReviewed By: aaronenyeshi, robieta\n\nDifferential Revision: D33137501\n\nfbshipit-source-id: ce0e8cf9aef7ea22fd8aed927e76be4ca375efc3", "pr_number": "70133", "files_changed": ["torch/csrc/profiler/util.cpp"], "labels": ["fb-exported", "cla signed", "ciflow/default"]}, "1681323ddc": {"title": "DOC: Merge extraheader block from theme instead of override (#70187)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/70185\n\nThe extraheader block in docs/source/_templates/layout.html overrides the one from the pytorch theme. The theme block adds Google Analytics, so they were missing from the `master` documentation. This came up in PR pytorch/pytorch.github.io#899.\n\nbrianjo\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/70187\n\nReviewed By: bdhirsh\n\nDifferential Revision: D33248466\n\nPulled By: malfet\n\nfbshipit-source-id: b314916a3f0789b6617cf9ba6bd938bf5ca27242", "pr_number": "70187", "files_changed": ["docs/source/_templates/layout.html"], "labels": ["open source", "cla signed", "ciflow/default"]}, "e1aa5db108": {"title": "Bazel: Only run ATen codegen once (#70147)", "body": "Summary:\nDue to a merge conflict, the new bazel cuda build does something\nrather obnoxious. It runs ATen codegen with `--per-operator-headers`\nenabled and extracts a subset of the generated files; then calls it\nagain without the flag to extract the CUDA files.\n\nThis PR instead calls the codegen once but keeps track of what is\nCPU and what is CUDA in separate lists.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/70147\n\nReviewed By: VitalyFedyunin\n\nDifferential Revision: D33413020\n\nPulled By: malfet\n\nfbshipit-source-id: 4b502c38a209d1aa63d715e2336df6fc5aac2212", "pr_number": "70147", "files_changed": ["BUILD.bazel"], "labels": ["triaged", "open source", "cla signed", "ciflow/default"]}, "adceb13da1": {"title": "Copy: Avoid extra dispatch in type-mismatch case (#68950)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/68950\n\nTest Plan: Imported from OSS\n\nReviewed By: albanD\n\nDifferential Revision: D33064447\n\nPulled By: anjali411\n\nfbshipit-source-id: 82bf4e144c1e629e30226eedc9d26ca63cfb4431", "pr_number": "68950", "files_changed": ["aten/src/ATen/native/cpu/CopyKernel.cpp"], "labels": ["open source", "cla signed", "ciflow/default"]}, "20489ebdc9": {"title": "Increase tensor size for mem check tests (#70603)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/70226\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/70603\n\nReviewed By: mruberry\n\nDifferential Revision: D33410439\n\nPulled By: janeyx99\n\nfbshipit-source-id: e94615ece6d0fdf230de5297118678b70f34a18c", "pr_number": "70603", "files_changed": ["test/test_cuda.py"], "labels": ["cla signed", "ciflow/default", "ciflow/win"]}, "70d3b2700f": {"title": "[LTC] Fix stride accessors in LTCTensorImpl (#70623)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/70623\n\nStrides on lazy tensor should only be read after calling setup_size_properties. This fixes a failure in hf_Longformer.\n\nTest Plan: CI on the lazy_tensor_staging branch\n\nReviewed By: wconstab, alanwaketan\n\nDifferential Revision: D33410142\n\nPulled By: desertfire\n\nfbshipit-source-id: ccb2ba8d258bdb88f6b51be6196563f9c4c06cbf", "pr_number": "70623", "files_changed": ["torch/csrc/lazy/core/tensor_impl.cpp", "torch/csrc/lazy/core/tensor_impl.h"], "labels": ["cla signed", "ciflow/default"]}, "93c7504438": {"title": "[PyTorch] Improve StorageImpl::set_data_ptr (#65432)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65432\n\nThere is no reason to do an extra write to the input DataPtr (via `std::swap`) before returning a new DataPtr.\nghstack-source-id: 146471376\n\nTest Plan:\nInspected assembly for this function to verify that we are\nreally getting fewer instructions generated. I don't have a specific\napplication for this at the moment, but it's clearly better IMO.\n\nReviewed By: mikeiovine\n\nDifferential Revision: D31097807\n\nfbshipit-source-id: 06ff6f5fc675df0f38b0315b4147ed959243b6d0", "pr_number": "65432", "files_changed": ["c10/core/StorageImpl.h"], "labels": ["cla signed", "ciflow/default"]}, "34c49d3d3b": {"title": "Document torch.quantile interpolation kwarg (#70637)", "body": "Summary:\nclone of https://github.com/pytorch/pytorch/pull/59397\n\nThis PR documents the interpolation kwarg parameter added in https://github.com/pytorch/pytorch/issues/49267. Now that the forward compatibility period is over, we can expose this parameter.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/70637\n\nReviewed By: jbschlosser\n\nDifferential Revision: D33411707\n\nPulled By: anjali411\n\nfbshipit-source-id: f5f2d0a6739b3a855bbdf58fc671ac2f0342ce69", "pr_number": "70637", "files_changed": ["aten/src/ATen/autocast_mode.cpp", "aten/src/ATen/native/Sorting.cpp", "aten/src/ATen/native/native_functions.yaml", "test/backward_compatibility/check_backward_compatibility.py", "torch/_tensor_docs.py", "torch/_torch_docs.py", "torch/overrides.py"], "labels": ["cla signed", "ciflow/default"]}, "025cd69a86": {"title": "[AMD] Fix some legacy hipify script (#70594)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/70594\n\nPull Request resolved: https://github.com/facebookincubator/gloo/pull/315\n\nFix some out-dated hipify script:\n* python -> python3 (fb internal)\n* rocblas return code\n* gloo makefile for hip clang\n\nTest Plan: Sandcastle + OSS build\n\nReviewed By: malfet, shintaro-iwasaki\n\nDifferential Revision: D33402839\n\nfbshipit-source-id: 5893039451bcf77bbbb1b88d2e46ae3e39caa154", "pr_number": "70594", "files_changed": ["caffe2/core/common_gpu.cc", "tools/amd_build/build_amd.py"], "labels": ["fb-exported", "cla signed", "ciflow/default"]}, "a5bc44422a": {"title": "[PyTorch] Remove the List/Dict move operations (#69370)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/69370\n\nThese operations are likely slower than copying because they perform a heap allocation and reference count bump, whereas copying is just a reference count bump. This diff is up to see 1) if anything breaks 2) if we can measure any improvements.\nghstack-source-id: 146468907\n\nTest Plan:\nRan //sigrid/lib/features/tests:pytorch_feature_conversion_benchmark before/after\n\n```\nswolchok@devbig032 ~/f/fbcode> for x in (seq 5); sudo scripts/bertrand/noise/denoise.sh /tmp/pytorch_feature_conversion_benchmark.Dec7Stable ; end\n============================================================================\nsigrid/lib/features/tests/PyTorchFeatureConversionBenchmark.cpprelative  time/iter  iters/s\n============================================================================\nPyTorchFeatureConversionDenseBenchmark                       2.43us  410.68K\nPyTorchFeatureConversionIdListBenchmark                      3.74us  267.65K\nPyTorchFeatureConversionIdScoreListBenchmark                 4.98us  200.81K\n============================================================================\n============================================================================\nsigrid/lib/features/tests/PyTorchFeatureConversionBenchmark.cpprelative  time/iter  iters/s\n============================================================================\nPyTorchFeatureConversionDenseBenchmark                       2.43us  410.75K\nPyTorchFeatureConversionIdListBenchmark                      3.75us  266.92K\nPyTorchFeatureConversionIdScoreListBenchmark                 4.98us  200.97K\n============================================================================\n============================================================================\nsigrid/lib/features/tests/PyTorchFeatureConversionBenchmark.cpprelative  time/iter  iters/s\n============================================================================\nPyTorchFeatureConversionDenseBenchmark                       2.44us  410.43K\nPyTorchFeatureConversionIdListBenchmark                      3.75us  266.75K\nPyTorchFeatureConversionIdScoreListBenchmark                 5.04us  198.23K\n============================================================================\n============================================================================\nsigrid/lib/features/tests/PyTorchFeatureConversionBenchmark.cpprelative  time/iter  iters/s\n============================================================================\nPyTorchFeatureConversionDenseBenchmark                       2.43us  411.17K\nPyTorchFeatureConversionIdListBenchmark                      3.74us  267.60K\nPyTorchFeatureConversionIdScoreListBenchmark                 5.00us  199.84K\n============================================================================\n============================================================================\nsigrid/lib/features/tests/PyTorchFeatureConversionBenchmark.cpprelative  time/iter  iters/s\n============================================================================\nPyTorchFeatureConversionDenseBenchmark                       2.44us  410.19K\nPyTorchFeatureConversionIdListBenchmark                      3.73us  267.89K\nPyTorchFeatureConversionIdScoreListBenchmark                 4.96us  201.46K\n============================================================================\nswolchok@devbig032 ~/f/fbcode> for x in (seq 5); sudo scripts/bertrand/noise/denoise.sh /tmp/pytorch_feature_conversion_benchmark.Dec8RemoveListAndDictMove ; end\n============================================================================\nsigrid/lib/features/tests/PyTorchFeatureConversionBenchmark.cpprelative  time/iter  iters/s\n============================================================================\nPyTorchFeatureConversionDenseBenchmark                       2.47us  405.12K\nPyTorchFeatureConversionIdListBenchmark                      3.60us  278.07K\nPyTorchFeatureConversionIdScoreListBenchmark                 4.87us  205.44K\n============================================================================\n============================================================================\nsigrid/lib/features/tests/PyTorchFeatureConversionBenchmark.cpprelative  time/iter  iters/s\n============================================================================\nPyTorchFeatureConversionDenseBenchmark                       2.45us  407.39K\nPyTorchFeatureConversionIdListBenchmark                      3.63us  275.56K\nPyTorchFeatureConversionIdScoreListBenchmark                 4.95us  202.17K\n============================================================================\n============================================================================\nsigrid/lib/features/tests/PyTorchFeatureConversionBenchmark.cpprelative  time/iter  iters/s\n============================================================================\nPyTorchFeatureConversionDenseBenchmark                       2.47us  405.49K\nPyTorchFeatureConversionIdListBenchmark                      3.63us  275.58K\nPyTorchFeatureConversionIdScoreListBenchmark                 4.88us  205.05K\n============================================================================\n============================================================================\nsigrid/lib/features/tests/PyTorchFeatureConversionBenchmark.cpprelative  time/iter  iters/s\n============================================================================\nPyTorchFeatureConversionDenseBenchmark                       2.52us  396.13K\nPyTorchFeatureConversionIdListBenchmark                      3.59us  278.29K\nPyTorchFeatureConversionIdScoreListBenchmark                 4.88us  204.94K\n============================================================================\n============================================================================\nsigrid/lib/features/tests/PyTorchFeatureConversionBenchmark.cpprelative  time/iter  iters/s\n============================================================================\nPyTorchFeatureConversionDenseBenchmark                       2.46us  406.77K\nPyTorchFeatureConversionIdListBenchmark                      3.62us  276.17K\nPyTorchFeatureConversionIdScoreListBenchmark                 4.92us  203.07K\n============================================================================\n```\n\nReviewed By: suo, hlu1\n\nDifferential Revision: D32836701\n\nfbshipit-source-id: 6e1c3d81f1b4ee13156320263dac17f5256c1462", "pr_number": "69370", "files_changed": ["aten/src/ATen/core/Dict.h", "aten/src/ATen/core/Dict_inl.h", "aten/src/ATen/core/List.h", "aten/src/ATen/core/List_inl.h", "aten/src/ATen/core/List_test.cpp", "aten/src/ATen/test/Dict_test.cpp"], "labels": ["module: bc-breaking", "cla signed", "ciflow/default"]}, "2431218ee4": {"title": "Jiterates more ops (#70663)", "body": "Summary:\nThis PR jiterates:\n\n- lcm\n- i0e\n- i1e\n- ndtri\n- erfcx\n- digamma\n- trigamma\n- lgamma\n\nIt also adds TODOs to jiterate `kaiser_window`, `igamma`, `igammac` and `polygamma`, but jiterating those ops requires more features.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/70663\n\nReviewed By: ngimel\n\nDifferential Revision: D33420854\n\nPulled By: mruberry\n\nfbshipit-source-id: 6f32ac3cf24eda051bf19b6d20e94cdf81f50761", "pr_number": "70663", "files_changed": ["aten/src/ATen/native/Math.h", "aten/src/ATen/native/cuda/GcdLcmKernel.cu", "aten/src/ATen/native/cuda/IGammaKernel.cu", "aten/src/ATen/native/cuda/Math.cuh", "aten/src/ATen/native/cuda/UnaryGammaKernels.cu", "aten/src/ATen/native/cuda/UnarySpecialOpsKernel.cu", "aten/src/ATen/native/cuda/ZetaKernel.cu", "aten/src/ATen/native/cuda/jit_utils.cu"], "labels": ["cla signed", "ciflow/default"]}, "f9e1a1c97f": {"title": "Increase tolerance for test_adadelta (#69919)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/69698\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/69919\n\nReviewed By: cpuhrsch\n\nDifferential Revision: D33286427\n\nPulled By: jbschlosser\n\nfbshipit-source-id: a2ca90683c14b6669f9b1804881ac675ba925fc5", "pr_number": "69919", "files_changed": ["test/test_optim.py"], "labels": ["triaged", "open source", "cla signed", "ciflow/default"]}, "3a21f38a2e": {"title": "Integrate multi_tensor zero_grad into Optimizer base class (#69936)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/69936\n\nCurrently, the optimizers in `torch/optim/_multi_tensor/` all override the base Optimizer class' implementation of `zero_grad` with the same foreach zero_grad implementation (e.g. [here](https://github.com/pytorch/pytorch/blob/master/torch/optim/_multi_tensor/adadelta.py#L93-L114)). There is a TODO that indicates that this should be refactored to the base class once the foreach ops are in good shape. This PR is intended to address that TODO.\n\nTest Plan: Imported from OSS\n\nReviewed By: mrshenli\n\nDifferential Revision: D33346748\n\nPulled By: mikaylagawarecki\n\nfbshipit-source-id: 6573f4776aeac757b6a778894681868191a1b4c7", "pr_number": "69936", "files_changed": ["torch/optim/_multi_tensor/adadelta.py", "torch/optim/_multi_tensor/adagrad.py", "torch/optim/_multi_tensor/adam.py", "torch/optim/_multi_tensor/adamax.py", "torch/optim/_multi_tensor/adamw.py", "torch/optim/_multi_tensor/asgd.py", "torch/optim/_multi_tensor/nadam.py", "torch/optim/_multi_tensor/radam.py", "torch/optim/_multi_tensor/rmsprop.py", "torch/optim/_multi_tensor/rprop.py", "torch/optim/_multi_tensor/sgd.py", "torch/optim/optimizer.py"], "labels": ["cla signed", "ciflow/default"]}, "917d56a7e4": {"title": "Copy: Fix conj bit being ignored on type mismatch (#68963)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/68963\n\nTest Plan: Imported from OSS\n\nReviewed By: ngimel\n\nDifferential Revision: D33064492\n\nPulled By: anjali411\n\nfbshipit-source-id: 043f927d6bfff46bf5f8ea6fce9409f250bf8ff8", "pr_number": "68963", "files_changed": ["aten/src/ATen/native/cuda/Copy.cu", "test/test_torch.py"], "labels": ["open source", "cla signed", "ciflow/default"]}, "012c38e04d": {"title": "Add contiguous_strides as a correct replacement of defaultStride (#67789)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/67789\n\n`at::defaultStride` was added in https://github.com/pytorch/pytorch/pull/18779.\nAs it was noted in that PR, it differs from the actual computation of\nthe default strides when one or more of the dimensions of the tensor are\nzero. See https://github.com/pytorch/pytorch/pull/18779#discussion_r272296140\n\nWe add two functions, `contiguous_strides` and `contiguous_strides_vec`\nwhich correct this issue and we replace the previous (wrong) uses of\n`defaultStride`.\n\ncc jianyuh nikitaved pearu mruberry walterddr IvanYashchuk xwang233 Lezcano\n\nTest Plan: Imported from OSS\n\nReviewed By: ngimel\n\nDifferential Revision: D32684852\n\nPulled By: mruberry\n\nfbshipit-source-id: 62997a5a97a4241a12e73e2be2e192b80b491cb1", "pr_number": "67789", "files_changed": ["aten/src/ATen/native/BatchLinearAlgebra.cpp", "aten/src/ATen/native/LinearAlgebraUtils.h", "aten/src/ATen/native/TriangularOpsUtils.h", "aten/src/ATen/native/cuda/BatchLinearAlgebra.cpp"], "labels": ["open source", "module: linear algebra", "cla signed", "ciflow/default"]}, "4d4e81d869": {"title": "Make linalg.lu_factor structured (#66934)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/66934\n\ncc jianyuh nikitaved pearu mruberry walterddr IvanYashchuk xwang233 Lezcano\n\nTest Plan: Imported from OSS\n\nReviewed By: ngimel\n\nDifferential Revision: D32684856\n\nPulled By: mruberry\n\nfbshipit-source-id: 1675448da9a8677c8420005ce753972234e7accc", "pr_number": "66934", "files_changed": ["aten/src/ATen/native/BatchLinearAlgebra.cpp", "aten/src/ATen/native/native_functions.yaml"], "labels": ["open source", "module: linear algebra", "cla signed", "module: structured kernels", "ciflow/default"]}, "bc514cb425": {"title": "Skip distributed tests if built with USE_DISTRIBUTED=0 (#70677)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/70676\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/70677\n\nReviewed By: albanD\n\nDifferential Revision: D33439808\n\nPulled By: janeyx99\n\nfbshipit-source-id: 7f9971eb564dbbb6625fe5f78328c3abe3808719", "pr_number": "70677", "files_changed": ["test/run_test.py"], "labels": ["open source", "cla signed", "ciflow/default"]}, "524bbb1442": {"title": "[LTC] Sync gen_lazy_tensor.py from the staging branch (#70385)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/70385\n\nThis commit sync gen_lazy_tensor.py from the lazy_tensor_staging branch\nto the master.\n\nTest Plan: CI in the lazy_tensor_staging branch.\n\nReviewed By: wconstab\n\nDifferential Revision: D33306232\n\nPulled By: alanwaketan\n\nfbshipit-source-id: a15c72b22418637f851a6cd4901a9f5c4be75449", "pr_number": "70385", "files_changed": ["tools/codegen/gen_lazy_tensor.py"], "labels": ["cla signed", "ciflow/default"]}, "4fa70a2483": {"title": "[pytorch] fix hipify_python (#70619)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/70619\n\nThis Diff improves `hipify_python`, which is needed for AMD GPUs.\n\nChange 1:\n```\nif (c == \",\" or ind == len(kernel_string) - 1) and closure == 0:\n```\nThis is needed to deal with the following case (ex: https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/test/cuda_vectorized_test.cu#L111)\n```\nkernel<<<val, func()>>>(...)\n// In this case, kernel_string is \"val, func()\"\n// so closure gets 0 when ind == len(kernel_string) - 1.\n```\n\nChange 2:\n```\nmask_comments()\n```\nThis is needed to deal with a case where \"<<<\" is included in a comment or a string literal (ex: https://github.com/pytorch/pytorch/blob/master/torch/csrc/deploy/interpreter/builtin_registry.cpp#L71)\n```\nabc = \"<<<XYZ>>>\"\n// Though this <<<XYZ>>> is irrelevant to CUDA kernels,\n// the current script attempts to hipify this and fails.\n```\n\nTest Plan:\nThis patch fixes errors I encountered by running\n```\npython3 tools/amd_build/build_amd.py\n```\n\nI confirmed, with Linux `diff`, that this patch does not change HIP code that was generated successfully with the original script.\n\nReviewed By: hyuen\n\nDifferential Revision: D33407743\n\nfbshipit-source-id: bec822e040a154be4cda1c294536792ca8d596ae", "pr_number": "70619", "files_changed": ["torch/utils/hipify/hipify_python.py"], "labels": ["fb-exported", "cla signed", "ciflow/default"]}, "cc7382dd92": {"title": "Enable upgraders in TS server (#70539)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/70539\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/70538\n\nghstack-source-id: 146384458\n\nTest Plan: python test/test_jit.py TestUpgraders\n\nReviewed By: gmagogsfm\n\nDifferential Revision: D33375195\n\nfbshipit-source-id: 170960b409175bb987cf9dbb65ffed3283e5f6f9", "pr_number": "70539", "files_changed": ["caffe2/serialize/versions.h"], "labels": ["oncall: jit", "fb-exported", "cla signed", "ciflow/default"]}, "23f902f7e4": {"title": "Fix incorrect variable in autograd docs (#70884)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/68362.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/70884\n\nReviewed By: mruberry\n\nDifferential Revision: D33463331\n\nPulled By: ngimel\n\nfbshipit-source-id: 834ba9c450972710e0424cc92af222551f0b4a4a", "pr_number": "70884", "files_changed": ["docs/source/notes/autograd.rst"], "labels": ["open source", "cla signed", "ciflow/default"]}, "36d9e03ab7": {"title": "Reserve vector in gather_ranges_to_dense_op.h (#70478)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/70478\n\nTest Plan: Sandcastle\n\nReviewed By: xw285cornell\n\nDifferential Revision: D33339890\n\nfbshipit-source-id: 50330e18e344f872d03f146cea0ed11eef4f506e", "pr_number": "70478", "files_changed": ["caffe2/operators/gather_ranges_to_dense_op.h"], "labels": ["fb-exported", "cla signed", "ciflow/default"]}, "87139d8532": {"title": "[LTC] Sync LazyGraphExecutor and LazyTensor with the staging branch (#70867)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/70867\n\nThis commit syncs LazyGraphExecutor and LazyTensor with the staging branch's\nlatest changes.\n\nTest Plan: CI in the lazy_tensor_staging branch.\n\nReviewed By: wconstab, desertfire\n\nDifferential Revision: D33440005\n\nPulled By: alanwaketan\n\nfbshipit-source-id: 0dd72643dbf81a87fc4b05019b6564fcb28f1979", "pr_number": "70867", "files_changed": ["torch/csrc/lazy/backend/backend_interface.h", "torch/csrc/lazy/core/lazy_graph_executor.cpp", "torch/csrc/lazy/core/lazy_graph_executor.h", "torch/csrc/lazy/core/tensor.cpp", "torch/csrc/lazy/core/tensor.h"], "labels": ["cla signed", "ciflow/default"]}, "4e7e8f2826": {"title": "[PyTorch] Outline destructor of CppFunction (#63688)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/63688\n\nCppFunction is used for function registration, so it's not performance-sensitive. Outlining the destructor should reduce code size.\nghstack-source-id: 146648927\n\nTest Plan: Mobile buildsizebot\n\nReviewed By: dhruvbird\n\nDifferential Revision: D30462640\n\nfbshipit-source-id: de410f933bf936c16769a10a52092469007c8487", "pr_number": "63688", "files_changed": ["aten/src/ATen/core/library.cpp", "torch/library.h"], "labels": ["cla signed", "ciflow/default"]}, "8dfff8b2e2": {"title": "Fix scatter for empty indexes (#70662)", "body": "Summary:\nThis PR fixes an issue with `scatter` where the output is garbage for zero-sized indexes.\n\n```py\nimport torch\n\nnull_index = torch.zeros((0, 4), dtype=torch.int64)\nnull_arr = torch.zeros((0, 4))\nzeros_arr = torch.zeros((1, 4))\n\nresult = zeros_arr.scatter(0, null_index, null_arr)\n\nprint(null_index)\nprint(null_arr)\nprint(zeros_arr)\nprint(result)\n```\n\n```\ntensor([], size=(0, 4), dtype=torch.int64)\ntensor([], size=(0, 4))\ntensor([[0., 0., 0., 0.]])\ntensor([[1.7036e+19, 2.9965e+32, 3.9133e-14, 1.3585e-19]])\n```\n\nthe out array is never filled if `index` arg has 0 elements.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/70662\n\nReviewed By: dagitses\n\nDifferential Revision: D33476807\n\nPulled By: albanD\n\nfbshipit-source-id: 97dbdd9c0133899e58828c43ecba81838807b8af", "pr_number": "70662", "files_changed": ["aten/src/ATen/native/TensorAdvancedIndexing.cpp", "test/test_torch.py"], "labels": ["open source", "cla signed", "ciflow/default"]}, "08074c8f2d": {"title": "Update gradcheck.py (#70950)", "body": "Summary:\nFollowing https://github.com/pytorch/pytorch/pull/64837#discussion_r779870974\n\nChanged torch.equal to torch.allclose as exact comparision could be flaky\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/70950\n\nReviewed By: albanD\n\nDifferential Revision: D33462426\n\nPulled By: anjali411\n\nfbshipit-source-id: aeaba9d2a98d1d0af04fa2cab8c495c23ec0a9cc", "pr_number": "70950", "files_changed": ["torch/autograd/gradcheck.py"], "labels": ["module: autograd", "open source", "cla signed", "ciflow/default"]}, "c6e727d05b": {"title": "Fix adamw formula doc (#68587)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/68482\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/68587\n\nReviewed By: dagitses, jbschlosser\n\nDifferential Revision: D33478646\n\nPulled By: albanD\n\nfbshipit-source-id: 4e6419829c3faa7449c041e7d467a6dab30fe917", "pr_number": "68587", "files_changed": ["torch/optim/adamw.py"], "labels": ["cla signed", "ciflow/default"]}, "dd1121435b": {"title": "SequentialLR update _last_lr on step (#70558)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/68956.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/70558\n\nReviewed By: dagitses\n\nDifferential Revision: D33430213\n\nPulled By: albanD\n\nfbshipit-source-id: 446f182610de32db224d55b244d76c3076e8080f", "pr_number": "70558", "files_changed": ["test/test_optim.py", "torch/optim/lr_scheduler.py"], "labels": ["open source", "cla signed", "ciflow/default"]}, "0517e719ac": {"title": "[jit] Add conformance test for DynamicType with server JIT types. (#69482)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/69482\n\nAdd a test to enumerate a number of JIT type combinations and see if their subtyping behavior is preserved in the new DynamicType system.\nghstack-source-id: 146670526\n\nTest Plan: buck test mode/opt //caffe2/test/cpp/jit:jit -- --exact 'caffe2/test/cpp/jit:jit - LiteInterpreterTest.DynamicType'\n\nReviewed By: gmagogsfm\n\nDifferential Revision: D32891263\n\nfbshipit-source-id: 728211b39778e93db011b69b0a4047df78a8fc5b", "pr_number": "69482", "files_changed": ["aten/src/ATen/core/jit_type.h", "aten/src/ATen/core/type.cpp", "test/cpp/jit/test_lite_interpreter.cpp"], "labels": ["oncall: jit", "cla signed", "ciflow/default"]}, "bc026c0577": {"title": "[jit] Split Union type and Optional type to separate impl file. (#69483)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/69483\n\nTo avoid accidental linking to Union type and Optional type in Edge runtimes, we can separate these types into different files, so that we don't accidentally link with them in type.cpp.\nghstack-source-id: 146670525\n\nTest Plan: just code move.\n\nReviewed By: ejguan\n\nDifferential Revision: D32264607\n\nfbshipit-source-id: c60b6246f21f3eb0a67f827a9782f70ce5200da7", "pr_number": "69483", "files_changed": ["aten/src/ATen/core/jit_type.h", "aten/src/ATen/core/type.cpp", "aten/src/ATen/core/union_type.cpp", "tools/build_variables.bzl"], "labels": ["cla signed", "ciflow/default"]}, "338eb1b2b3": {"title": "[LTC] Export torch::lazy::GetBackendDevice() (#70963)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/70963\n\nThis commit exports torch::lazy::GetBackendDevice().\n\nTest Plan: CI in the lazy_tensor_staging branch.\n\nReviewed By: wconstab\n\nDifferential Revision: D33468938\n\nPulled By: alanwaketan\n\nfbshipit-source-id: f65599c9238bf6b4f4ffbd5194befdc267272831", "pr_number": "70963", "files_changed": ["test/cpp/lazy/test_backend_device.cpp", "torch/csrc/lazy/backend/backend_device.h"], "labels": ["cla signed", "ciflow/default"]}, "ad88354e25": {"title": "torch.futures doc formatting (#70630)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/70630\n\nParams is incorrectly formatted [here](https://pytorch.org/docs/master/futures.html?highlight=future#:~:text=way%20as%20then().-,Parameters,-callback%20(Future)%20%E2%80%93%20a):\n\n![image](https://user-images.githubusercontent.com/14858254/148119877-6c719851-4edd-4126-8ef7-e6c1920304cf.png)\n\nUpdated docs:\n\nhttps://docs-preview.pytorch.org/70630/futures.html?highlight=future#:~:text=way%20as%20then().-,Parameters,-callback%20(Future)%20%E2%80%93%20a\n\ncc pietern mrshenli pritamdamania87 zhaojuanmao satgera rohan-varma gqchen aazzolini osalpekar jiayisuse SciPioneer H-Huang\n\nTest Plan: Imported from OSS\n\nReviewed By: dagitses, mrshenli\n\nDifferential Revision: D33478214\n\nPulled By: H-Huang\n\nfbshipit-source-id: 8cd7022ae79a8e6fe8b5fa8b767c55903c9ac368", "pr_number": "70630", "files_changed": ["torch/futures/__init__.py"], "labels": ["oncall: distributed", "cla signed", "ciflow/default"]}, "3f3eae6737": {"title": "[jit] Split Tensor type implementations to separate file. (#70121)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/70121\n\nCode move all TensorType dependencies into a separate `tensor_type.cpp`, so that we don't link with it in the min runtime accidentally.\nghstack-source-id: 146727331\n\n(Note: this ignores all push blocking failures!)\n\nTest Plan: no behavior change.\n\nReviewed By: gmagogsfm\n\nDifferential Revision: D33102286\n\nfbshipit-source-id: e9fe176201bd2696cb8c65c670fcf225e81e8908", "pr_number": "70121", "files_changed": ["aten/src/ATen/core/tensor_type.cpp", "aten/src/ATen/core/type.cpp", "tools/build_variables.bzl"], "labels": ["cla signed", "ciflow/default"]}, "704af23ee4": {"title": "Use a reference in GetSingleArgument (#71007)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/71007\n\nA string copy at Line 417 is currently consuming 125,749,287,000 cycles/day. I suspect the issue is with a copy-on-return, but we can experiment with introducing a reference in the middle to see if that produces a good savings without changing the interface.\n\nReference\n```\n[\"Inline caffe2::ArgumentHelper::GetSingleArgument @ caffe2/caffe2/utils/proto_utils.cc:417\"]\n```\n\nTest Plan: Sandcastle\n\nReviewed By: xw285cornell\n\nDifferential Revision: D33478883\n\nfbshipit-source-id: e863e359c0c718fcd0d52fd4b3c7858067de0670", "pr_number": "71007", "files_changed": ["caffe2/utils/proto_utils.cc"], "labels": ["fb-exported", "cla signed", "ciflow/default"]}, "11aa1961c1": {"title": "Use (void)error_unused to avoid unused warning (#71000)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/71000\n\nTest Plan: Sandcastle\n\nReviewed By: ngimel\n\nDifferential Revision: D33470600\n\nfbshipit-source-id: 868a6ee33a04846bd1efbe06ab306fbaad3bf9db", "pr_number": "71000", "files_changed": ["c10/cuda/CUDAException.h"], "labels": ["fb-exported", "cla signed", "ciflow/default"]}, "785b6905de": {"title": "reduce plan generation log spam (#70880)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/70880\n\nChange loglevel to `debug` in caffe2 `optimizer.py` for logging rowwise Adagrad engine.\n\nTest Plan: CI + sandcastle\n\nReviewed By: boryiingsu\n\nDifferential Revision: D33439337\n\nfbshipit-source-id: b158249b8df771c0ec8b642210ede39972929b00", "pr_number": "70880", "files_changed": ["caffe2/python/optimizer.py"], "labels": ["fb-exported", "cla signed", "ciflow/default"]}, "9267fd8d73": {"title": "[WIP] [ATen] Add native_multi_attention_self_attention CPU + GPU implementation (#70649)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/70649\n\nAs described in https://fb.quip.com/oxpiA1uDBjgP\n\nThis implements the first parts of the RFC, and is a rough draft showing the approach. The idea is that for the first cut we can maintain very close (identical I believe in this diff) numerical equivalence to the existing nn.MHA implementation, which is what this diff attempts to do. In subsequent implementations, once we have a working and adopted native self-attention implementation, we could then explore alternative implementations, etc.\n\nThe current implementation is similar to existing dedicated implementations such as LightSeq/FasterTransformer/DeepSpeed, and for MHA on both CPUs and GPUs is between 1.2x and 2x faster depending on the setting. It makes some approximations/restrictions (doesn't handle masking in masked softmax, etc), but these shouldn't materially impact performance.\n\nThis does the first few items:\n\n* add native_multi_head_attention(...) , native_multi_head_attention_backward(..) to native_functions.yaml\n* Implement native_multi_head_attention(..) on GPU, extracting bits and pieces out of LS/DS/FT as appropriate\n* Implement native_multi_head_attention(..) on CPU\n\nThe backward implementation is still WIP, but the idea would be to:\n\n* Hook these up in derivatives.yaml\nImplement native_multi_head_attention_backward(..) on GPU, extracting out bits and pieces out of LS/DS (not FT since it\u2019s inference only)\n* Implement native_multi_head_attention_backward(..) on CPU\n* In torch.nn.functional.multi_head_attention_forward https://github.com/pytorch/pytorch/blob/23321ba7a3b634ee734455aab4a984689802cad0/torch/nn/functional.py#L4953, add some conditionals to check if we are being called in a BERT/ViT-style encoder fashion, and invoke the native function directly.\n\nTest Plan: TODO\n\nReviewed By: mikekgfb\n\nDifferential Revision: D31829981\n\nfbshipit-source-id: c430344d91ba7a5fbee3138e50b3e62efbb33d96", "pr_number": "70649", "files_changed": ["aten/src/ATen/native/attention.cpp", "aten/src/ATen/native/cuda/attention.cu", "aten/src/ATen/native/native_functions.yaml", "tools/build_variables.bzl", "torch/overrides.py"], "labels": ["fb-exported", "cla signed", "ciflow/default"]}, "e1b84e1b6b": {"title": "fix loading of older models that don't have maximize (#71023)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/71023\n\nReviewed By: jbschlosser\n\nDifferential Revision: D33483687\n\nPulled By: albanD\n\nfbshipit-source-id: 2f3c6e97a9579be9ba15eca0756fc1f2c466fbb6", "pr_number": "71023", "files_changed": ["test/test_optim.py", "torch/optim/_multi_tensor/adam.py", "torch/optim/_multi_tensor/adamw.py", "torch/optim/_multi_tensor/sgd.py", "torch/optim/adam.py", "torch/optim/adamw.py"], "labels": ["cla signed", "ciflow/default"]}, "cfc1117591": {"title": "Update sparse.rst to warn about _values() (#71088)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/70357\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/71088\n\nReviewed By: jbschlosser\n\nDifferential Revision: D33511207\n\nPulled By: cpuhrsch\n\nfbshipit-source-id: 9d0c5445842ed96999eb88445cbea7ae284b1a6f", "pr_number": "71088", "files_changed": ["docs/source/sparse.rst"], "labels": ["open source", "cla signed", "ciflow/default"]}, "1b496cf158": {"title": "Fixes doc errors in `Tensor.triu()`, `Tensor.tril()`, `Tensor.ravel()`. (#71057)", "body": "Summary:\nHi, PyTorch Team!\nI am very much interested in starting up my contribution to PyTorch. I made several contributions in NumPy and CuPy, but this is my first PR towards PyTorch. I aim to contribute more in the upcoming future.\n\nThe PR fixes https://github.com/pytorch/pytorch/issues/70972  https://github.com/pytorch/pytorch/issues/70975.\n\n#### Aim of PR\nThe functions like `Tensor.ravel`, `Tensor.tril`, `Tensor.tril_`, `Tensor.triu`, and `Tensor.triu_` had a couple of typos in docs. The PR aims to resolve that.\n\nI'm looking forward to your viewpoints. Thanks!\n\ncc: kshitij12345 vadimkantorov Lezcano TestSomething22\n\ncc brianjo mruberry\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/71057\n\nReviewed By: preeti1205\n\nDifferential Revision: D33502911\n\nPulled By: mruberry\n\nfbshipit-source-id: 8ce0b68a29658a5a0be79bc807dfa7d71653532d", "pr_number": "71057", "files_changed": ["torch/_tensor_docs.py"], "labels": ["module: docs", "open source", "cla signed", "ciflow/default"]}, "cf61738097": {"title": "Drop unused variables; make things const; use some auto (#71107)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/71107\n\nTest Plan: Sandcastle\n\nReviewed By: ngimel\n\nDifferential Revision: D33490773\n\nfbshipit-source-id: 0d259db9c58c9b33aecc560075f6dcfa78883467", "pr_number": "71107", "files_changed": ["aten/src/ATen/Dispatch.h", "caffe2/operators/local_response_normalization_op.cu", "caffe2/operators/lp_pool_op.cu", "caffe2/operators/prelu_op.cu", "caffe2/operators/roi_pool_op.cu", "caffe2/operators/segment_reduction_op_gpu.cu", "caffe2/operators/slice_op.cu", "caffe2/operators/spatial_batch_norm_op_impl.cuh"], "labels": ["fb-exported", "cla signed", "ciflow/default"]}, "ecb6defa36": {"title": "Fixed docs for forward_ad.make_dual (#71159)", "body": "Summary:\nMinor docs change.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/71159\n\nReviewed By: mruberry\n\nDifferential Revision: D33530031\n\nPulled By: albanD\n\nfbshipit-source-id: e0bbe3a29a7de675fa4c9bf90976616f0e093f74", "pr_number": "71159", "files_changed": ["torch/autograd/forward_ad.py"], "labels": ["open source", "cla signed", "ciflow/default"]}, "a3b7dd7b78": {"title": "Enable nested default hooks (#70932)", "body": "Summary:\nWhen default hooks are set, they are pushed onto a stack.\nWhen nesting context-manager, only the inner-most hooks will\nbe applied.\n\nThere is special care needed to update the TLS code. See also https://github.com/pytorch/pytorch/issues/70940 (i.e. do we need to be storing the enabled flag as well?)\n\nFixes https://github.com/pytorch/pytorch/issues/70134\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/70932\n\nReviewed By: mruberry\n\nDifferential Revision: D33530370\n\nPulled By: albanD\n\nfbshipit-source-id: 3197d585d77563f36c175d3949115a0776b309f4", "pr_number": "70932", "files_changed": ["aten/src/ATen/SavedTensorHooks.cpp", "aten/src/ATen/SavedTensorHooks.h", "aten/src/ATen/ThreadLocalState.cpp", "aten/src/ATen/ThreadLocalState.h", "test/test_autograd.py", "torch/_C/_autograd.pyi", "torch/autograd/__init__.py", "torch/autograd/graph.py", "torch/csrc/autograd/init.cpp", "torch/csrc/autograd/python_saved_variable_hooks.cpp", "torch/csrc/autograd/python_saved_variable_hooks.h"], "labels": ["triaged", "open source", "cla signed", "ciflow/default"]}, "786f946098": {"title": "[Profiler] Add glue layer to reduce the use of `#ifdef USE_KINETO` in the profiler code. (#69798)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/69798\n\nOne of the major sources of complexity in `profiler_kineto.cpp` is that kineto may or may not be available. The code (including the types) follows two related but often distict codepaths, and large sections may or may not be `#ifdef`'d out.\n\nOptimizing such code which preserving correctness is quite difficult; at one point I realized that I had broken the non-Kineto case, because moving work into the finalize step runs astray of a very large `#ifdef` around the finalize logic.\n\nIn order to make optimization more tractable, I gathered all of the calls to Kineto APIs and isolated them in the `kineto_shim.h/.cpp` files: the header allows callers to pretend as though Kineto is always available (mostly), and the cpp file hides most of the horrible `#ifdef`s so they don't pollute the main profiler code.\n\nTest Plan: Unit tests.\n\nReviewed By: aaronenyeshi\n\nDifferential Revision: D32690568\n\nfbshipit-source-id: 9a276654ef0ff9d40817c2f88f95071683f150c5", "pr_number": "69798", "files_changed": ["tools/build_variables.bzl", "torch/csrc/autograd/init.cpp", "torch/csrc/autograd/profiler_kineto.cpp", "torch/csrc/autograd/profiler_kineto.h", "torch/csrc/autograd/profiler_utils.cpp", "torch/csrc/profiler/kineto_shim.cpp", "torch/csrc/profiler/kineto_shim.h", "torch/csrc/profiler/util.cpp", "torch/csrc/profiler/util.h"], "labels": ["oncall: distributed", "oncall: jit", "fb-exported", "cla signed", "ciflow/default"]}, "831c129e85": {"title": "fx quant: fix test_fx_acc_tracer::test_quantized_batch_norm2d (#71175)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/71175\n\nD33330022 was landed with a Meta test failure (ghstack clobbered the fix),\nresubmitting the Meta-only part to fix CI.\n\nTest Plan:\n```\nbuck test mode/opt //caffe2/test:test_fx_acc_tracer -- --exact 'caffe2/test:test_fx_acc_tracer - test_quantized_batch_norm2d (fx_acc.test_acc_tracer.AccTracerTest)' --run-disabled\n```\n\nReviewed By: HDCharles\n\nDifferential Revision: D33531994\n\nfbshipit-source-id: 39dc945c54fb9a7205c9d4114ede6b5ab99c5012", "pr_number": "71175", "files_changed": ["test/fx_acc/test_acc_tracer.py"], "labels": ["fb-exported", "cla signed", "ciflow/default"]}, "523d448968": {"title": "Remove deprecated cuDNN convolution ops (#71128)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/71128\n\nTest Plan: Imported from OSS\n\nReviewed By: anjali411\n\nDifferential Revision: D33517677\n\nPulled By: jbschlosser\n\nfbshipit-source-id: 1690fd38a38ee7cf16865209280a9c457c5f70ff", "pr_number": "71128", "files_changed": ["aten/src/ATen/autocast_mode.cpp", "aten/src/ATen/native/cudnn/ConvPlaceholders.cpp", "aten/src/ATen/native/cudnn/ConvShared.cpp", "aten/src/ATen/native/native_functions.yaml", "test/forward_backward_compatibility/check_forward_backward_compatibility.py", "torch/testing/_internal/autocast_test_lists.py"], "labels": ["cla signed", "ciflow/default"]}, "b652887ad7": {"title": "improve documentation of comparison internals (#68977)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/68977\n\nFollow-up to #68722 to address the review comments that were left open before merge.\n\nTest Plan: Imported from OSS\n\nReviewed By: ngimel\n\nDifferential Revision: D33542998\n\nPulled By: mruberry\n\nfbshipit-source-id: 23c567cd328f83ae4df561ac8ee6c40c259408c9", "pr_number": "68977", "files_changed": ["test/test_testing.py", "torch/testing/_comparison.py"], "labels": ["open source", "cla signed", "ciflow/default"]}, "8d05174def": {"title": "make meta tensor data access error message for expressive in assert_close (#68802)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/68802\n\nWithout this patch, the error message of comparing meta tensors looks like this after #68722 was merged:\n\n```python\n>>> t = torch.empty((), device=\"meta\")\n>>> assert_close(t, t)\nNotImplementedError: Could not run 'aten::abs.out' with arguments from the 'Meta' backend. [...]\n[...]\nThe above exception was the direct cause of the following exception:\n[...]\nRuntimeError: Comparing\n\nTensorLikePair(\n    id=(),\n    actual=tensor(..., device='meta', size=()),\n    expected=tensor(..., device='meta', size=()),\n    rtol=1.3e-06,\n    atol=1e-05,\n    equal_nan=False,\n    check_device=True,\n    check_dtype=True,\n    check_layout=True,\n    check_stride=False,\n    check_is_coalesced=True,\n)\n\nresulted in the unexpected exception above. If you are a user and see this message during normal operation please file an issue at https://github.com/pytorch/pytorch/issues. If you are a developer and working on the comparison functions, please except the previous error and raise an expressive `ErrorMeta` instead.\n```\n\nThus, we follow our own advice and turn it into an expected exception until #68592 is resolved:\n\n```python\n>>> t = torch.empty((), device=\"meta\")\n>>> assert_close(t, t)\nValueError: Comparing meta tensors is currently not supported\n```\n\nTest Plan: Imported from OSS\n\nReviewed By: ngimel\n\nDifferential Revision: D33542999\n\nPulled By: mruberry\n\nfbshipit-source-id: 0fe1ddee15b5decdbd4c5dd84f03804ca7eac95b", "pr_number": "68802", "files_changed": ["test/test_testing.py", "torch/testing/_comparison.py"], "labels": ["open source", "cla signed", "module: testing", "module: meta tensors", "ciflow/default"]}, "802dd2b725": {"title": "change sparse COO comparison strategy in assert_close (#68728)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/68728\n\nThis removes the ability for `assert_close` to `.coalesce()` the tensors internally. Additionally, we now also check `.sparse_dim()`. Sparse team: please make sure that is the behavior you want for all sparse COO comparisons in the future. #67796 will temporarily keep BC by always coalescing, but in the future `TestCase.assertEqual` will no longer do that.\n\ncc nikitaved pearu cpuhrsch IvanYashchuk\n\nTest Plan: Imported from OSS\n\nReviewed By: ngimel\n\nDifferential Revision: D33542996\n\nPulled By: mruberry\n\nfbshipit-source-id: a8d2322c6ee1ca424e3efb14ab21787328cf28fc", "pr_number": "68728", "files_changed": ["test/test_testing.py", "torch/testing/_comparison.py", "torch/testing/_deprecated.py"], "labels": ["module: sparse", "open source", "cla signed", "module: testing", "ciflow/default"]}, "b0a10a709f": {"title": "add explanation of quantized comparison strategy in assert_close (#68911)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/68911\n\nCloses #68548.\n\nTest Plan: Imported from OSS\n\nReviewed By: ngimel\n\nDifferential Revision: D33542997\n\nPulled By: mruberry\n\nfbshipit-source-id: 78accf20a83cd72254ae0036dc23f9e5376a4c65", "pr_number": "68911", "files_changed": ["torch/testing/_comparison.py"], "labels": ["oncall: quantization", "open source", "cla signed", "module: testing", "ciflow/default"]}, "49a5b33a74": {"title": "add a equality comparison helper for assert_close internals (#69750)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/69750\n\nTest Plan: Imported from OSS\n\nReviewed By: ngimel\n\nDifferential Revision: D33542993\n\nPulled By: mruberry\n\nfbshipit-source-id: 0de0559c33ec0f1dad205113cb363a652140b62d", "pr_number": "69750", "files_changed": ["torch/testing/_comparison.py"], "labels": ["open source", "cla signed", "module: testing", "ciflow/default"]}, "928ca95ff0": {"title": "fix TensorLikePair origination (#70304)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/70304\n\nWithout this patch `TensorLikePair` will try to instantiate everything although it should only do so for tensor-likes. This is problematic if it is used before a different pair that would be able to handle the inputs but never gets to do so, because `TensorLikePair` bails out before.\n\n```python\nfrom torch.testing._comparison import assert_equal, TensorLikePair, ObjectPair\n\nassert_equal(\"a\", \"a\", pair_types=(TensorLikePair, ObjectPair))\n```\n\n```\nValueError: Constructing a tensor from <class 'str'> failed with\nnew(): invalid data type 'str'.\n```\n\nTest Plan: Imported from OSS\n\nReviewed By: ngimel\n\nDifferential Revision: D33542995\n\nPulled By: mruberry\n\nfbshipit-source-id: 77a5cc0abad44356c3ec64c7ec46e84d166ab2dd", "pr_number": "70304", "files_changed": ["test/test_testing.py", "torch/testing/_comparison.py"], "labels": ["open source", "cla signed", "ciflow/default"]}, "e1aea9b968": {"title": "Add retry to disabled tests file download (#71030)", "body": "Summary:\nHelps with spotty disabling brought up in https://github.com/pytorch/pytorch/issues/70877 and https://github.com/pytorch/pytorch/issues/70875\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/71030\n\nReviewed By: malfet, atalman\n\nDifferential Revision: D33486379\n\nPulled By: janeyx99\n\nfbshipit-source-id: 56c4d56c2bd8be47a51dee19373aac6c9c5d1691", "pr_number": "71030", "files_changed": ["tools/stats/import_test_stats.py"], "labels": ["cla signed", "ciflow/default"]}, "4d28cef03a": {"title": "Added AutocastCPU string (#70013)", "body": "Summary:\nDescription:\n- Added \"AutocastCPU\" string repr into `toString` method\n\nBefore\n```\nstd::cout << c10::DispatchKey::AutocastCPU;\n> UNKNOWN_TENSOR_TYPE_ID\n```\nand now:\n```\nstd::cout << c10::DispatchKey::AutocastCPU;\n> AutocastCPU\n```\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/70013\n\nReviewed By: ejguan\n\nDifferential Revision: D33550777\n\nPulled By: bdhirsh\n\nfbshipit-source-id: b31e15e6d52fc1768af085e428328117d588f283", "pr_number": "70013", "files_changed": ["c10/core/DispatchKey.cpp"], "labels": ["triaged", "open source", "cla signed", "ciflow/default"]}, "479ce1c3a0": {"title": "[PyTorch] Add isUndefined to ExclusivelyOwnedTraits<TensorBase> debug msg (#70638)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/70638\n\nWe are seeing these assertions fire infrequently. Add more information to aid in debugging when they fire.\nghstack-source-id: 146819527\n\nTest Plan: CI\n\nReviewed By: bdhirsh\n\nDifferential Revision: D33412651\n\nfbshipit-source-id: 7e35faf9f4eeaa5f2455a4392e00f62fe692811c", "pr_number": "70638", "files_changed": ["aten/src/ATen/core/TensorBase.h"], "labels": ["cla signed", "ciflow/default"]}, "90ef54f8ea": {"title": "[PyTorch] Remove buggy ExclusivelyOwnedTraits<intrusive_ptr<T>> (#70647)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/70647\n\nIt wasn't checking for the null state and it wasn't used.\nghstack-source-id: 146819525\n\nTest Plan: CI\n\nReviewed By: hlu1\n\nDifferential Revision: D33414728\n\nfbshipit-source-id: 7fcd648577cbfc35320c5c3ca9a19a14bd4d6858", "pr_number": "70647", "files_changed": ["aten/src/ATen/test/ExclusivelyOwned_test.cpp", "c10/util/ExclusivelyOwned.h", "c10/util/intrusive_ptr.h"], "labels": ["cla signed", "ciflow/default"]}, "bfe1abd3b5": {"title": "torch/monitor: add pybind (#69567)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/69567\n\nThis exposes torch.monitor events and stats via pybind11 to the underlying C++ implementation.\n\n* The registration interface is a tad different since it takes a lambda function in Python where as in C++ it's a full class.\n* This has a small amount of changes to the counter interfaces since there's no way to create an initializer list at runtime so they now also take a vector.\n* Only double based stats are provided in Python since it's intended more for high level stats where float imprecision shouldn't be an issue. This can be changed down the line if need arises.\n\n```\nevents = []\n\ndef handler(event):\n    events.append(event)\n\nhandle = register_event_handler(handler)\n\nlog_event(Event(type=\"torch.monitor.TestEvent\", timestamp=datetime.now(), metadata={\"foo\": 1.0}))\n```\n\nD32969391 is now included in this diff.\nThis cleans up the naming for events. type is now name, message is gone, and metadata is renamed data.\n\nTest Plan: buck test //caffe2/test:monitor //caffe2/test/cpp/monitor:monitor\n\nReviewed By: kiukchung\n\nDifferential Revision: D32924141\n\nfbshipit-source-id: 563304c2e3261a4754e40cca39fc64c5a04b43e8", "pr_number": "69567", "files_changed": ["docs/source/index.rst", "docs/source/monitor.rst", "test/cpp/monitor/test_counters.cpp", "test/cpp/monitor/test_events.cpp", "test/test_monitor.py", "tools/build_variables.bzl", "torch/_C/_monitor.pyi", "torch/csrc/Module.cpp", "torch/csrc/monitor/counters.cpp", "torch/csrc/monitor/counters.h", "torch/csrc/monitor/events.h", "torch/csrc/monitor/python_init.cpp", "torch/csrc/monitor/python_init.h", "torch/monitor/__init__.py"], "labels": ["fb-exported", "cla signed", "ci/windows", "ciflow/default", "ciflow/macos"]}, "3c0c5bde0e": {"title": "[cmake] Uncomment binaries (#71157)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/71157\n\nTest Plan: Imported from OSS\n\nReviewed By: ejguan\n\nDifferential Revision: D33528259\n\nPulled By: IvanKobzarev\n\nfbshipit-source-id: b8c216558ca612bedd4c37205f38ed29c2c82b3c", "pr_number": "71157", "files_changed": ["binaries/CMakeLists.txt"], "labels": ["cla signed", "ciflow/default"]}, "67941c8a94": {"title": "Document `torch.cuda.ExternalStream`, `torch.cuda.caching_allocator_alloc` and `torch.cuda.caching_allocator_delete` (#70126)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/67414. Fixes https://github.com/pytorch/pytorch/issues/70117.\n\ncc brianjo mruberry ngimel\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/70126\n\nReviewed By: mruberry\n\nDifferential Revision: D33542910\n\nPulled By: ngimel\n\nfbshipit-source-id: 4b870f4dceca6ee4cc8fba58819f1cb18ac9f857", "pr_number": "70126", "files_changed": ["docs/source/cuda.rst", "test/test_cuda.py", "torch/_tensor.py", "torch/cuda/__init__.py"], "labels": ["module: docs", "module: cuda", "triaged", "open source", "cla signed", "ciflow/default"]}, "f6b804ba9f": {"title": "Fallback to server JIT type for type checking.", "body": "Summary:\nT109800703\nIn runtime fallback to server JIT type if a DynamicType is parsed.\n\nTest Plan: local headset\n\nReviewed By: scramsby\n\nDifferential Revision: D33557763\n\nfbshipit-source-id: f5fe7dabf668de2f55cc26f9ebe8bcbccd570ce3", "pr_number": null, "files_changed": ["aten/src/ATen/core/dynamic_type.cpp"], "labels": []}, "71b274d34d": {"title": "[pytorch] move ATen/CUDAGeneratorImpl.h to ATen/cuda (#71224)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/71224\n\nPull Request resolved: https://github.com/facebookresearch/FBTT-Embedding/pull/19\n\nPull Request resolved: https://github.com/pytorch/FBGEMM/pull/860\n\nThis patch follows up D33414890 (https://github.com/pytorch/pytorch/commit/5cae40c169b3da48d2782947d072d20b7acb15d1).\n\nThis patch removes an alias header \"`ATen/CUDAGeneratorImpl.h`\" since it has been moved to `ATen/cuda/CUDAGeneratorImpl.h`. This change should have already been propagated.\n\nTest Plan: Internal and external CI\n\nReviewed By: jianyuh\n\nDifferential Revision: D33534276\n\nfbshipit-source-id: 368177784ec84f003aad911cf4dd4da4a6e8e3d4", "pr_number": "71224", "files_changed": ["aten/src/ATen/CUDAGeneratorImpl.h"], "labels": ["fb-exported", "cla signed", "ciflow/default"]}, "dabcbb2726": {"title": "Testing for Default Inference for Device Type (#69052)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/69052\n\nTest Plan: Imported from OSS\n\nReviewed By: anjali411\n\nDifferential Revision: D33555888\n\nPulled By: Gamrix\n\nfbshipit-source-id: dbd43ebfc1bea4b17a96bdd378ea730ccf5944b2", "pr_number": "69052", "files_changed": ["test/jit/test_device_analysis.py", "test/test_jit.py"], "labels": ["oncall: jit", "cla signed", "ciflow/default"]}, "e4d522a3cf": {"title": "More informative messages for None types comparisons (#69802)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/69802\n\nTest Plan: Imported from OSS\n\nReviewed By: anjali411\n\nDifferential Revision: D33555886\n\nPulled By: Gamrix\n\nfbshipit-source-id: 3045cbe04de22f05db41a99ad3dda90c5271aa0f", "pr_number": "69802", "files_changed": ["torch/testing/_internal/common_utils.py"], "labels": ["cla signed", "ciflow/default"]}, "5f2b4be3b9": {"title": "[jit] Split DynamicType conformance test into smaller pieces. (#71275)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/71275\n\nCurrently it's taking more than 10 minutes to run the conformance test. Instead we should use parametrized test to shard into test segments so that they can run in parallel.\nghstack-source-id: 146990608\n\nTest Plan:\n```\n[zhxchen17@devbig560.ftw3 /data/users/zhxchen17/fbsource/fbcode] buck test mode/dev-tsan //caffe2/test/cpp/jit:jit -- -r 'LiteInterpreterDynamicTypeTestFixture'\nBuilding... 34.9 sec (99%) 12110/12111 jobs, 0/12111 updated\nTpx test run coordinator for Facebook. See https://fburl.com/tpx for details.\nRunning with tpx session id: ebea52b3-7c7f-46be-9f69-18e2e7b040cc\nTrace available for this run at /tmp/tpx-20220113-113635.717778/trace.log\nRemoteExecution session id: reSessionID-ebea52b3-7c7f-46be-9f69-18e2e7b040cc-tpx\nStarted reporting to test run: https://www.internalfb.com/intern/testinfra/testrun/4222124735827748\n    \u2713 ListingSuccess: caffe2/test/cpp/jit:jit : 431 tests discovered (11.173)\n    \u2713 Pass: caffe2/test/cpp/jit:jit - Conformance/LiteInterpreterDynamicTypeTestFixture.Conformance/0 (51.331)\n    \u2713 Pass: caffe2/test/cpp/jit:jit - Conformance/LiteInterpreterDynamicTypeTestFixture.Conformance/1 (65.614)\n    \u2713 Pass: caffe2/test/cpp/jit:jit - Conformance/LiteInterpreterDynamicTypeTestFixture.Conformance/3 (76.875)\n    \u2713 Pass: caffe2/test/cpp/jit:jit - Conformance/LiteInterpreterDynamicTypeTestFixture.Conformance/5 (77.271)\n    \u2713 Pass: caffe2/test/cpp/jit:jit - Conformance/LiteInterpreterDynamicTypeTestFixture.Conformance/4 (78.871)\n    \u2713 Pass: caffe2/test/cpp/jit:jit - Conformance/LiteInterpreterDynamicTypeTestFixture.Conformance/6 (78.984)\n    \u2713 Pass: caffe2/test/cpp/jit:jit - Conformance/LiteInterpreterDynamicTypeTestFixture.Conformance/7 (84.068)\n    \u2713 Pass: caffe2/test/cpp/jit:jit - Conformance/LiteInterpreterDynamicTypeTestFixture.Conformance/2 (85.198)\n    \u2713 Pass: caffe2/test/cpp/jit:jit - Conformance/LiteInterpreterDynamicTypeTestFixture.Conformance/8 (88.815)\n    \u2713 Pass: caffe2/test/cpp/jit:jit - Conformance/LiteInterpreterDynamicTypeTestFixture.Conformance/9 (90.332)\nSummary\n  Pass: 10\n  ListingSuccess: 1\nIf you need help understanding your runs, please follow the wiki: https://fburl.com/posting_in_tpx_users\nFinished test run: https://www.internalfb.com/intern/testinfra/testrun/4222124735827748\n```\n\nReviewed By: qihqi\n\nDifferential Revision: D33570442\n\nfbshipit-source-id: 5c49e03b0f88068d444c84b4adeaaf45433ce1fa", "pr_number": "71275", "files_changed": ["test/cpp/jit/test_lite_interpreter.cpp"], "labels": ["oncall: jit", "cla signed", "ciflow/default"]}, "675acfc1f4": {"title": "Remove unwanted comma (#71193)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/70611\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/71193\n\nReviewed By: ngimel\n\nDifferential Revision: D33542841\n\nPulled By: mruberry\n\nfbshipit-source-id: 0f2f1218c056aea7ecf86ba4036cfb10df6e8614", "pr_number": "71193", "files_changed": ["test/test_numba_integration.py"], "labels": ["module: numba", "open source", "cla signed", "ciflow/default"]}, "356af8f857": {"title": "Do not use `ssize_t` in `python_arg_parser.[cpp|h]` (#71250)", "body": "Summary:\nUse `Py_ssize_t` when calling Python API\nUse `c10::irange` to automatically infer loop type\n Use `size_t` or `unsigned` for unsigned type\n\n Partially addresses https://github.com/pytorch/pytorch/issues/69948\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/71250\n\nReviewed By: atalman\n\nDifferential Revision: D33569724\n\nPulled By: malfet\n\nfbshipit-source-id: c9eb75be9859d586c00db2f824c68840488a2822", "pr_number": "71250", "files_changed": ["torch/csrc/utils/python_arg_parser.cpp", "torch/csrc/utils/python_arg_parser.h"], "labels": ["cla signed", "ciflow/default"]}, "17bb68618f": {"title": "Copy: Fix CPU transpose path ignoring neg and conj bits (#69026)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/69026\n\nTest Plan: Imported from OSS\n\nReviewed By: ngimel\n\nDifferential Revision: D33064533\n\nPulled By: anjali411\n\nfbshipit-source-id: 98c25586a1707ac2324f69f652ce5a14dd59c0ad", "pr_number": "69026", "files_changed": ["aten/src/ATen/native/Copy.cpp", "test/test_torch.py"], "labels": ["open source", "cla signed", "ciflow/default"]}, "054b90f0d6": {"title": "add channels last support for ChannelShuffle (#50247)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/50247\n\nTest Plan: Imported from OSS\n\nReviewed By: albanD\n\nDifferential Revision: D26007052\n\nPulled By: VitalyFedyunin\n\nfbshipit-source-id: 08f737d64a65791c8002ffd56b79b02cf14d6159", "pr_number": "50247", "files_changed": ["aten/src/ATen/native/ChanelShuffle.cpp", "aten/src/ATen/native/cpu/ChannelShuffleKernel.cpp", "aten/src/ATen/native/cpu/ChannelShuffleKernel.h", "aten/src/ATen/native/cpu/utils.h", "aten/src/ATen/native/native_functions.yaml", "tools/build_variables.bzl", "torch/overrides.py"], "labels": ["open source", "cla signed", "ciflow/default", "intel priority"]}, "a4196a9abf": {"title": "Remove unused `optimizers` variable in test (#70668)", "body": "Summary:\nIn `TestLRScheduler._test()`, an unused variable `optimizers` is created. This PR is a minor refactoring that removes the variable and the loop block that populates the set.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/70668\n\nReviewed By: wenleix\n\nDifferential Revision: D33586236\n\nPulled By: albanD\n\nfbshipit-source-id: cabf870a8221f144df9d3e2f2b564cdc5c255f5a", "pr_number": "70668", "files_changed": ["test/test_optim.py"], "labels": ["triaged", "open source", "cla signed", "ciflow/default"]}, "c7d1501e4d": {"title": "fractional_maxpool3d: port to structured kernel (#70414)", "body": "Summary:\nPort fractional maxpool 3d to structured kernel\n\nFixes https://github.com/pytorch/pytorch/issues/55070\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/70414\n\nReviewed By: zdevito, wenleix\n\nDifferential Revision: D33572110\n\nPulled By: bdhirsh\n\nfbshipit-source-id: 1f89eb511335f51cc7abbb0230e165da8752f9fc", "pr_number": "70414", "files_changed": ["aten/src/ATen/native/FractionalMaxPool3d.cpp", "aten/src/ATen/native/cuda/FractionalMaxPool3d.cu", "aten/src/ATen/native/native_functions.yaml"], "labels": ["triaged", "open source", "cla signed", "ciflow/default"]}, "680d61daab": {"title": "[LT] Remove torch::lazy::convertShapes (#71291)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/71291\n\nThis commit removes torch::lazy::convertShapes since it's no longer used.\nIn addition, it replaces a numel logic within LTCTensorImpl.\n\nTest Plan:\n./build/bin/test_lazy\nCI in lazy_tensor_staging branch\n\nReviewed By: wconstab\n\nDifferential Revision: D33575084\n\nPulled By: alanwaketan\n\nfbshipit-source-id: b104ef39fd552822e1f4069eab2cb942d48423a6", "pr_number": "71291", "files_changed": ["test/cpp/lazy/test_shape.cpp", "torch/csrc/lazy/core/shape.cpp", "torch/csrc/lazy/core/shape.h", "torch/csrc/lazy/core/tensor_impl.cpp"], "labels": ["cla signed", "ciflow/default"]}, "b7222e15b6": {"title": "[fix] max_pool1d: composite compliance (#70900)", "body": "Summary:\nReference: https://github.com/pytorch/pytorch/issues/69991\n\nNot sure if this is a good idea as this increases the number of operators.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/70900\n\nReviewed By: wenleix\n\nDifferential Revision: D33585964\n\nPulled By: zou3519\n\nfbshipit-source-id: 11bfa2e00ee123a6d36f7d4cccdf0c1a3e664d8c", "pr_number": "70900", "files_changed": ["aten/src/ATen/native/MaxPooling.cpp", "aten/src/ATen/native/native_functions.yaml", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["triaged", "open source", "cla signed", "ciflow/default"]}, "c43e0286a9": {"title": "[PyTorch][Lazy] Make hashing null optionals cheap (#71290)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/71290\n\nThe existing code called an out-of-line hash function on a constant. This is just going to get the same random-looking 64-bit integer every time, so I just changed the constant to an integer I generated with `hex(random.randint(0x1000000000000000, 0xFFFFFFFFFFFFFFFF))` to get the same effect but without the runtime hashing.\nghstack-source-id: 146991945\n\nTest Plan: CI\n\nReviewed By: wconstab\n\nDifferential Revision: D33574676\n\nfbshipit-source-id: d6ce1e1cc0db67dfede148b7e3173508ec311ea8", "pr_number": "71290", "files_changed": ["torch/csrc/lazy/core/hash.h"], "labels": ["cla signed", "ciflow/default"]}, "3ed27a96ed": {"title": "[BE] Refactor repetitions into TorchVersion._cmp_wrapper` (#71344)", "body": "Summary:\nFirst step towards https://github.com/pytorch/pytorch/issues/71280\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/71344\n\nReviewed By: b0noI\n\nDifferential Revision: D33594463\n\nPulled By: malfet\n\nfbshipit-source-id: 0295f0d9f0342f05a390b2bd4aa0a5958c76579b", "pr_number": "71344", "files_changed": ["torch/torch_version.py"], "labels": ["cla signed", "ciflow/default"]}, "cf47338191": {"title": "[Caffe2][warnings] Suppress -Wimplicit-int-float-conversion in TypeSafeSignMath.h for clang (#71369)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/71369\n\nSuppress `-Wimplicit-int-float-conversion` in `TypeSafeSignMath.h` when building with clang\n\nTest Plan: CI check\n\nReviewed By: r-barnes\n\nDifferential Revision: D33612983\n\nfbshipit-source-id: cff1239bc252d4a2f54a50a2bbcd48aeb8bf31ca", "pr_number": "71369", "files_changed": ["c10/util/TypeSafeSignMath.h"], "labels": ["fb-exported", "cla signed", "ciflow/default"]}, "7b9fff90d2": {"title": "empty_generic: Remove redundant device argument (#70612)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/70612\n\nThe device information is embedded in the `DataPtr` returned from the\nallocator, so this argument is completely ignored.\n\nTest Plan: Imported from OSS\n\nReviewed By: mruberry\n\nDifferential Revision: D33623681\n\nPulled By: ngimel\n\nfbshipit-source-id: bea64707bb17d46debb0ed7c1175493df56fee77", "pr_number": "70612", "files_changed": ["aten/src/ATen/Utils.cpp", "aten/src/ATen/Utils.h", "aten/src/ATen/native/MetaTensor.cpp", "aten/src/ATen/native/TensorFactories.cpp"], "labels": ["open source", "cla signed", "ciflow/default"]}, "d17f340a2e": {"title": "The Cacherator (#71350)", "body": "Summary:\nThis PR adds a persistent filesystem cache for jitted kernels. The cache is disabled on Windows because it relies on POSIX headers.\n\nThe cache writes, by default, to `~/.cache/torch/kernels`, but the location can be controlled by setting the `PYTORCH_KERNEL_CACHE_PATH`. A separate environment variable, `USE_PYTORCH_KERNEL_CACHE`, will disable all caching logic when set to zero.\n\nThe use of a persistent fileystem cache dramatically lowers the \"first call time\" for an operator AFTER its has been compiled, because it skips (most of) the jit compilation process. On systems where we're compiling only to ptx that ptx still has to be just-in-time compiled by the driver API, so an additional latency of around 10 milliseconds is expected at first call time. On systems which compile to SASS the additional first call time latency is about one millisecond. This compares with times of 150 milliseconds+ for just-in-time kernel compilation.\n\nFiles in the cache use a mostly human readable string that includes an SHA1 hash of the CUDA C string used to generate them. Note that this is not an SHA1 hash of the file's contents, because the contents are the compiled ptx or SASS. No verification is done when the file is loaded to ensure the kernel is what's expected, but it's far more likely you'll be struck by a meteor than observe two file names conflict. Using SHA1 hashes to generate unique ids this way is a common practice (GitHub does it, too).\n\nThis cache design could be reused by other fusion systems and should allow us to jiterate more operations without fear of regressing the \"incremental development\" scenario where users are tweaking or extending programs slightly, rerunning then, and then repeating that process again and again. Without a cache each run of the program would have to recompile every jitted kernel, but with this cache we expect a negligible impact to the user experience.\n\ncc kshitij12345, xwang233\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/71350\n\nReviewed By: ngimel\n\nDifferential Revision: D33626671\n\nPulled By: mruberry\n\nfbshipit-source-id: d55df53416fbe46348623846f699f9b998e6c318", "pr_number": "71350", "files_changed": ["aten/src/ATen/jit_macros.h", "aten/src/ATen/native/cuda/CUDALoops.cuh", "aten/src/ATen/native/cuda/Loops.cuh", "aten/src/ATen/native/cuda/Math.cuh", "aten/src/ATen/native/cuda/jit_utils.cpp", "aten/src/ATen/native/cuda/jit_utils.cu", "c10/util/hash.h", "caffe2/CMakeLists.txt"], "labels": ["cla signed", "ciflow/slow", "ciflow/default", "ciflow/all"]}, "8d0e354191": {"title": "fix CAFFE2_BUILD_MAIN_LIB to the correct C10_BUILD_MAIN_LIB (#70848)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/70848\n\nThis is the C10 library, it that's the main lib we are building\nhere. While here, use `local_defines` instead of `copts` for this\ndefinition. Both `copts` and `local_defines` only apply to the\ncompilation units in the library, and not transitively.\nghstack-source-id: 146998039\n\nTest Plan: We are relying on CI to verify this doesn't cause any problems.\n\nReviewed By: malfet\n\nDifferential Revision: D33429420\n\nfbshipit-source-id: b3fc84c0588bd43346e3f9f77e851d293bde9428", "pr_number": "70848", "files_changed": ["c10/BUILD.bazel"], "labels": ["cla signed", "ciflow/default"]}, "ffdc6b4994": {"title": "extract //c10/macros to its own package (#70849)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/70849\n\nghstack-source-id: 146960563\n\nTest Plan: Bazel CI tests will protect this.\n\nReviewed By: malfet\n\nDifferential Revision: D33297235\n\nfbshipit-source-id: 6504a977e82ad2f2232a74233b96cdea8bf94a20", "pr_number": "70849", "files_changed": ["c10/BUILD.bazel", "c10/macros/BUILD.bazel"], "labels": ["cla signed", "ciflow/default"]}, "d665097cad": {"title": "allow Bazel to build without glog and gflags (#70850)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/70850\n\nWe support both, so we want to ensure both continue to work.\nghstack-source-id: 146960552\n\nTest Plan: Tested manually. A subsequent diff adds this test configuration to CI.\n\nReviewed By: malfet\n\nDifferential Revision: D33297464\n\nfbshipit-source-id: 70e1431d0907d480c576239af93ef57036d5e4d7", "pr_number": "70850", "files_changed": ["c10/BUILD.bazel", "c10/macros/BUILD.bazel", "c10/macros/cmake_configure_file.bzl"], "labels": ["cla signed", "ciflow/default"]}, "2bb6a4f437": {"title": "Generate aten_interned_strings.h automatically (#69407)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/69407\n\nThis generates aten_interned_strings.h from `native_functions.yaml`\nwhich is more like how it was originally done. The items deleted from\n`interned_strings.h` are duplicates that need to be removed in order\nfor the code to compile, some of the remaining items may still be out\nof date but it is fairly benign even if that's the case.\n\nTest Plan: Imported from OSS\n\nReviewed By: zou3519\n\nDifferential Revision: D32923636\n\nPulled By: albanD\n\nfbshipit-source-id: a0fd6b3714e70454c5f4ea9b19da5e047d2a4687", "pr_number": "69407", "files_changed": ["BUILD.bazel", "aten/src/ATen/core/aten_interned_strings.h", "aten/src/ATen/core/interned_strings.h", "aten/src/ATen/native/native_functions.yaml", "aten/src/ATen/templates/aten_interned_strings.h", "tools/codegen/gen.py", "tools/jit/templates/aten_interned_strings.h"], "labels": ["oncall: jit", "open source", "cla signed", "ciflow/default"]}, "efd274bbcb": {"title": "Fix for windows builds with python 3.10 , getting rid of ssize_t (ssize_t is not a C++ defined type) (#71390)", "body": "Summary:\nFix for windows builds with python 3.10 , getting rid of ssize_t\n\nHere is the completed bin build : https://app.circleci.com/pipelines/github/pytorch/pytorch/441527/workflows/144edb79-b398-4d70-92fe-b63158c1b439/jobs/16954881\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/71390\n\nReviewed By: samdow\n\nDifferential Revision: D33637686\n\nPulled By: atalman\n\nfbshipit-source-id: fcdfca672dc20385a3d2339c20e69bd2d1717e88\n(cherry picked from commit 2ac58b0dc13f152bea180dd3d64b7c36fe0ba755)", "pr_number": "71390", "files_changed": ["torch/csrc/cuda/Module.cpp", "torch/csrc/serialization.cpp", "torch/csrc/utils/python_arg_parser.cpp"], "labels": ["ci/binaries", "cla signed", "ciflow/default"]}, "a986154950": {"title": "Lazy import `packaging` in `torch_version` (#71345)", "body": "Summary:\nAs it is a pretty big package and to be used during normal\ncourse of PyTorch initialization\n\nFixes https://github.com/pytorch/pytorch/issues/71280\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/71345\n\nReviewed By: seemethere\n\nDifferential Revision: D33594547\n\nPulled By: malfet\n\nfbshipit-source-id: e0abea82dbdc29914512b610692701140d3e68a2\n(cherry picked from commit 1ff7f65cc1ad499a71457368894ca14bed069749)", "pr_number": "71345", "files_changed": ["torch/torch_version.py"], "labels": ["cla signed", "ciflow/default"]}, "322f13d914": {"title": "[Profiler] Fix memory profile type from recent refactor (#71417)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/71417\n\nI accidentally changed CPU_INSTANT_EVENT to CPU_OP, which broke TensorBoard.\n\nTest Plan: Make memory profiling unit test check this case.\n\nReviewed By: aaronenyeshi\n\nDifferential Revision: D33637286\n\nfbshipit-source-id: c95945f6b85cd4168820bd4d2a9203274a0a5bd6\n(cherry picked from commit b1e258672af4b83d824b8c8eb565af0ffdfa895b)", "pr_number": "71417", "files_changed": ["test/test_profiler.py", "torch/csrc/profiler/kineto_shim.cpp"], "labels": ["fb-exported", "cla signed", "ciflow/default"]}, "4fd1992a60": {"title": "[Docs][BE] DDP doc fix (#71363)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/71363\n\nLooks like DDP example is currently broken as per\nhttps://discuss.pytorch.org/t/official-ddp-example-is-broken/141493. Fix the\nissue by setting the correct env variable.\nghstack-source-id: 147080377\n\nTest Plan: CI\n\nReviewed By: mrshenli\n\nDifferential Revision: D33607250\n\nfbshipit-source-id: e0e7d03cc365c186253b959c4c5405a5e3609218\n(cherry picked from commit 32472884ec04d0e9b348b07d645dd1027389f8e8)", "pr_number": "71363", "files_changed": ["docs/source/notes/ddp.rst"], "labels": ["cla signed", "ciflow/default"]}, "87215ed526": {"title": "empty_strided: Factor out generic implementation (#70614)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/70614\n\nThis creates an `empty_strided_generic` function which, similar to\n`empty_generic`, is a device-independent tensor constructor. This also\nadds `at::detail::empty_strided_cpu` to complement\n`at::detail::empty_cpu`.\n\nTest Plan: Imported from OSS\n\nReviewed By: samdow\n\nDifferential Revision: D33623679\n\nPulled By: ngimel\n\nfbshipit-source-id: 85994e88d664870bf425f398dfcdfc467885c694\n(cherry picked from commit 2ff2a89df5752cfad667463aa3c3bffe8479ec9a)", "pr_number": "70614", "files_changed": ["aten/src/ATen/EmptyTensor.cpp", "aten/src/ATen/EmptyTensor.h", "aten/src/ATen/TensorUtils.cpp", "aten/src/ATen/TensorUtils.h", "aten/src/ATen/native/MetaTensor.cpp", "aten/src/ATen/native/TensorFactories.cpp", "aten/src/ATen/native/cuda/TensorFactories.cu"], "labels": ["open source", "cla signed", "ciflow/default"]}, "125bdb6d51": {"title": "empty_meta: Add functions that don't depend on Tensor (#70615)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/70615\n\nThis adds `at::detail::empty_meta` and\n`at::detail::empty_strided_meta` to complement the cpu API.\n\nTest Plan: Imported from OSS\n\nReviewed By: samdow\n\nDifferential Revision: D33623678\n\nPulled By: ngimel\n\nfbshipit-source-id: 59e003116361fb547ec2c633bbc15a7973e21d0e\n(cherry picked from commit b4f5836fa106418755381abedf327125bde744ef)", "pr_number": "70615", "files_changed": ["aten/src/ATen/EmptyTensor.cpp", "aten/src/ATen/EmptyTensor.h", "aten/src/ATen/native/MetaTensor.cpp"], "labels": ["open source", "cla signed", "ciflow/default"]}, "9b9b878c89": {"title": "Fixes jiterator cache macro include + updates CUDA note with cache variables (#71452)", "body": "Summary:\nPer title.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/71452\n\nReviewed By: ngimel\n\nDifferential Revision: D33646495\n\nPulled By: mruberry\n\nfbshipit-source-id: bbf627e6d7a724a83a3ea2ae9c0f50430f8d578e\n(cherry picked from commit d1e72b144aad9607ce53c477b7edfdce17cfd1c0)", "pr_number": "71452", "files_changed": ["aten/src/ATen/jit_macros.h", "aten/src/ATen/native/cuda/jit_utils.cpp", "docs/source/notes/cuda.rst"], "labels": ["cla signed", "ciflow/default"]}, "677fab6d1d": {"title": "Support broadcast_to on sparse COO tensors (#71073)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/71073\n\ncc nikitaved pearu cpuhrsch\n\nTest Plan: Imported from OSS\n\nReviewed By: mikaylagawarecki\n\nDifferential Revision: D33645744\n\nPulled By: cpuhrsch\n\nfbshipit-source-id: 4775c9636c4e868022a8c1bbfec93e351d1cf885\n(cherry picked from commit 640f21e09a935a1231b99ddd6472b03158bdc283)", "pr_number": "71073", "files_changed": ["aten/src/ATen/FunctionalInverses.cpp", "aten/src/ATen/native/TensorShape.cpp", "aten/src/ATen/native/native_functions.yaml", "test/test_sparse.py"], "labels": ["module: sparse", "open source", "cla signed", "ciflow/default", "ciflow/all"]}, "9515213070": {"title": "[Operator Versioning] Remove version compare as they are decoupled now (#71461)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/71461\n\nAfter operator versioning work, the version in model file is used for operator versioning, while bytecode_version is used for bytecode versioning (for bytecode schema). They are two seperate things now and this comparison is not needed.\nghstack-source-id: 147209286\n\nTest Plan: CI\n\nReviewed By: iseeyuan, tugsbayasgalan\n\nDifferential Revision: D33648592\n\nfbshipit-source-id: beaa136a728f88435176a00c07b2d521210f107f\n(cherry picked from commit e90e650e1a5134473117eda802d679171e035082)", "pr_number": "71461", "files_changed": ["caffe2/serialize/versions.h"], "labels": ["cla signed", "ciflow/default"]}, "a0ada2d22b": {"title": "Back out \"[pytorch][PR] Performance and memory improvements to batched torch.linalg.solve\" (#71421)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/71421\n\nOriginal commit changeset: 7a0dd443cd0e\n\nOriginal Phabricator Diff: D33028236 (https://github.com/pytorch/pytorch/commit/410e91adee19121f2cae76274d735fe441d8f5c7)\n\nTest Plan: PyTorch OSS CI\n\nReviewed By: ngimel\n\nDifferential Revision: D33637628\n\nfbshipit-source-id: 1e81485be202b2f9d6a1ff315279cc099754c2dc\n(cherry picked from commit c2d730bfeb2a9e4a3af1442b8d1fe5bf28a95f2b)", "pr_number": "71421", "files_changed": ["aten/src/ATen/native/BatchLinearAlgebra.cpp", "aten/src/ATen/native/BatchLinearAlgebraKernel.cpp", "aten/src/ATen/native/cuda/BatchLinearAlgebra.cpp", "aten/src/ATen/native/cuda/BatchLinearAlgebraLib.cpp"], "labels": ["fb-exported", "cla signed", "ciflow/default"]}, "908fd3d78b": {"title": "[fix] composite compliance: quantile and nanquantile (#70894)", "body": "Summary:\nReference https://github.com/pytorch/pytorch/issues/69991\n\nRefactored such that only `out` variant copies the result into `out` otherwise we just return the result of the composite functions as is.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/70894\n\nReviewed By: samdow\n\nDifferential Revision: D33641742\n\nPulled By: zou3519\n\nfbshipit-source-id: 671be13b31a7fff3afc0b7976706a5ecfc51ccac\n(cherry picked from commit e7d5ac9af319be327adc16d2d7048139a4b2ddd3)", "pr_number": "70894", "files_changed": ["aten/src/ATen/native/Sorting.cpp", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["triaged", "open source", "cla signed", "ciflow/default"]}, "75aaa9f92b": {"title": "Remove simd qualifier for pragma omp loop in upsample_nearest_op.h (#71462)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/71462\n\nFixes\n```\n      6 aienv/aienv_ig_reels_base:caffe2/modules/detectron/upsample_nearest_op.h:65:1: error: loop not vectorized: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering [-Werror,-Wpass-failed=transform-warning]\n      6 deep_entity_classification/si_dec_gnn:caffe2/modules/detectron/upsample_nearest_op.h:65:1: error: loop not vectorized: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering [-Werror,-Wpass-failed=transform-warning]\n      6 feed_recommendation_infra/multifeed_execution_graph_service_nosan:caffe2/modules/detectron/upsample_nearest_op.h:65:1: error: loop not vectorized: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering [-Werror,-Wpass-failed=transform-warning]\n     12 mobile_cv/mobile-vision_experimental:caffe2/modules/detectron/upsample_nearest_op.h:65:1: error: loop not vectorized: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering [-Werror,-Wpass-failed=transform-warning]\n     30 mobile_cv/mobile-vision_xraymobilev2_detection_caffe2:caffe2/modules/detectron/upsample_nearest_op.h:65:1: error: loop not vectorized: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering [-Werror,-Wpass-failed=transform-warning]\n     42 aienv/aienv:caffe2/modules/detectron/upsample_nearest_op.h:65:1: error: loop not vectorized: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering [-Werror,-Wpass-failed=transform-warning]\n    128 feed_recommendation_infra/multifeed_recagg_dev:caffe2/modules/detectron/upsample_nearest_op.h:65:1: error: loop not vectorized: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering [-Werror,-Wpass-failed=transform-warning]\n    136 fluent2/fblearner_flow_projects_fluent2_nosan:caffe2/modules/detectron/upsample_nearest_op.h:65:1: error: loop not vectorized: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering [-Werror,-Wpass-failed=transform-warning]\n   1338 f6/f6_nosan:caffe2/modules/detectron/upsample_nearest_op.h:65:1: error: loop not vectorized: the optimizer was unable to perform the requested transformation; the transformation might be disabled or specified as part of an unsupported transformation ordering [-Werror,-Wpass-failed=transform-warning]\n```\n\nTest Plan: Sandcastle\n\nReviewed By: luciang\n\nDifferential Revision: D33641869\n\nfbshipit-source-id: 8424849cfac5cb0109272dec2086863067bbde66\n(cherry picked from commit d18429905c7661486ed8ec0cdcdd7d94b9c62762)", "pr_number": "71462", "files_changed": ["modules/detectron/upsample_nearest_op.h"], "labels": ["fb-exported", "cla signed", "ciflow/default"]}, "15e7d18124": {"title": "[jit][edge] Create convinience wrapper for dynamic type construcytors. (#71457)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/71457\n\nToday DynamicType is hard to be created because we have separare APIs for different types. In this diff we introduce an easier API to create types like the following:\n```\n#include <ATen/core/type_factory.h>\n\nauto type = dynT<ListType>(dynT<TensorType>()); // etc...\n```\nghstack-source-id: 147211236\n\nTest Plan: CI\n\nReviewed By: iseeyuan\n\nDifferential Revision: D33647746\n\nfbshipit-source-id: c850cf31ae781244eac805906a2fc110ef065a70\n(cherry picked from commit 8cfd51d75f010ca6f7f98b7e8ef807ead4d5f8f3)", "pr_number": "71457", "files_changed": ["aten/src/ATen/core/dynamic_type.cpp", "aten/src/ATen/core/dynamic_type.h", "aten/src/ATen/core/type_factory.h"], "labels": ["cla signed", "ciflow/default"]}, "661d10aab4": {"title": "use c10/macros/cmake_macros.h in fbcode build (#70851)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/70851\n\nThis is a step towards OSS/fbcode convergence since OSS uses this file\nin both CMake and Bazel.\nghstack-source-id: 147170896\n\nTest Plan: Relying on the extensive CI internal tests for this.\n\nReviewed By: malfet\n\nDifferential Revision: D33299102\n\nfbshipit-source-id: c650dd4755f8d696d5fce81c583d5c73782e3990\n(cherry picked from commit 741ca140c82f728e3b349d703a7de239e5bbf13c)", "pr_number": "70851", "files_changed": ["c10/cuda/CUDAMacros.h"], "labels": ["cla signed", "ciflow/default"]}, "78e1f9db34": {"title": "port //c10/macros to common build structure (#70852)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/70852\n\nThis is the first change that uses a common build file, build.bzl, to\nhold most of the build logic.\nghstack-source-id: 147170895\n\nTest Plan: Relying on internal and external CI.\n\nReviewed By: malfet\n\nDifferential Revision: D33299331\n\nfbshipit-source-id: a66afffba6deec76b758dfb39bdf61d747b5bd99\n(cherry picked from commit d9163c56f55cfc97c20f5a6d505474d5b8839201)", "pr_number": "70852", "files_changed": ["c10/macros/BUILD.bazel", "c10/macros/build.bzl", "tools/bazel.bzl"], "labels": ["cla signed", "ciflow/default"]}, "f45e217c01": {"title": "Consolidate the overloads of TensorImpl::shallow_copy_and_detach (#68953)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/68953\n\nThis PR consolidates the almost identical lvalue and rvalue implementations of shallow_copy_and_detach into a single templated function.\nghstack-source-id: 147238376\n\nTest Plan: Run existing unit tests.\n\nReviewed By: fduwjj\n\nDifferential Revision: D32679741\n\nfbshipit-source-id: 89a870335d2e09ffd005c943733a787d20d352f9\n(cherry picked from commit 750344c8600e05d4ab593956257c8191919eeef8)", "pr_number": "68953", "files_changed": ["c10/core/TensorImpl.cpp", "c10/core/TensorImpl.h"], "labels": ["cla signed", "ciflow/default"]}, "811af25963": {"title": "Fix trivial typo at the doc of `torch.lobpcg` (#71464)", "body": "Summary:\nI think `symmetric positive defined generalized eigenvalue problem` should be changed to `symmetric positive definite generalized eigenvalue problem`.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/71464\n\nReviewed By: ejguan\n\nDifferential Revision: D33660670\n\nPulled By: H-Huang\n\nfbshipit-source-id: 85dc830ed56a98d8a38bd2843f575f6ce08498cf\n(cherry picked from commit dbbef542c04a8dd93514ac7783f4546e5da7ca58)", "pr_number": "71464", "files_changed": ["torch/_lobpcg.py"], "labels": ["open source", "cla signed", "ciflow/default"]}, "565f78f571": {"title": "[Pytorch] Speed up LayerNorm 4-5% (#71423)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/71423\n\nReplacing this math with a load seems to improve perf.\nghstack-source-id: 147171800\n\nTest Plan: ptvsc2_predictor_bench runs on model from mikeiovine courtesy of mikeiovine\n\nReviewed By: mikeiovine, xiaomengy\n\nDifferential Revision: D33552176\n\nfbshipit-source-id: f21a4cd66c13b9fcb7bcf48f356bdc85e94c4216\n(cherry picked from commit 0354fcb9889e7345321fe4dc9e30495a67709a4d)", "pr_number": "71423", "files_changed": ["aten/src/ATen/native/cpu/moments_utils.h"], "labels": ["cla signed", "ciflow/default"]}, "c59942ac73": {"title": "[PyTorch] Fix a bunch of structured kernel refcounting (#71140)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/71140\n\nStructured kernels need to use the borrowing variants of the build APIs to TensorIterator. (I am working on a debug check for this, but it is currently too strict, and relaxing it does not catch these bugs.)\nghstack-source-id: 147191022\n\nTest Plan: CI\n\nReviewed By: bdhirsh\n\nDifferential Revision: D33520003\n\nfbshipit-source-id: 3b0ff9036acdb78ae6fc7489ed0ed487d5ff080f\n(cherry picked from commit 80ef4e14e33718a9ad5aaefc218bb773e3b15a5c)", "pr_number": "71140", "files_changed": ["aten/src/ATen/TensorIterator.cpp", "aten/src/ATen/TensorIterator.h", "aten/src/ATen/native/BinaryOps.cpp", "aten/src/ATen/native/Pow.cpp", "aten/src/ATen/native/TensorCompare.cpp", "aten/src/ATen/native/UnaryOps.cpp", "aten/src/ATen/native/mkldnn/Matmul.cpp", "aten/src/ATen/native/mkldnn/Matmul.h"], "labels": ["cla signed", "ciflow/default"]}, "21b697b646": {"title": "add flatbuffer_loader and flatbuffer_serializer as BUCK target (#71463)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/71463\n\ntitle\n\nTest Plan: unittest\n\nReviewed By: zhxchen17\n\nDifferential Revision: D33651339\n\nfbshipit-source-id: 4bf325a40e263a441fd86bce560645ad0c1ebb23\n(cherry picked from commit 4cb02e62a68f338e3388ad09276ced9b8f4cdcb1)", "pr_number": "71463", "files_changed": ["test/cpp/jit/test_flatbuffer.cpp"], "labels": ["oncall: jit", "fb-exported", "cla signed", "ciflow/default"]}, "17d2a5167e": {"title": "Refactor convolution_backward's CudaDepthwise2d case (#71489)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/71489\n\nDeleted unnecessary .contiguous() calls in convolution_backward. The\nCudaDepthwise2d case always hits conv_depthwise2d_backward_cuda_out,\nwhich makes the grad_output / self contiguous.\n\nChanged conv_depthwise2d_backward_cuda_out to change `self_` (aka the\nimage input to convolution) to be contiguous only when we're computing\nthe grad_weight. This is because when we are computing the grad_input,\nwe only need the values from the grad_output and the weight.\n\nTest Plan: - pytest test/test_nn.py -v -k \"conv\"\n\nReviewed By: jbschlosser\n\nDifferential Revision: D33664697\n\nPulled By: zou3519\n\nfbshipit-source-id: 7a755fa8a076809c5490422d69fdf7ed80c8e29a\n(cherry picked from commit 862ae63bab74113b3607b1bbc0a82f27992550fe)", "pr_number": "71489", "files_changed": ["aten/src/ATen/native/Convolution.cpp", "aten/src/ATen/native/cuda/DepthwiseConv2d.cu"], "labels": ["cla signed", "ciflow/default"]}, "06f14c2d63": {"title": "Refactor convolution_backward's CudaDepthwise3d case (#71490)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/71490\n\nDeleted unnecessary .contiguous() calls in convolution_backward. The\nCudaDepthwise3d case always hits _depthwise_3d_backward_cuda_out,\nwhich will make arguments contiguous as necessary.\n\nChanged _depthwise_3d_backward_cuda_out\n- to make the input contiguous only when we're computing grad_weight\n- to make the weight contiguous only when we're computing grad_input\n\nTest Plan: - pytest test/test_nn.py -v -k \"conv\"\n\nReviewed By: jbschlosser\n\nDifferential Revision: D33664696\n\nPulled By: zou3519\n\nfbshipit-source-id: d01d4f213e21ef4778de089a158933737b191cdf\n(cherry picked from commit c6eb977c94a07f9812567a43b125b453eb5c5051)", "pr_number": "71490", "files_changed": ["aten/src/ATen/native/Convolution.cpp", "aten/src/ATen/native/cuda/DepthwiseConv3d.cu"], "labels": ["cla signed", "ciflow/default"]}, "640bfa7e6f": {"title": "Refactor convolution_backward's cudnn cases (#71491)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/71491\n\nChanged the Cudnn and CudnnTranspose cases to only make the input\ncontiguous when it is needed for the grad_weight computation.\n\nReading the implementation of cudnn_convolution_transpose_backward and\ncudnn_convolution_backward give me confidence that `input` isn't used\nfor the grad_weight computation. However, the memory format logic is so\nconvoluted that I'm 100$ sure this correct. All the tests though\nand on request I can directly pass `backend_memory_format` to\n{cudnn_convolution_backward, cudnn_convolution_transpose_backward}.\n\nTest Plan: - pytest test/test_nn.py -v -k \"conv\"\n\nReviewed By: jbschlosser\n\nDifferential Revision: D33664694\n\nPulled By: zou3519\n\nfbshipit-source-id: 9f4929686fe34f7aaf5331bfa49e98022b9d6c08\n(cherry picked from commit 9e2ba0daca88139f7941bcb56bbc23825585d7a2)", "pr_number": "71491", "files_changed": ["aten/src/ATen/native/Convolution.cpp"], "labels": ["cla signed", "ciflow/default"]}, "89c844db9b": {"title": "[torch.distributions] Implement positive-semidefinite constraint (#71375)", "body": "Summary:\nWhile implementing https://github.com/pytorch/pytorch/issues/70275, I thought that it will be useful if there is a `torch.distributions.constraints` to check the positive-semidefiniteness of matrix random variables.\nThis PR implements it with `torch.linalg.eigvalsh`, different from `torch.distributions.constraints.positive_definite` implemented with `torch.linalg.cholesky_ex`.\nCurrently, `torch.linalg.cholesky_ex` returns only the order of the leading minor that is not positive-definite in symmetric matrices and we can't check positive semi-definiteness by the mechanism.\ncc neerajprad\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/71375\n\nReviewed By: H-Huang\n\nDifferential Revision: D33663990\n\nPulled By: neerajprad\n\nfbshipit-source-id: 02cefbb595a1da5e54a239d4f17b33c619416518\n(cherry picked from commit 43eaea5bd861714f234e9efc1a7fb571631298f4)", "pr_number": "71375", "files_changed": ["test/distributions/test_constraints.py", "torch/distributions/constraints.py"], "labels": ["triaged", "open source", "cla signed", "ciflow/default"]}, "4868907cf3": {"title": "[binaries] fix dump_operator_name binary (#71246)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/71246\n\nTest Plan: Imported from OSS\n\nReviewed By: malfet\n\nDifferential Revision: D33555962\n\nPulled By: IvanKobzarev\n\nfbshipit-source-id: 2b386e52fa8e76c877fec5b6b15d99f7d280801f\n(cherry picked from commit f6d60fdff68964f77aa46ca2c51327cb66566194)", "pr_number": "71246", "files_changed": ["binaries/dump_operator_names.cc"], "labels": ["cla signed", "ciflow/default"]}, "67385918ab": {"title": "move header inclusion (#71307)", "body": "Summary:\nA header is used only in the .cc file and it is included by the public header. This causes errors when I try to include the public header.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/71307\n\nReviewed By: zou3519\n\nDifferential Revision: D33650700\n\nPulled By: ngimel\n\nfbshipit-source-id: d08dd335208da3aaafe333522d9525976c513151\n(cherry picked from commit 94805495a0d30c54f22b0609db177d7ac3e26093)", "pr_number": "71307", "files_changed": ["aten/src/ATen/native/cuda/jit_utils.cpp", "aten/src/ATen/native/cuda/jit_utils.h"], "labels": ["triaged", "open source", "cla signed", "ciflow/default"]}, "2eb4b05b94": {"title": "torch/monitor: make tests more robust on windows (#71581)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/71581\n\nFixes https://github.com/pytorch/pytorch/issues/71553\n\nTest Plan:\nadd ciflow/windows to CI\n\n  buck test //caffe2/test:monitor -- --stress-runs 100 test_interval_sec\n\nI don't have a windows machine so need to rely on CI to test\n\nReviewed By: edward-io\n\nDifferential Revision: D33691540\n\nfbshipit-source-id: 69f28f1dfa7243e4eeda642f9bef6d5d168381d2\n(cherry picked from commit 5d24dc7c2f5e8e0f48fdd602b1eaa3a8e6929715)", "pr_number": "71581", "files_changed": ["test/test_monitor.py"], "labels": ["fb-exported", "cla signed", "ciflow/default", "ciflow/win"]}, "db2fc82a54": {"title": "Generalize IValue's aliased hash handling for opaque tensors (#70371)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/70371\n\nThis PR generalizes the aliased hash handling for opaque tensors beyond MKL-DNN.\nghstack-source-id: 147328304\n\nTest Plan: Run existing tests.\n\nReviewed By: zou3519\n\nDifferential Revision: D33301787\n\nfbshipit-source-id: db741ac347e933f8d65b029cd5be5f01804a960e\n(cherry picked from commit aa8822a31a1002ea0c2440041e5e8cb862666535)", "pr_number": "70371", "files_changed": ["aten/src/ATen/core/ivalue.h"], "labels": ["cla signed", "ciflow/default"]}, "dc0a8a6587": {"title": "Improve storage assertion of Tensor's enforce_invariants (#70380)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/70380\n\nA small change in `Tensor`'s `enforce_invariants` that addresses tensor types that don't use the regular storage mechanism.\nghstack-source-id: 147328303\n\nTest Plan: Existing unit tests.\n\nReviewed By: zou3519\n\nDifferential Revision: D33304602\n\nfbshipit-source-id: c8cc41ed38a3eec147f40fe1029fd059748c87b5\n(cherry picked from commit da4e87f20b0ec8bb1003e519ed39ba32de62a89d)", "pr_number": "70380", "files_changed": ["aten/src/ATen/core/Tensor.cpp"], "labels": ["cla signed", "ciflow/default"]}, "c92ff47afd": {"title": "Use == operator to test type equivalance in pytorch_jni_common.cpp (#71508)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/71508\n\n\"==\" is the more universal way to test type equalities, and also ::get() doesn't incur any refcount overhead now, so we can swtich to == instead of relying on type kinds.\nghstack-source-id: 147353057\n\nTest Plan:\nCI\nbuck test xplat/caffe2/android:pytorch_jni_common_test\n\nDifferential Revision: D33672433\n\nfbshipit-source-id: 5973fd40de48b8017b5c3ebaa55bcf5b4b391aa3\n(cherry picked from commit db84a4b566d1f2f17cda8785e11bc11739e6f50c)", "pr_number": "71508", "files_changed": ["android/pytorch_android/src/androidTest/cpp/pytorch_jni_common_test.cpp", "android/pytorch_android/src/main/cpp/pytorch_jni_common.cpp", "android/pytorch_android/src/main/cpp/pytorch_jni_common.h"], "labels": ["cla signed", "ciflow/default"]}, "76fd3cfd38": {"title": "fix python version error (#71021)", "body": "Summary:\nif python3 is the one running the tests but there exists a \"python\" installed as python2.7 the test will fail with a syntax issue\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/71021\n\nReviewed By: zou3519\n\nDifferential Revision: D33667073\n\nPulled By: albanD\n\nfbshipit-source-id: 8e489b491439be1740fc32ca5c7cdceb2145771e\n(cherry picked from commit 5adfece429fcfe6ace778dd67f060d04a3d54699)", "pr_number": "71021", "files_changed": ["test/test_functional_autograd_benchmark.py"], "labels": ["triaged", "open source", "cla signed", "ciflow/default"]}, "7ee0712642": {"title": "Fix torch.{unique, unique_consecutive} out of bound (#71540)", "body": "Summary:\nThis PR ensures that the input iterator is always in front of the output\niterator. Thus, we won't have a out of bound issue since the input\niterator will meet the end before output iterator meets.\n\nFixes https://github.com/pytorch/pytorch/issues/71089\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/71540\n\nReviewed By: mruberry\n\nDifferential Revision: D33688123\n\nPulled By: ngimel\n\nfbshipit-source-id: f57718931d09a0fbea76ac1bd6cc8c7150af0978\n(cherry picked from commit dc6e0e219a9e9b9ccea9ff5406458b56f556b2e4)", "pr_number": "71540", "files_changed": ["aten/src/ATen/native/Unique.cpp", "test/test_sort_and_select.py"], "labels": ["triaged", "open source", "cla signed", "ciflow/default"]}, "abe361754e": {"title": "[fix] isin : non-contiguous input on cpu (#70659)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/67432\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/70659\n\nReviewed By: anjali411\n\nDifferential Revision: D33532405\n\nPulled By: mruberry\n\nfbshipit-source-id: fc3cbe3893e4c87b6a11f1bbaad66f49d6eda215\n(cherry picked from commit 65aef8031c30f45f10b66d8c904845033e67bd50)", "pr_number": "70659", "files_changed": ["aten/src/ATen/native/cpu/TensorCompareKernel.cpp", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["triaged", "open source", "cla signed", "ciflow/default"]}, "26c123efbd": {"title": "empty_cuda: Add functions that don't depend on Tensor (#70616)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/70616\n\nThis adds `at::detail::empty_cuda` and\n`at::detail::empty_strided_cuda` to complement the cpu and meta APIs.\n\nThese functions also include the `lazyInitCUDA` and `DeviceGuard` that\nare missing from the `at::native::empty_cuda` interface and so is\nsafer to use.\n\nTest Plan: Imported from OSS\n\nReviewed By: mruberry\n\nDifferential Revision: D33623677\n\nPulled By: ngimel\n\nfbshipit-source-id: 1c38e84881083df8e025250388f0c8f392974b92\n(cherry picked from commit 4bc48c7008acf2394db7d02dee69dd7a8cfb87b8)", "pr_number": "70616", "files_changed": ["aten/src/ATen/cuda/EmptyTensor.cpp", "aten/src/ATen/cuda/EmptyTensor.h", "aten/src/ATen/native/cuda/TensorFactories.cu"], "labels": ["open source", "cla signed", "ciflow/default"]}, "401e755354": {"title": "Fix hsplit vsplit dsplit crash when section is 0 (#69342)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/69270 #68970\n\nDim modulo by zero when split_size is 0.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/69342\n\nReviewed By: ngimel\n\nDifferential Revision: D33692683\n\nPulled By: mruberry\n\nfbshipit-source-id: aab82e5617c23c265b7dd3a8bac2dd245aaef5b4\n(cherry picked from commit bcbf3b4c4d587394bc7586f32310f637fc9e3de7)", "pr_number": "69342", "files_changed": ["aten/src/ATen/native/TensorShape.cpp", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["triaged", "open source", "cla signed", "ciflow/default"]}, "1a917e637c": {"title": "Bump dlpack.h to latest version (#65047)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/64995\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/65047\n\nReviewed By: VitalyFedyunin\n\nDifferential Revision: D32468916\n\nPulled By: mruberry\n\nfbshipit-source-id: 3e0a17a3a264a77956ea7b795bd472c6fc79566c\n(cherry picked from commit bd480b9892b9fa8a3a46fd0d7babeaf5d649a8b6)", "pr_number": "65047", "files_changed": ["aten/src/ATen/DLConvertor.cpp", "aten/src/ATen/DLConvertor.h", "aten/src/ATen/dlpack.h", "aten/src/ATen/test/cuda_dlconvertor_test.cpp", "caffe2/python/dlpack.h", "caffe2/python/pybind_state_dlpack.cc", "caffe2/python/pybind_state_dlpack.h"], "labels": ["open source", "Merged", "cla signed", "ciflow/default"]}, "b09d6224e2": {"title": "Register{Schema,BackendSelect}.cpp: cleanup header includes (#70021)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/70021\n\n`RegisterSchema.cpp` only uses strings to register operator schemas,\nso doesn't need to include any operator headers at all (except\nindirectly through `torch/library.h`).\n\n`RegisterBackendSelect.cpp` only needs the dispatcher API.\n\nTest Plan: Imported from OSS\n\nReviewed By: jbschlosser\n\nDifferential Revision: D33160028\n\nPulled By: albanD\n\nfbshipit-source-id: 68fb5cb8775077b6f174428a1fcced2a7836b714\n(cherry picked from commit 35774ad7ac6ebbb6d17552ca9eb76fd3c06dcf43)", "pr_number": "70021", "files_changed": ["aten/src/ATen/templates/RegisterBackendSelect.cpp", "aten/src/ATen/templates/RegisterSchema.cpp"], "labels": ["open source", "cla signed", "ciflow/default"]}, "71a41323bb": {"title": "BackendSelect: Use at::_ops API and per-operator headers (#69840)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/69840\n\nTest Plan: Imported from OSS\n\nReviewed By: jbschlosser\n\nDifferential Revision: D33160027\n\nPulled By: albanD\n\nfbshipit-source-id: 0e492ec8bab73da90afd9df70f48c17a8206a768\n(cherry picked from commit 133ec77e9f970fa042ce6fd3fd85c888334f8086)", "pr_number": "69840", "files_changed": ["aten/src/ATen/templates/RegisterBackendSelect.cpp", "tools/codegen/gen.py"], "labels": ["open source", "cla signed", "ciflow/default"]}, "84fe4279db": {"title": "Structured Kernels: Use at::detail::empty functions (#70617)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/70617\n\nThis reduces the divergence between the code generated for\n`create_out` different devices, and means the `TensorOptions` don't\nneed to be unpacked.\n\nTest Plan: Imported from OSS\n\nReviewed By: mruberry\n\nDifferential Revision: D33623680\n\nPulled By: ngimel\n\nfbshipit-source-id: 54f36774a8530be99c26a54270d4d95f3e38d684\n(cherry picked from commit b22ba92e27e638f96a290835b71ad162411fa535)", "pr_number": "70617", "files_changed": ["tools/codegen/dest/register_dispatch_key.py"], "labels": ["open source", "cla signed", "ciflow/default"]}, "26d54b4076": {"title": "monitor: add docstrings to pybind interface (#71481)", "body": "Summary:\nThis adds argument names and docstrings so the docs are a lot more understandable.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/71481\n\nTest Plan:\ndocs/tests CI should suffice\n\n![Screenshot 2022-01-19 at 16-35-10 torch monitor \u2014 PyTorch master documentation](https://user-images.githubusercontent.com/909104/150240882-e69cfa17-e2be-4569-8ced-71979a89b369.png)\n\nReviewed By: edward-io\n\nDifferential Revision: D33661255\n\nPulled By: d4l3k\n\nfbshipit-source-id: 686835dfe331b92a51f4409ec37f8ee6211e49d3\n(cherry picked from commit 0a6accda1bec839bbc9387d80caa51194e81d828)", "pr_number": "71481", "files_changed": ["docs/source/monitor.rst", "test/test_monitor.py", "torch/csrc/monitor/python_init.cpp"], "labels": ["cla signed", "ciflow/default"]}, "47cf0dbf8b": {"title": "Prefer at::detail::empty_cuda to the native function (#70618)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/70618\n\n`at::native::empty_cuda` is called directly in some places to avoid\nthe extra dispatch, however it's features like device guards and a\n`TensorOptions` overload.\n\nTest Plan: Imported from OSS\n\nReviewed By: mruberry\n\nDifferential Revision: D33623676\n\nPulled By: ngimel\n\nfbshipit-source-id: 3ac56c4f8acc90281323195a34fc0a1ef8148fbe\n(cherry picked from commit 4aaf8b29d0de927ec9ced9f8749a96b2be9c4a89)", "pr_number": "70618", "files_changed": ["aten/src/ATen/native/cuda/MultinomialKernel.cu", "aten/src/ATen/native/cuda/Nonzero.cu", "aten/src/ATen/native/cudnn/ConvShared.cpp", "aten/src/ATen/native/miopen/Conv_miopen.cpp"], "labels": ["open source", "cla signed", "ciflow/default"]}, "f269f990f2": {"title": "[jiterator] polygamma (#71162)", "body": "Summary:\nReference: https://github.com/pytorch/pytorch/issues/69463\n\nTODO:\n* [x] Add note regarding how to capture value and it's limitations.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/71162\n\nReviewed By: ngimel\n\nDifferential Revision: D33697346\n\nPulled By: mruberry\n\nfbshipit-source-id: 0308a6c12cf4b488ab26bdae14291c1f6dbba9ab\n(cherry picked from commit 0d3f8c52a17b63079c0d09c965d76cc27cd69146)", "pr_number": "71162", "files_changed": ["aten/src/ATen/native/Math.h", "aten/src/ATen/native/cpu/UnaryOpsKernel.cpp", "aten/src/ATen/native/cuda/CUDALoops.cuh", "aten/src/ATen/native/cuda/Loops.cuh", "aten/src/ATen/native/cuda/Math.cuh", "aten/src/ATen/native/cuda/UnaryGammaKernels.cu", "aten/src/ATen/native/cuda/jit_utils.cpp", "aten/src/ATen/native/cuda/jit_utils.h"], "labels": ["triaged", "open source", "cla signed", "ciflow/default"]}, "e0d829a266": {"title": "Kill the test_torch.py mixin and creates test_scatter_gather_ops (#71691)", "body": "Summary:\nPer title.\n\nAlso annotates test_torch.py with additional cleanup tasks and adds empty sample inputs to elementwise unary and binary OpInfos.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/71691\n\nReviewed By: ngimel\n\nDifferential Revision: D33735126\n\nPulled By: mruberry\n\nfbshipit-source-id: 8cc097a7581a8b620540c95b2a5889c1165ecf23\n(cherry picked from commit 5c6a245a3f9ba7c064fc77c8cd4045f903e73cfd)", "pr_number": "71691", "files_changed": ["test/test_cuda.py", "test/test_scatter_gather_ops.py", "test/test_tensor_creation_ops.py", "test/test_torch.py", "torch/testing/_internal/common_methods_invocations.py", "torch/testing/_internal/common_utils.py"], "labels": ["module: scatter & gather ops", "cla signed", "ciflow/default"]}, "f75e92a936": {"title": "Fix for retracing documentation which would break for n-ary operators (#71599)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/68195\n\nUpdated fx.rst documentation and followed the instructions in [contributing.md](https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md#writing-documentation) to generate html. Faced errors which looked very similar to https://github.com/pytorch/pytorch/issues/32703 but gathered from the thread that a non-0 exit is OK for documentation building and these are warnings not affecting the html generation (at least for root rst folder). The HTML output is plain without any styling, please confirm this is intentional.\n\nScreenshot of generated html:\n<img width=\"1438\" alt=\"Screen Shot 2022-01-20 at 4 31 24 PM\" src=\"https://user-images.githubusercontent.com/9580531/150439448-1a626d74-68ba-4f94-91f2-a6942959b049.png\">\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/71599\n\nReviewed By: jamesr66a\n\nDifferential Revision: D33719546\n\nPulled By: zephirefaith\n\nfbshipit-source-id: cc9b8ddb13cfdb9f14ebff54cf0d894a8b842aa1\n(cherry picked from commit 170db5d7be005e90980c41449b6a9a4c23b3a91f)", "pr_number": "71599", "files_changed": ["docs/source/fx.rst"], "labels": ["cla signed", "ciflow/default"]}, "8ba1ee6aa7": {"title": "[tensorexpr][easy] add missing comma to test_jit_fuser_te.py (#71642)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/71642\n\nMissing comma was causing string concatenation in a list of strings\n\nTest Plan: Imported from OSS\n\nReviewed By: ZolotukhinM\n\nDifferential Revision: D33713185\n\nPulled By: davidberard98\n\nfbshipit-source-id: a2458629d78202713a5bb2f8c720ff9b81939c31\n(cherry picked from commit b077598f1d41948ebe05e2d644ba2dd37446b900)", "pr_number": "71642", "files_changed": ["test/test_jit_fuser_te.py"], "labels": ["cla signed", "ciflow/default"]}, "506d41d659": {"title": "Improve disable name match (#71499)", "body": "Summary:\nAllows disabling issues to disable all parametrized tests with dtypes.\n\nTested locally with:\n1. .pytorch-disabled-tests.json as\n```\n{\"test_bitwise_ops (__main__.TestBinaryUfuncs)\": [\"https://github.com/pytorch/pytorch/issues/99999\", [\"mac\"]]}\n```\nand running `python test_binary_ufuncs.py --import-disabled-tests -k test_bitwise_ops` yields all tests skipped.\n\n2. .pytorch-disabled-tests.json as\n```\n{\"test_bitwise_ops_cpu_int16 (__main__.TestBinaryUfuncsCPU)\": [\"https://github.com/pytorch/pytorch/issues/99999\", [\"mac\"]]}\n```\nand running `python test_binary_ufuncs.py --import-disabled-tests -k test_bitwise_ops` yields only `test_bitwise_ops_cpu_int16` skipped.\n\nNOTE: this only works with dtype parametrization, not all prefixes, e.g., disabling `test_async_script` would NOT disable `test_async_script_capture`. This is the most intuitive behavior, I believe, but I can be convinced otherwise.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/71499\n\nReviewed By: mruberry\n\nDifferential Revision: D33742723\n\nPulled By: janeyx99\n\nfbshipit-source-id: 98a84f9e80402978fa8d22e0f018e6c6c4339a72\n(cherry picked from commit 3f778919caebd3f5cae13963b4824088543e2311)", "pr_number": "71499", "files_changed": ["torch/testing/_internal/common_utils.py"], "labels": ["cla signed", "ciflow/default"]}, "35e7ac3fa1": {"title": "Fix bug in singleCheckErrors (#71706)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/71706\n\nThis fixes a bug in singleCheckErrors introduced by #69437 (thanks\nLezcano for the catch). Checking existence of a substring in a larger\nstring is done with (name.find(text) != name.npos) but we omitted the\nsecond half of the check.\n\nTest Plan: - Code reading; I guess there are no tests for this in CI\n\nReviewed By: mikaylagawarecki\n\nDifferential Revision: D33742822\n\nPulled By: zou3519\n\nfbshipit-source-id: a12017bb12b941282704bd2110e8632f02c24b04\n(cherry picked from commit afb5a04a44232671961d554139e5e19ee711fcab)", "pr_number": "71706", "files_changed": ["aten/src/ATen/native/LinearAlgebraUtils.h"], "labels": ["cla signed", "ciflow/default"]}, "b82c4a890d": {"title": "Fix aten's native's folder docs. (#71395)", "body": "Summary:\nFixes typo's in `aten/src/ATen/native/README.md`. The following were the fixes:\n- Update string type to `c10::string_view` instead of `std::string`.\n- Update the link `torch/_python_dispatcher.py`, which was broken.\n\n**Link to docs:** https://github.com/pytorch/pytorch/tree/master/aten/src/ATen/native/README.md\n\nThanks!\n\ncc: mruberry kshitij12345\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/71395\n\nReviewed By: mikaylagawarecki\n\nDifferential Revision: D33743229\n\nPulled By: mruberry\n\nfbshipit-source-id: 9deebffede20bf68dfc8e45088c8ab2dffb7564c\n(cherry picked from commit 8bedb2cb60aa62b189f6341cf2d92fe46e9f3f7a)", "pr_number": "71395", "files_changed": ["aten/src/ATen/native/README.md"], "labels": ["triaged", "open source", "cla signed", "ciflow/default"]}, "1295d2699f": {"title": "don't include Loops.cuh from Reduce.cuh (#71730)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/71730\n\nReviewed By: mruberry\n\nDifferential Revision: D33754606\n\nPulled By: ngimel\n\nfbshipit-source-id: ddebd147fdfdd66fa723ca7c4341c3d4648b5182\n(cherry picked from commit a8754002dcc2a3e279eb76cd2d91b9440be5dbe3)", "pr_number": "71730", "files_changed": ["aten/src/ATen/native/cuda/CUDALoops.cuh", "aten/src/ATen/native/cuda/Loops.cuh", "aten/src/ATen/native/cuda/ROCmLoops.cuh", "aten/src/ATen/native/cuda/Reduce.cuh", "aten/src/ATen/native/cuda/thread_constants.h"], "labels": ["cla signed", "ciflow/default"]}, "1cc3291716": {"title": "Fix custom function when non tensor argument precedes tensor argument (#71530)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/71530\n\nTest Plan: Imported from OSS\n\nReviewed By: albanD\n\nDifferential Revision: D33695397\n\nPulled By: soulitzer\n\nfbshipit-source-id: 49ccd062f73ccf69c47aca2552fde182d582be2a\n(cherry picked from commit 68d502a01332701f80588735bb174df715fb3bcf)", "pr_number": "71530", "files_changed": ["test/test_autograd.py", "torch/csrc/autograd/python_function.cpp"], "labels": ["cla signed", "ciflow/default"]}, "09aeadf4ab": {"title": "Fix custom function forward AD internal assert (#71531)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/71531\n\nBased on the comment above the original internal assert, this is the desired check.\n1. Don't error, and automatically make jvp return a view for that tensor output (this is easier than I originally thought: https://github.com/pytorch/pytorch/pull/71531#discussion_r789211877)\n2. Error (currently doing)\n\nTest Plan: Imported from OSS\n\nReviewed By: albanD\n\nDifferential Revision: D33695399\n\nPulled By: soulitzer\n\nfbshipit-source-id: dba49890a55ad1dd59ed5c41faa96bf7cfc9e562\n(cherry picked from commit fdb0f266f51e939e122676ab378f4cacba4295aa)", "pr_number": "71531", "files_changed": ["test/test_autograd.py", "torch/csrc/autograd/custom_function.cpp"], "labels": ["cla signed", "ciflow/default"]}, "7a0c97195f": {"title": "Add save_for_forward to custom function (#71569)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/71569\n\nNot sure if this is the right API\n\nTest Plan: Imported from OSS\n\nReviewed By: albanD\n\nDifferential Revision: D33695395\n\nPulled By: soulitzer\n\nfbshipit-source-id: 652b5758f15d901f98ff0da94e977030c7f3415b\n(cherry picked from commit 9421a6846ad35cebbb84bd052769527505092a0c)", "pr_number": "71569", "files_changed": ["test/test_autograd.py", "torch/autograd/function.py", "torch/csrc/autograd/custom_function.cpp", "torch/csrc/autograd/python_function.cpp", "torch/csrc/autograd/python_function.h"], "labels": ["cla signed", "ciflow/default"]}, "e33cd8f382": {"title": "Drop unused variables (#71685)", "body": "Summary:\nThis fixes a number of unused variable warnings that appear when compiling with LLVM-12 on platform010. Fixes are made by removing the variable when possible or by using `/* */` comments to unname the variable when a shared interface is used or eliminating the variable entirely would require extensive changes or risk modifying a public API.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/71685\n\nTest Plan: Sandcastle\n\nReviewed By: luciang, meyering\n\nDifferential Revision: D33728264\n\nfbshipit-source-id: 49286ad7f5271ca1cb48dc70039097305285c948\n(cherry picked from commit a2306cddd67b5f2d83d7c2829aea7cb3d1ce767e)", "pr_number": "71685", "files_changed": ["aten/src/ATen/native/DilatedMaxPool2d.cpp", "aten/src/ATen/native/Pool.h", "aten/src/ATen/native/cuda/DilatedMaxPool2d.cu", "aten/src/ATen/native/cuda/EmbeddingBag.cu", "aten/src/ATen/native/cuda/NLLLoss2d.cu", "aten/src/ATen/native/cuda/UpSampleBicubic2d.cu", "aten/src/ATen/native/cuda/UpSampleBilinear2d.cu", "aten/src/ATen/native/cuda/UpSampleLinear1d.cu", "aten/src/ATen/native/cuda/UpSampleTrilinear3d.cu", "aten/src/ATen/native/cuda/attention.cu", "aten/src/ATen/native/cuda/group_norm_kernel.cu", "aten/src/ATen/native/cuda/layer_norm_kernel.cu"], "labels": ["fb-exported", "cla signed", "ciflow/default"]}, "1df4eca6d7": {"title": "[Operator Versioning][Test] Automate model generating process (#70629)", "body": "Summary:\nThis change is to automate the process to generate the old models for testing upgraders. Developer will\n1. Add a module in `caffe2/test/jit/fixtures_srcs/fixtures_src.py`\n2. Register the module in `caffe2/test/jit/fixtures_srcs/generate_models.py`\n3. Run `python test/jit/fixtures_src/generate_models.py`\n\nThe model will be dumped to `test/jit/fixtures`.\n\nThis script also ensure:\n1. The output model operator version is as expected\n2. The output model will include the changed operator\n\nExample log:\n```\n(base) chenlai@chenlai-mp pytorch % python3 /Users/chenlai/pytorch/test/jit/fixtures_src/generate_models.py\nTestVersionedDivTensorExampleV4() aten::div.Tensor\nINFO:__main__:Processing TestVersionedDivTensorExampleV4\nINFO:__main__:Generating model test_versioned_div_tensor_example_v4 and it's save to /Users/chenlai/pytorch/test/jit/fixtures/test_versioned_div_tensor_example_v4.ptl\n```\nThe second time to run\n```\n(base) chenlai@chenlai-mp pytorch % python3 /Users/chenlai/pytorch/test/jit/fixtures_src/generate_models.py\nTestVersionedDivTensorExampleV4() aten::div.Tensor\nINFO:__main__:Processing TestVersionedDivTensorExampleV4\nINFO:__main__:Model test_versioned_div_tensor_example_v4 already exists, skipping\n```\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/70629\n\nghstack-source-id: 147585826\n\nTest Plan:\nOSS\n```\npython3 /Users/chenlai/pytorch/test/jit/fixtures_src/generate_models.py\n```\nInternal:\n```\nbuck run mode/opt //caffe2/torch/fb/mobile/upgrader_codegen:upgrader_test_models_gen\n```\n\nReviewed By: iseeyuan, tugsbayasgalan\n\nDifferential Revision: D33410841\n\nfbshipit-source-id: 28e2b851a30f12a74e4ac8a539d76e83bbc4fb3a\n(cherry picked from commit 6614f1bdf360b69bcf9eb4bca30707e5bd0e8a2b)", "pr_number": "70629", "files_changed": ["test/jit/fixtures/test_versioned_div_tensor_example_v4.ptl", "test/jit/fixtures_srcs/__init__.py", "test/jit/fixtures_srcs/fixtures_src.py", "test/jit/fixtures_srcs/generate_models.py"], "labels": ["oncall: jit", "cla signed", "ciflow/default"]}, "d32b7d9585": {"title": "Logic to auto-categorize commits (#64929)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/64929\n\nAuto categorized 63% of the commits for PyTorch 1.10 release (2.2k out of 3.4k commits)\n\nTest Plan: Imported from OSS\n\nReviewed By: malfet\n\nDifferential Revision: D33768760\n\nPulled By: anjali411\n\nfbshipit-source-id: 0655090af83e923f8c26fa1ce9f190edc542b97e\n(cherry picked from commit 2fe30f77b83cbcfcb8fc09f728c8853600e8f303)", "pr_number": "64929", "files_changed": ["scripts/release_notes/commitlist.py", "scripts/release_notes/common.py"], "labels": ["better-engineering", "cla signed", "ciflow/default"]}, "211deb0364": {"title": "Fix CI quick-checks (#71773)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/71773\n\nTest Plan: Imported from OSS\n\nReviewed By: pbelevich\n\nDifferential Revision: D33770042\n\nPulled By: anjali411\n\nfbshipit-source-id: 9dd3f8c8592663d385ab0cd4376aaa4b9c7d9ec2\n(cherry picked from commit 739c8885c78b3e39c0b5814f1bece0e3bbb6521b)", "pr_number": "71773", "files_changed": ["scripts/release_notes/commitlist.py"], "labels": ["cla signed", "ciflow/default"]}, "9a2b43085d": {"title": "Improve docs for `from_dlpack` and `to_dlpack` (#70437)", "body": "Summary:\nThis moves the warning to the legacy function where it belongs, improves the phrasing, and adds examples.\n\nThere may be more to do to make `from_dlpack` more discoverable as a follow-up, because in multiple issues/PR we discovered people wanted new things (e.g., a memoryview-like object, or `__array_interface__` support) that `from_dlpack` already provides.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/70437\n\nReviewed By: albanD\n\nDifferential Revision: D33760552\n\nPulled By: mruberry\n\nfbshipit-source-id: e8a61fa99d42331cc4bf3adfe494cab13ca6d499\n(cherry picked from commit 880ad9665956078958af93132a4c6ae820bbaac9)", "pr_number": "70437", "files_changed": ["torch/utils/dlpack.py"], "labels": ["triaged", "open source", "cla signed", "ciflow/default", "module: dlpack"]}, "07ca1fc88b": {"title": "remove hasPrimaryContext workaround on ROCm (#71146)", "body": "Summary:\nAs issue https://github.com/pytorch/pytorch/issues/59750 is fixed, this PR is to remove the workaround implemented for it on ROCm.\n\nEnabled hasPrimaryContext() related PyTorch unit tests on ROCm.\n\ncc: amathews-amd, jithunnair-amd\n\ncc jeffdaily sunway513 jithunnair-amd ROCmSupport KyleCZH\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/71146\n\nReviewed By: anjali411\n\nDifferential Revision: D33754615\n\nPulled By: albanD\n\nfbshipit-source-id: b3a5c65a20c6d52d5f2ffc9e6f9628c819329b5d\n(cherry picked from commit cfdd12166cfd1365de0ebe5a75ce40ac7fde15cc)", "pr_number": "71146", "files_changed": ["test/test_cuda_primary_ctx.py", "torch/csrc/autograd/engine.cpp"], "labels": ["module: rocm", "triaged", "open source", "cla signed", "ciflow/default"]}, "33403f4848": {"title": "edge_order check in torch.gradient only applies to dim argument (#67926)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/67919\n\nThe compatibility check on `edge_order` in `pre_check_gradient` now looks only at dim argument if it is present, otherwise it checks all dimensions.\n\nPreviously, it would check all dimensions regardless of the dim argument and throw unnecessary errors.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/67926\n\nReviewed By: albanD\n\nDifferential Revision: D33760621\n\nPulled By: mruberry\n\nfbshipit-source-id: d490cd8610c68ff3787e670fc947de3cbf2db062\n(cherry picked from commit 45bc56de9e287f715186378682e22bc6ac7a6ccc)", "pr_number": "67926", "files_changed": ["aten/src/ATen/native/ReduceOps.cpp", "test/test_torch.py"], "labels": ["triaged", "open source", "cla signed", "ciflow/default"]}, "332d67b065": {"title": "Add hascuSOLVER flag to Context (#69825)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/69825\n\nAs per title.\n\ncc ngimel jianyuh nikitaved pearu mruberry walterddr IvanYashchuk xwang233 Lezcano\n\nTest Plan: Imported from OSS\n\nReviewed By: mikaylagawarecki, ngimel\n\nDifferential Revision: D33751986\n\nPulled By: mruberry\n\nfbshipit-source-id: 8625c7246d627b5c3680d92d4e8afdd7efc7dd69\n(cherry picked from commit 7ca16beb28bc541b65cd07271f40c889c30e3b85)", "pr_number": "69825", "files_changed": ["aten/src/ATen/Context.h", "aten/src/ATen/cuda/detail/CUDAHooks.cpp", "aten/src/ATen/cuda/detail/CUDAHooks.h", "aten/src/ATen/detail/CUDAHooksInterface.h"], "labels": ["module: cuda", "open source", "module: linear algebra", "cla signed", "ciflow/default"]}, "03f1f0cfe4": {"title": "Check the availability of MAGMA / cuSOLVER when setting the Linalg backend. (#69826)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/69826\n\nThis simplifies the logic needed to handle the defaultBackend flag in linalg functions.\n\ncc ngimel jianyuh nikitaved pearu mruberry walterddr IvanYashchuk xwang233 Lezcano\n\nTest Plan: Imported from OSS\n\nReviewed By: mikaylagawarecki, ngimel\n\nDifferential Revision: D33751984\n\nPulled By: mruberry\n\nfbshipit-source-id: 6963820be38d4f2d82ebb5196dfcccf034ad6784\n(cherry picked from commit 49c81220160062a05bc10a25d487a1f14a2959cd)", "pr_number": "69826", "files_changed": ["aten/src/ATen/Context.cpp"], "labels": ["module: cuda", "open source", "module: linear algebra", "cla signed", "ciflow/default"]}, "de8d0203e9": {"title": "Allow torch.Tensor.real on real-valued tensors (#71718)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/71718\n\nTest Plan: Imported from OSS\n\nReviewed By: albanD\n\nDifferential Revision: D33770668\n\nPulled By: anjali411\n\nfbshipit-source-id: bad21ebe72220b9017a0b8efa71eaeab84bd9e9f\n(cherry picked from commit aa0a922757277ac7b3ad4d633648a89c385ccc0d)", "pr_number": "71718", "files_changed": ["aten/src/ATen/native/UnaryOps.cpp", "test/jit/test_builtins.py", "test/test_autograd.py", "test/test_view_ops.py", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["cla signed", "ciflow/default"]}, "dba42056d8": {"title": "Release GIL in Tensor indexing functions (#71728)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/71728\n\nFixes gh-68739\n\nFor simple indexing this adds a `gil_scoped_release` before calling\n`set_item`. For tuple indexing, the slicing operation is done with the\nGIL because otherwise it would have to re-aquire the GIL for each\nelement in the tuple. However, the GIL is released for the final\n`copy_to` operation which is where the actual kernels are called.\n\nTest Plan: Imported from OSS\n\nReviewed By: anjali411\n\nDifferential Revision: D33770047\n\nPulled By: albanD\n\nfbshipit-source-id: 67304a65e2cbf3b3ba9843687d9c63926d29298f\n(cherry picked from commit d0a85046b7a497df8f377ff43f1667982ede7f2a)", "pr_number": "71728", "files_changed": ["torch/csrc/autograd/python_variable_indexing.cpp"], "labels": ["open source", "cla signed", "ciflow/default"]}, "f3ebf06e98": {"title": "Release GIL when assigning to real or imag components (#71747)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/71747\n\nThe getter is trivial as it's just creating a view tensor, but the\nsetter is actually copying data so does call into kernel code.\n\nTest Plan: Imported from OSS\n\nReviewed By: anjali411\n\nDifferential Revision: D33770046\n\nPulled By: albanD\n\nfbshipit-source-id: f0a70acaef790ae1e5b2f68ac4ce046e850c9624\n(cherry picked from commit 36a0109400b256b32a185fcd05f21f302197c081)", "pr_number": "71747", "files_changed": ["torch/csrc/autograd/python_variable.cpp"], "labels": ["open source", "cla signed", "ciflow/default"]}, "965b9f483e": {"title": "[cuDNN] Add a new optimized cuDNN RNN algorithm for small RNN hidden_size (#62143)", "body": "Summary:\nThis PR enables a new cuDNN RNN/LSTM algorithm `CUDNN_RNN_ALGO_PERSIST_STATIC_SMALL_H` when the hidden_size is small. Operator benchmark observes 10x performance improvement in some shapes.\n\n- [X] forward https://github.com/xwang233/code-snippet/tree/master/cudnn-rnn-bench-62143/forward\n- [X] backward https://github.com/xwang233/code-snippet/tree/master/cudnn-rnn-bench-62143/backward\n- [X] end-to-end model: benchmark looks good\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/62143\n\nReviewed By: anjali411\n\nDifferential Revision: D33771442\n\nPulled By: ngimel\n\nfbshipit-source-id: 0640abc6b90ebd2428c3182ce03bf0b9c30a2ec9\n(cherry picked from commit 73b153a528fb9b64b994c1174882bc2f64b1ed47)", "pr_number": "62143", "files_changed": ["aten/src/ATen/native/cudnn/RNN.cpp"], "labels": ["triaged", "open source", "cla signed", "ciflow/default"]}, "530e7f6195": {"title": "Define check_sizes_nonnegative as inline (#71640)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/71640\n\nMoving this function into the cpp file caused a small regression in\nempty_cpu's callgrind instruction count, so moving it back.\n\nTest Plan: Imported from OSS\n\nReviewed By: mruberry\n\nDifferential Revision: D33712880\n\nPulled By: ngimel\n\nfbshipit-source-id: 64b3cb76308da38a3f0384de69500bea6ce6a30b\n(cherry picked from commit d3791bc986d12a2e995bfb65fed5c35ddf7a9ae6)", "pr_number": "71640", "files_changed": ["aten/src/ATen/EmptyTensor.cpp", "aten/src/ATen/EmptyTensor.h"], "labels": ["open source", "cla signed", "ciflow/default"]}, "40e88b75c4": {"title": "extract out //c10/util:base library (#70854)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/70854\n\nWe can't do the entire package since parts of it depend on //c10/core.\nghstack-source-id: 147170901\n\nTest Plan: Rely on CI.\n\nReviewed By: malfet\n\nDifferential Revision: D33321821\n\nfbshipit-source-id: 6d634da872a382a60548e2eea37a0f9f93c6f080\n(cherry picked from commit 0afa808367ff92b6011b61dcbb398a2a32e5e90d)", "pr_number": "70854", "files_changed": [".bazelrc", "c10/BUILD.bazel", "c10/util/BUILD.bazel", "c10/util/build.bzl", "tools/bazel.bzl"], "labels": ["cla signed", "ciflow/default"]}, "942a084c46": {"title": "Remove state_dict from AveragedModel and use buffers instead (#71763)", "body": "Summary:\nFixes [https://github.com/pytorch/pytorch/issues/66686](https://github.com/pytorch/pytorch/issues/66686)\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/71763\n\nReviewed By: anjali411\n\nDifferential Revision: D33770907\n\nPulled By: prabhat00155\n\nfbshipit-source-id: ee32f2cb8475c9add4e1a9a5d3d784ef95825efc\n(cherry picked from commit a15898b072ae5234c76afa005ec492ed158c51aa)", "pr_number": "71763", "files_changed": ["test/test_optim.py", "torch/optim/swa_utils.py"], "labels": ["module: optimizer", "cla signed", "ciflow/default"]}, "bfc481cf67": {"title": "extract //c10/core:ScalarType to its own library (#70855)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/70855\n\nThis library is depended on by parts of util so it has to go next.\nghstack-source-id: 147170897\n\nTest Plan: Rely on CI.\n\nReviewed By: malfet\n\nDifferential Revision: D33329527\n\nfbshipit-source-id: 28a111f602ee085c1d9b0acec29790488f8c8f0b\n(cherry picked from commit e3601b94ff4a89caeb0c012a0d946613934646b9)", "pr_number": "70855", "files_changed": ["c10/BUILD.bazel", "c10/core/BUILD.bazel", "c10/core/build.bzl"], "labels": ["cla signed", "ciflow/default"]}, "130ca58601": {"title": "extract final two libraries out of //c10/util (#70856)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/70856\n\nghstack-source-id: 147302541\n\nTest Plan: Relying on CI\n\nReviewed By: malfet\n\nDifferential Revision: D33329555\n\nfbshipit-source-id: 1e7884b2df1c294a8fe9e7f3664a139487d27978\n(cherry picked from commit 643cc436ec416ea42a73ec2e376b1d5e747192ac)", "pr_number": "70856", "files_changed": ["c10/BUILD.bazel", "c10/util/build.bzl"], "labels": ["cla signed", "ciflow/default"]}, "c6d885e489": {"title": "extract out //c10/core:base library (#70857)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/70857\n\nghstack-source-id: 147302543\n\nTest Plan: Relying on CI\n\nReviewed By: malfet\n\nDifferential Revision: D33329579\n\nfbshipit-source-id: 961abdecabb7b2c6f090e00a6a670e5b70aa5bca\n(cherry picked from commit 2b8c4bb0a4f6b22e028aa4cfbf06f09fb6873fa3)", "pr_number": "70857", "files_changed": ["c10/BUILD.bazel", "c10/core/build.bzl"], "labels": ["cla signed", "ciflow/default"]}, "0891c908bb": {"title": "Revert D33768645: Set correct device id on efficientzerotensors", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD33768645 (https://github.com/pytorch/pytorch/commit/5dd6cd55ba0ec758617f648439796cbd3a8f88bd)\n\nOriginal commit changeset: 66ce9907630b\n\nOriginal Phabricator Diff: D33768645 (https://github.com/pytorch/pytorch/commit/5dd6cd55ba0ec758617f648439796cbd3a8f88bd)\n\nfbshipit-source-id: 4bb1ad46f01cd33aeb813bdc123741cf665194a8\n(cherry picked from commit 8ca385b1d8f80f7d2d40a1f177f5155c228ab46e)", "pr_number": null, "files_changed": ["aten/src/ATen/native/TensorFactories.cpp", "aten/src/ATen/native/TensorFactories.h", "aten/src/ATen/native/cuda/TensorFactories.cu", "aten/src/ATen/native/native_functions.yaml", "test/test_torch.py"], "labels": []}, "e04ade92ae": {"title": "Skip compiledWithCuDNN() call for mobile to avoid segfault (#71775)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/71775\n\nMobile is running into segfaults at the `compiledWithCuDNN()` call as described in T110194934. This fix works around this with an #ifdef following the approach done [here](https://github.com/pytorch/pytorch/blob/d32b7d9585419caa18b57305ac7fdc547903fbc3/aten/src/ATen/native/Convolution.cpp#L1076-L1088). TBD how to fix the underlying cause.\n\nTest Plan: Imported from OSS\n\nReviewed By: anjali411\n\nDifferential Revision: D33778888\n\nPulled By: jbschlosser\n\nfbshipit-source-id: 2a22b2eaa858ee6adf5b3c25a1c470c6aebc3f87\n(cherry picked from commit e90a6bb402f45f45b7219f453ca38ee85603f3eb)", "pr_number": "71775", "files_changed": ["aten/src/ATen/native/Convolution.cpp"], "labels": ["cla signed", "ciflow/default"]}, "46817895bd": {"title": "[Profiler] Split observer implementations based on ProfilerState (#71135)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/71135\n\nThe NVTX profiler is quite different from the other Kineto cases, so it's worth it to peel it off early so that later logic can assume either KINETO or KINETO_GPU_FALLBACK. This is more important since we're going to change the Kineto internals. (You can see the python tracer was unnecessarily coupled to NVTX just because the control logic was intermingled.)\n\nThere's also no reason to put the legacy observer state in the header rather than the cpp file now that the kineto profiler doesn't need it, so we should shield it from prying eyes.\n\nThe recent headaches with TLS downcasting and RPC integration (D32678163 (https://github.com/pytorch/pytorch/commit/7ea86dfdb162758c9fbbf6807ab1dd778591c062), D33283314 (https://github.com/pytorch/pytorch/commit/681e78bacec69c3ac6653483da2236d0e0416c6e), D33437773 (https://github.com/pytorch/pytorch/commit/7d6535cab39a6277aa0b40cfca3b9c918ef9e095)) have made crystal clear that we need a lot more safety in the profiler, particularly as we shift things around.\n\nTest Plan: Unit tests. This is no longer a performance PR.\n\nReviewed By: aaronenyeshi\n\nDifferential Revision: D32710829\n\nfbshipit-source-id: f9138598b3cfeba71872905a7afab3c03c0d56e7\n(cherry picked from commit 059a39d8e3b184337ddd401cfd242c47b8ad0538)", "pr_number": "71135", "files_changed": ["tools/build_variables.bzl", "torch/csrc/autograd/profiler_kineto.cpp", "torch/csrc/autograd/profiler_legacy.cpp", "torch/csrc/autograd/profiler_legacy.h", "torch/csrc/profiler/api.cpp", "torch/csrc/profiler/api.h", "torch/csrc/profiler/nvtx_observer.cpp", "torch/csrc/profiler/nvtx_observer.h"], "labels": ["fb-exported", "cla signed", "ciflow/default"]}, "b066931106": {"title": "fixing of usage of rel_tol for test adadelta (#71880)", "body": "Summary:\nRecently I made a PR to change some test tolerances: https://github.com/pytorch/pytorch/pull/69919\nIt turns out that the previous decorator does not work with the test optim unit test framework. I have summarized the issue in the following doc:\nhttps://docs.google.com/document/d/1BOrp29r31A2WXwM0O6ydsCs43wi01sAgdduKd7is_ec/edit?usp=sharing\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/71880\n\nReviewed By: cpuhrsch\n\nDifferential Revision: D33801967\n\nPulled By: jbschlosser\n\nfbshipit-source-id: 094feba10e2ee2a94e3ab754e4140e16b634ea09\n(cherry picked from commit d504ddd950f69a6784b93a2e7630d24d5c7051fe)", "pr_number": "71880", "files_changed": ["test/test_optim.py"], "labels": ["open source", "cla signed", "ciflow/default"]}, "cf3ef23713": {"title": "Propagate full autocast state to CheckpointFunction's forward-inside-backward (#71169)", "body": "Summary:\nShould fix https://github.com/pytorch/pytorch/issues/71124 (implements https://github.com/pytorch/pytorch/issues/71124#issuecomment-1009436056).\n\ncc mcarilli ptrblck\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/71169\n\nReviewed By: albanD\n\nDifferential Revision: D33793556\n\nPulled By: ngimel\n\nfbshipit-source-id: 80a4b4f0657b922002e3446fb6b48f082fa98453\n(cherry picked from commit cf9beee28bf7b541b4631c13fa35bb494212e1cd)", "pr_number": "71169", "files_changed": ["torch/utils/checkpoint.py"], "labels": ["module: checkpoint", "open source", "module: amp (automated mixed precision)", "cla signed", "ciflow/default"]}, "0099796978": {"title": "[CUDA Pinned Memory] [Retry] Alternative implementation of pinned memory allocator focusing on multi-threaded scalability (#69299)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/69299\n\nhttps://github.com/pytorch/pytorch/pull/68906 + https://github.com/pytorch/pytorch/pull/68749 plugged one correctness hole (non-blocking copies of offset pinned memory tensors) while introducing another (non-blocking copies of pinned memory tensors with a non-standard DataPtr context).\n\nIn this revision, we use both the tensor data pointer and context to attempt to identify the originating block in the pinned memory allocator.\n\nTest Plan: New unit tests added to cover the missing case previously.\n\nReviewed By: yinghai\n\nDifferential Revision: D32787087\n\nfbshipit-source-id: 0cb0d29d7c39a13f433eb1cd423dc0d2a303c955\n(cherry picked from commit 297157b1a13b5c75d860cac9eba4fe7fe1ad5e6f)", "pr_number": "69299", "files_changed": ["aten/src/ATen/cuda/CachingHostAllocator.cpp", "aten/src/ATen/cuda/CachingHostAllocator.h", "aten/src/ATen/native/cuda/Copy.cu", "aten/src/ATen/test/CMakeLists.txt", "aten/src/ATen/test/cuda_caching_host_allocator_test.cpp", "test/test_cuda.py"], "labels": ["oncall: distributed", "fb-exported", "cla signed", "ciflow/default", "ciflow/cuda"]}, "027c0d7f8e": {"title": "fixed compilations on xla tensor print (#71147)", "body": "Summary:\nFixes multiple compilation on xla tensor print. Please check the conversation here: https://github.com/pytorch/xla/pull/3253\n\nThis is done to avoid compilations during tensor printing. Torch performs some tensor operations like slicing to make the tensor readable. These operations result in compilations. Hence to avoid the compilations, copying the tensor to cpu before printing.\n\nexample:\n\n```\ndev = xm.xla_device()\ndef test_linear(input_shape=(8, 1024)):\n    import pdb\n    pdb.set_trace()\n    linear = torch.nn.Linear(in_features=1024, out_features=4096, bias=True).to(dev)\n    inp = torch.randn(*input_shape).to(dev)\n    output = linear(inp)\n    xm.mark_step()\n    return output\n```\nReturning from this function would have resulted in 63 compiles, since PDB prints the value of the return output. In this case it is a xla tensor.\n\nNow with the current change, there is no compilation.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/71147\n\nReviewed By: shunting314\n\nDifferential Revision: D33795177\n\nPulled By: wconstab\n\nfbshipit-source-id: 74b53d9a1cb7ef67f9d8b0a32064f3896be449b5\n(cherry picked from commit a9e0687fc5c9981fb55ea4dc406c283c80fa20c9)", "pr_number": "71147", "files_changed": ["torch/_tensor_str.py"], "labels": ["triaged", "open source", "cla signed", "ciflow/default"]}, "fc6a488e9a": {"title": "extract out //c10/core:alignment (#70858)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/70858\n\nghstack-source-id: 147642533\n\nTest Plan: Extracted a constant to a new header, trusting CI build to validate.\n\nReviewed By: malfet\n\nDifferential Revision: D33329689\n\nfbshipit-source-id: 8697bb81a5cc3366462ebdf1f214b62d478fa77c\n(cherry picked from commit 16663847e179ea1c2a16f2bb538cfe3aca032593)", "pr_number": "70858", "files_changed": ["c10/BUILD.bazel", "c10/core/CPUAllocator.cpp", "c10/core/CPUAllocator.h", "c10/core/alignment.h", "c10/core/build.bzl"], "labels": ["cla signed", "ciflow/default"]}, "844a4b47df": {"title": "extract out //c10/core:alloc_cpu (#70859)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/70859\n\nghstack-source-id: 147642534\n\nTest Plan: Extracting code unmodified to a new library: relying on CI to validate.\n\nReviewed By: malfet\n\nDifferential Revision: D33329688\n\nfbshipit-source-id: f60327467d197ec1862fb3554f8b83e6c84cab5c\n(cherry picked from commit f82e7c0e9beba1113defe6d55cf8a232551e913b)", "pr_number": "70859", "files_changed": ["c10/BUILD.bazel", "c10/core/CPUAllocator.cpp", "c10/core/CPUAllocator.h", "c10/core/build.bzl", "c10/core/impl/alloc_cpu.cpp", "c10/core/impl/alloc_cpu.h", "c10/mobile/CPUCachingAllocator.cpp", "c10/mobile/CPUCachingAllocator.h", "c10/mobile/CPUProfilingAllocator.cpp", "c10/mobile/CPUProfilingAllocator.h"], "labels": ["cla signed", "ciflow/default"]}, "41690d7804": {"title": "define //c10/mobile targets (#70861)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/70861\n\nghstack-source-id: 147642549\n\nTest Plan: Should be a no-op. Rely on CI to validate.\n\nReviewed By: malfet\n\nDifferential Revision: D33329870\n\nfbshipit-source-id: 7dbccaa994737c5fe7195d02dffd61eeceb19ceb\n(cherry picked from commit 2b5264ebc49e4a5445c066e07f15bad041f42ac8)", "pr_number": "70861", "files_changed": ["c10/BUILD.bazel", "c10/mobile/BUILD.bazel", "c10/mobile/build.bzl"], "labels": ["cla signed", "ciflow/default"]}, "de58a27769": {"title": "define //c10/core:CPUAllocator target (#70862)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/70862\n\nghstack-source-id: 147642558\n\nTest Plan: Should be a no-op, rely on CI to validate.\n\nReviewed By: malfet\n\nDifferential Revision: D33330151\n\nfbshipit-source-id: f566993f47cffa0df85105f3787bb5c6385cf5d6\n(cherry picked from commit a17c3865efb6f1fa7e14adb20e5d5ed441543885)", "pr_number": "70862", "files_changed": ["c10/BUILD.bazel", "c10/core/build.bzl"], "labels": ["cla signed", "ciflow/default"]}, "7aa4a1f63e": {"title": "torch/monitor: TensorboardEventHandler (#71658)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/71658\n\nThis adds the beginnings of a TensorboardEventHandler which will log stats to Tensorboard.\n\nTest Plan: buck test //caffe2/test:monitor\n\nReviewed By: edward-io\n\nDifferential Revision: D33719954\n\nfbshipit-source-id: e9847c1319255ce0d9cf2d85d8b54b7a3c681bd2\n(cherry picked from commit 5c8520a6baea51db02e4e29d0210b3ced60fa18d)", "pr_number": "71658", "files_changed": ["docs/source/monitor.rst", "test/test_monitor.py", "torch/monitor/__init__.py"], "labels": ["fb-exported", "cla signed", "ciflow/default"]}, "fdec94504f": {"title": "Rename _scatter_reduce to scatter_reduce and make it unstructured (#71787)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/71787\n\nTest Plan: Imported from OSS\n\nReviewed By: mikaylagawarecki\n\nDifferential Revision: D33778524\n\nPulled By: cpuhrsch\n\nfbshipit-source-id: 55a330e1c2227c0eaaa1c0d2f9205a4dee24a11b\n(cherry picked from commit 6e4a8a91dac179f2302f87964090a4f991a4392f)", "pr_number": "71787", "files_changed": ["aten/src/ATen/native/TensorAdvancedIndexing.cpp", "aten/src/ATen/native/native_functions.yaml", "test/forward_backward_compatibility/check_forward_backward_compatibility.py", "test/test_torch.py", "torch/overrides.py"], "labels": ["cla signed", "ciflow/default", "ciflow/all"]}, "0cae3c0481": {"title": "Improved error messages for `max_unpool{}d` operators (#67328)", "body": "Summary:\n~As per the title, this PR adds OpInfos for `max_unpoolNd` operators. There are a few TODOs:~\n\n* [x] Improve error messages for the rest of the functions in the CUDA file for the un-pooling operators.\n~* [x] Raise issues for the failures, and provide descriptions.~\n\n~Note to the reviewers: I'll add descriptions and reasons for the skips, I'm not totally sure about them, hence the skips for now.~\n\ncc: mruberry saketh-are\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/67328\n\nReviewed By: george-qi\n\nDifferential Revision: D33818126\n\nPulled By: albanD\n\nfbshipit-source-id: 8ddc8510be7f4ea19eca3ae7f052aeca590d8d48\n(cherry picked from commit bd9903d16ceed7e1a5e0d1ead747df085434a53d)", "pr_number": "67328", "files_changed": ["aten/src/ATen/native/MaxUnpooling.cpp", "aten/src/ATen/native/cpu/MaxUnpoolKernel.cpp", "aten/src/ATen/native/cuda/MaxUnpooling.cu"], "labels": ["triaged", "open source", "cla signed", "module: testing", "ciflow/default"]}, "81d1ce05fd": {"title": "Add complex support for Jiterator, port sinc to Jiterator (#71577)", "body": "Summary:\nI copy-pasted part of std c++ from LLVM, make it a string, and modify it to use it to implement complex support for Jiterator\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/71577\n\nReviewed By: george-qi\n\nDifferential Revision: D33820258\n\nPulled By: ngimel\n\nfbshipit-source-id: 3d4ea834803b99904a79e430f749407635a3cf6d\n(cherry picked from commit f2c3b2a9a5d89099c3752605b7c4394f2d61a00d)", "pr_number": "71577", "files_changed": ["aten/src/ATen/cuda/llvm_basic.cpp", "aten/src/ATen/cuda/llvm_complex.cpp", "aten/src/ATen/cuda/llvm_jit_strings.h", "aten/src/ATen/native/cuda/Loops.cuh", "aten/src/ATen/native/cuda/Math.cuh", "aten/src/ATen/native/cuda/UnarySpecialOpsKernel.cu", "aten/src/ATen/native/cuda/jit_utils.cpp", "aten/src/ATen/native/cuda/jit_utils.h"], "labels": ["triaged", "open source", "cla signed", "ciflow/default"]}, "b62780fc4f": {"title": "[warnings] Disable broken TORCH_CHECK (#71947)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/71947\n\nThis is a recent regression that blocks our migration to turning `-Wstring-conversion` into an error.\nComment it out until albanD can resolve in the future.\n\nTest Plan: compiles locally\n\nReviewed By: stephinphection\n\nDifferential Revision: D33829899\n\nfbshipit-source-id: 47833d0d8dada087d748ee7e500179ff16f2a138\n(cherry picked from commit e3c77ff4458aed174e08a5ec233c606509fb5bc6)", "pr_number": "71947", "files_changed": ["aten/src/ATen/native/MaxUnpooling.cpp"], "labels": ["fb-exported", "cla signed", "ciflow/default"]}, "171cf153d2": {"title": "Make repeat_interleave respect the conj and neg bits. (#68523)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/68523\n\nAs per title.\n\ncc ezyang gchanan anjali411 dylanbespalko mruberry Lezcano nikitaved\n\nTest Plan: Imported from OSS\n\nReviewed By: anjali411\n\nDifferential Revision: D33774421\n\nPulled By: mruberry\n\nfbshipit-source-id: f47918c53c10ebec61198d5926287b711e141643\n(cherry picked from commit 5b8cc5086683dc8a13d24bf52a2880b62587b1f5)", "pr_number": "68523", "files_changed": ["aten/src/ATen/ConjugateFallback.cpp", "aten/src/ATen/native/NegateFallback.cpp", "aten/src/ATen/native/Repeat.cpp"], "labels": ["module: complex", "open source", "cla signed", "ciflow/default"]}, "6cb128c8dd": {"title": "Generalize noncontiguous tests to several outputs (#67996)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/67996\n\nThis is necessary for most matrix decompositions in `linalg`.\n\ncc mruberry\n\nTest Plan: Imported from OSS\n\nReviewed By: anjali411\n\nDifferential Revision: D33774418\n\nPulled By: mruberry\n\nfbshipit-source-id: 576f2dda9d484808b4acf0621514c0ffe26834e6\n(cherry picked from commit fb07c50aa9c143aa9dafab57936a8a8a7d3b4ec4)", "pr_number": "67996", "files_changed": ["test/test_ops.py", "torch/testing/_internal/common_methods_invocations.py", "torch/testing/_internal/common_utils.py"], "labels": ["module: tests", "open source", "cla signed", "module: testing", "ciflow/default"]}, "f115a42362": {"title": "Revert D33805315: [pytorch][PR] Automated submodule update: FBGEMM", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD33805315 (https://github.com/pytorch/pytorch/commit/75cc2184e16677395705db37a3426421b970e752)\n\nOriginal commit changeset: 6c341cdff97b\n\nOriginal Phabricator Diff: D33805315 (https://github.com/pytorch/pytorch/commit/75cc2184e16677395705db37a3426421b970e752)\n\nfbshipit-source-id: 4c5580fbb0258dc8d69b3f321a124568355abc8d\n(cherry picked from commit c891b1cddf83109085f6e5bf11a18925cba08fb6)", "pr_number": null, "files_changed": ["third_party/fbgemm"], "labels": []}, "8548657ddb": {"title": "TransformedDistribution.icdf: Fix erroneous icdf ValueError (#71393)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/66946\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/71393\n\nReviewed By: albanD\n\nDifferential Revision: D33810839\n\nPulled By: neerajprad\n\nfbshipit-source-id: a86d8ce3b196b6b06e1466e4030fc549bc07d332\n(cherry picked from commit 88743f53b2877a3ab43365c6cb5771856bf24967)", "pr_number": "71393", "files_changed": ["test/distributions/test_distributions.py", "torch/distributions/transformed_distribution.py"], "labels": ["triaged", "open source", "cla signed", "ciflow/default"]}, "0c2b1b8bcf": {"title": "Update docs for forward AD and make them public (#71643)", "body": "Summary:\nFollow up: we would need to update the links to the tutorial later\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/71643\n\nReviewed By: albanD\n\nDifferential Revision: D33713982\n\nPulled By: soulitzer\n\nfbshipit-source-id: a314ffa4e7d5c5ebdef9c50033f338b06578d71c\n(cherry picked from commit ba30daaaa5bb79619332f59e6826f19623bc1697)", "pr_number": "71643", "files_changed": ["docs/source/autograd.rst", "torch/autograd/forward_ad.py"], "labels": ["cla signed", "ciflow/default"]}, "fb0e27d38a": {"title": "Add mechanism for functorch to error out on autograd.Function (#71866)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/71866\n\nSee title. There is a minimal perf regression for the non-functorch case\n(a TLS access and a null check).\n\nTest Plan: Imported from OSS\n\nReviewed By: soulitzer\n\nDifferential Revision: D33825279\n\nPulled By: zou3519\n\nfbshipit-source-id: afa2ad5a672cc9225d2bb6b46ee7f3f1513c1e02\n(cherry picked from commit 17ae1d3e9dcf57193a2d90f755e18994671c9f13)", "pr_number": "71866", "files_changed": ["aten/src/ATen/FuncTorchTLS.h", "torch/csrc/autograd/python_function.cpp"], "labels": ["cla signed", "ciflow/default"]}, "fa38e93fe9": {"title": "Add lightweight reparametrization for `_stateless` calls (#68969)", "body": "Summary:\nhttps://github.com/pytorch/pytorch/issues/61447 introduced a mechanism for performing functional calls in a model using the reparametrization API. However, the overhead introduced in a single call was too large.\nI tried to address this by modifying the reparametrization code to support spare tensors, but the changes needed were too large due to type checking and several parts of the code expecting actual `nn.Module` objects so this option was not feasible.\n\nresnet50 and call functional with a parameters dict covering the 0, 25, 50, and 100% of the model total parameters.\n\nUsed script:\nhttps://gist.github.com/emcastillo/f344a58638bd71d130c71c45f86f0c3a\n\n| % of parameters passed | CPU Time (us) | GPU Time (us) |\n|------------------------|---------------|---------------|\n| regular call           | 5539          | 184909        |\n| 0                      | 5561          | 184843        |\n| 25                     | 11363         | 189236        |\n| 50                     | 18716         | 195378        |\n| 75                     | 22851         | 198641        |\n| 100                    | 27441         | 202281        |\n\nThis PR just swaps the `__getattr__` of the submodules to look into a dict holding only the parameters when called, greatly reducing the burden of having to instantiate custom modules and calling forward to just retrieve a tensor.\n\nThe execution times now are as follows:\n\n| % of parameters passed | CPU Time (us) | GPU Time (us) |\n|------------------------|---------------|---------------|\n| regular call           | 5939          | 187533        |\n| 0                      | 5899          | 187570        |\n| 25                     | 8541         | 188953        |\n| 50                     | 10045         | 189826        |\n| 75                     | 11049         | 190344        |\n| 100                    | 11911         | 190800        |\n| functorch with 100% params | 14014 | 191727\n\nNow we see that the CPU time overhead is greatly reduced and the GPU time barely increases due to the effective overlap.\n\ncc albanD zou3519\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/68969\n\nReviewed By: george-qi\n\nDifferential Revision: D33836360\n\nPulled By: albanD\n\nfbshipit-source-id: 532561f64b18ca14c6ae2d77dcacb339397a589d\n(cherry picked from commit fd4b6bdfbff4cb3d1da47b7fd73f1edfe43ba65c)", "pr_number": "68969", "files_changed": ["test/test_stateless.py", "torch/nn/utils/_stateless.py"], "labels": ["triaged", "open source", "cla signed", "ciflow/default"]}, "5735f2f875": {"title": "Make detach redispatch like a regular PyTorch operator (#71707)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/71707\n\nWhy?\n- detach should behave like jax.stop_gradient in functorch. Because it\ndoes not detach all the way through, functorch (as well as a Tensor\nSubclass wrapping a Tensor subclass) won't see it after the first\nlayer/subclass handles it.\n\nHow?\n- This PR changes detach to dispatch all the way through to the backend.\n- This PR also modifies native::detach to call shallow_copy_and_detach\ninstead of native::alias. This is because today, the semantics of detach\nand alias are differently -- they differ only by\nallow_tensor_metadata_change. In the future, we may choose to deprecate\nthis flag.\n- NB: Before and after this PR, detach() shows up twice in\ntorch_dispatch: https://github.com/pytorch/pytorch/issues/71725. This is\nnot a regression so I didn't want to fix it in this PR because it is\nweird to fix.\n\nTest Plan: - added new tests; run existing tests\n\nReviewed By: albanD\n\nDifferential Revision: D33752860\n\nPulled By: zou3519\n\nfbshipit-source-id: 40cc2dc8232e75a02586a4ba5b0ef5f16cb76617\n(cherry picked from commit f88aae426ec00bba907e9ad5d1cd6ed2c40bf14a)", "pr_number": "71707", "files_changed": ["aten/src/ATen/native/TensorShape.cpp", "test/test_autograd.py", "test/test_python_dispatch.py", "torch/csrc/autograd/VariableTypeManual.cpp"], "labels": ["cla signed", "ciflow/default"]}, "de44a50f14": {"title": "index_backward: use out-of-place index_put if any input is subclass (#71779)", "body": "Summary:\nReference: https://github.com/pytorch/functorch/issues/393\n\nContext :\n\nThe derivative of `__getitem__`/`index` is\nhttps://github.com/pytorch/pytorch/blob/f5a71ec2d6956a1ba393657f4b8297bb931eeec2/tools/autograd/derivatives.yaml#L733-L734\n\nwhere `index_backward` is defined as\nhttps://github.com/pytorch/pytorch/blob/f5a71ec2d6956a1ba393657f4b8297bb931eeec2/torch/csrc/autograd/FunctionsManual.cpp#L3892-L3894\n\nProblem arises when `grad` is not BatchedTensor but one of the other input is. In that case, `grad.new_zeros` returns an unbatched tensor and call to the inplace `_index_put_impl_` errors as it expects `zeros_like_self` to be Batched.\n\nTo avoid this, we dispatch to out-of-place `index_put` if any of the input tensor is subclassed otherwise we dispatch to the inplace `_index_put_impl_`.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/71779\n\nReviewed By: albanD\n\nDifferential Revision: D33790596\n\nPulled By: zou3519\n\nfbshipit-source-id: 9d6d81b758740cab7b3db9b905f1e8053f82b835\n(cherry picked from commit ba0407a86ef3cabf885cd127649fa6dcd7f75117)", "pr_number": "71779", "files_changed": ["aten/src/ATen/TensorSubclassLikeUtils.h", "torch/csrc/autograd/FunctionsManual.cpp"], "labels": ["triaged", "open source", "cla signed", "ciflow/default"]}, "0c3bc426a8": {"title": "LTC move squeeze to master (#71677)", "body": "Summary:\nFixes #ISSUE_NUMBER\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/71677\n\nReviewed By: wconstab, alanwaketan\n\nDifferential Revision: D33750874\n\nPulled By: Krovatkin\n\nfbshipit-source-id: 124783a955ae27ded5bbf5a90fa99bb1d599d3f6\n(cherry picked from commit 7cc9293cefb29831127c73689f80f3d44847d18e)", "pr_number": "71677", "files_changed": ["tools/build_variables.bzl", "torch/csrc/lazy/core/view_ops/squeeze.cpp", "torch/csrc/lazy/core/view_ops/squeeze.h"], "labels": ["cla signed", "ciflow/default"]}, "765669e1b9": {"title": "Update docs for torch.real to indicate that it's supported for real tensors (#71962)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/71962\n\nTest Plan: Imported from OSS\n\nReviewed By: davidberard98\n\nDifferential Revision: D33846613\n\nPulled By: anjali411\n\nfbshipit-source-id: a9782bf4e8a7f3ae1fcd4f7ff558ba80b6af012c\n(cherry picked from commit 93ea37800ffaae9cd4e085f7d963ad5fc8ce78fa)", "pr_number": "71962", "files_changed": ["torch/_tensor_docs.py"], "labels": ["cla signed", "ciflow/default"]}, "65d3adc65d": {"title": "Add linspace test modules (#71850)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/71850\n\nTest Plan: none\n\nReviewed By: cccclai\n\nDifferential Revision: D33785383\n\nfbshipit-source-id: 67441df869f8cff8e75aec9adbeff2d31736a879\n(cherry picked from commit 7043ae8f76db0632cd6a5dbb19a06cbc1c9a9a5a)", "pr_number": "71850", "files_changed": ["test/jit/fixtures/test_versioned_linspace_out_v7.ptl", "test/jit/fixtures/test_versioned_linspace_v7.ptl", "test/jit/fixtures_srcs/fixtures_src.py", "test/jit/fixtures_srcs/generate_models.py"], "labels": ["oncall: jit", "fb-exported", "cla signed", "ciflow/default"]}, "57a9b499dc": {"title": "torch/monitor: update pyi definitions (#71950)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/71950\n\nThis updates the .pyi definitions to match the pybind interfaces.\n\nTest Plan:\n```\npyre\n```\n\nCI\n\nReviewed By: kiukchung, edward-io\n\nDifferential Revision: D33830311\n\nfbshipit-source-id: 147b1fbfd242dd9cec1cff05768f7a96d9599af4\n(cherry picked from commit 347a5ebcc34c4583f80ccaa65b194e6f51714475)", "pr_number": "71950", "files_changed": ["torch/_C/_monitor.pyi"], "labels": ["fb-exported", "cla signed", "ciflow/default"]}, "bc9d1e709a": {"title": "[EASY] Adding virtual to the isUnionType op (#69554)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/69554\n\nTest Plan: Imported from OSS\n\nReviewed By: mikaylagawarecki\n\nDifferential Revision: D33750966\n\nPulled By: Gamrix\n\nfbshipit-source-id: d2da8138c72709e6d1359df638ac29ca9d0f1556\n(cherry picked from commit 00f434ee04ca458941b240732c10d006efce69cc)", "pr_number": "69554", "files_changed": ["aten/src/ATen/core/jit_type.h", "aten/src/ATen/core/jit_type_base.h"], "labels": ["cla signed", "ciflow/default"]}, "8ca7484ce7": {"title": "[FIX] Enable TORCH_CHECK again (#71971)", "body": "Summary:\nFrom https://github.com/pytorch/pytorch/pull/71947, the `TORCH_CHECK` should be fixed now.\n\ncc: albanD NSProgrammer\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/71971\n\nReviewed By: mrshenli\n\nDifferential Revision: D33845192\n\nPulled By: albanD\n\nfbshipit-source-id: 020cbe1504ef6dd54f703d7bbc57c2cd22253363\n(cherry picked from commit a154fdfb727f3762a8e1c9c71e48222ecdb3966e)", "pr_number": "71971", "files_changed": ["aten/src/ATen/native/MaxUnpooling.cpp"], "labels": ["open source", "cla signed", "ciflow/default"]}, "7a69752c27": {"title": "Make upgrader test model generation more robust (#72030)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/72030\n\nTest Plan: Imported from OSS\n\nReviewed By: mrshenli\n\nDifferential Revision: D33863263\n\nPulled By: tugsbayasgalan\n\nfbshipit-source-id: 931578848ba530583008be6540003b2dcf4d55ce\n(cherry picked from commit 67cd085104631264eb12c2c808eb4ed7b973a652)", "pr_number": "72030", "files_changed": ["test/jit/fixtures_srcs/generate_models.py"], "labels": ["oncall: jit", "cla signed", "ciflow/default"]}, "815532d40c": {"title": "Unsqueeze ops to reduce the number of reshapes in we use in LTC (#72011)", "body": "Summary:\nFixes #ISSUE_NUMBER\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/72011\n\nReviewed By: gmagogsfm\n\nDifferential Revision: D33855760\n\nPulled By: Krovatkin\n\nfbshipit-source-id: abe5572567c8f7746e7b06a552dfbe5566c3d3ce\n(cherry picked from commit 8eac12685f17a145e1d5d78fcf0d65131248c5c3)", "pr_number": "72011", "files_changed": ["tools/build_variables.bzl", "torch/csrc/lazy/core/view_ops/unsqueeze.cpp", "torch/csrc/lazy/core/view_ops/unsqueeze.h"], "labels": ["cla signed", "ciflow/default"]}, "af65634d1c": {"title": "Move generated keyword out of gen_mobile_upgraders.py (#71938)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/71938\n\n`generated` will trigger the generated changes and hide the file changes. It's also misleading, because `gen_mobile_upgraders.py` itself is not autogen. Separate the keyword out from `gen_mobile_upgraders.py` so it's easier to see the changes from `gen_mobile_upgraders.py`.\nghstack-source-id: 147957825\n\nTest Plan:\n```\nbuck run mode/opt //caffe2/torch/fb/mobile/upgrader_codegen:upgrader_codegen\n```\n\nReviewed By: tugsbayasgalan\n\nDifferential Revision: D33826982\n\nfbshipit-source-id: 593c19f8ef4c9da776b11650863dc43c0b171cd5\n(cherry picked from commit 43038d5bc7a41312a005d62f432c5ca19ed79f21)", "pr_number": "71938", "files_changed": ["tools/codegen/operator_versions/gen_mobile_upgraders.py", "tools/codegen/operator_versions/gen_mobile_upgraders_constant.py"], "labels": ["cla signed", "ciflow/default"]}, "6208c2800e": {"title": "torch/monitor: merge Interval and FixedCount stats (#72009)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/72009\n\nThis simplifies the Stats interface by merging IntervalStat and FixedCountStat into a single Stat w/ a specific window size duration and an optional max samples per window. This allows for the original intention of having comparably sized windows (for statistical purposes) while also having a consistent output bandwidth.\n\nTest Plan:\n```\nbuck test //caffe2/test:monitor //caffe2/test/cpp/monitor:monitor\n```\n\nReviewed By: kiukchung\n\nDifferential Revision: D33822956\n\nfbshipit-source-id: a74782492421be613a1a8b14341b6fb2e8eeb8b4\n(cherry picked from commit 293b94e0b4646521ffe047e5222c4bba7e688464)", "pr_number": "72009", "files_changed": ["docs/source/monitor.rst", "test/cpp/monitor/test_counters.cpp", "test/test_monitor.py", "torch/_C/_monitor.pyi", "torch/csrc/monitor/counters.h", "torch/csrc/monitor/python_init.cpp"], "labels": ["fb-exported", "cla signed", "ciflow/default"]}, "1e4aefaa2f": {"title": "Revert D33834916: Set correct device id on efficientzerotensors", "body": "Test Plan: revert-hammer\n\nDifferential Revision:\nD33834916 (https://github.com/pytorch/pytorch/commit/a18cfb790d0998aa7939c01174fb2f40237897eb)\n\nOriginal commit changeset: 11cec343e95e\n\nOriginal Phabricator Diff: D33834916 (https://github.com/pytorch/pytorch/commit/a18cfb790d0998aa7939c01174fb2f40237897eb)\n\nfbshipit-source-id: 3d3f60b760b445383768161b1d21ea4dadbe5d7c\n(cherry picked from commit eba41aa646461aa341e3a629f668d327581b2f9c)", "pr_number": null, "files_changed": ["aten/src/ATen/native/TensorFactories.cpp", "aten/src/ATen/native/TensorFactories.h", "aten/src/ATen/native/cuda/TensorFactories.cu", "aten/src/ATen/native/native_functions.yaml", "test/test_torch.py", "torch/testing/_internal/common_methods_invocations.py"], "labels": []}, "72c972e1e1": {"title": "Fix bug in linspace model generation (#72027)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/72027\n\nTest Plan: build is successful\n\nReviewed By: cccclai\n\nDifferential Revision: D33858104\n\nfbshipit-source-id: 77a5d4ab15efc21f128efbba1fcce63c6dea8018\n(cherry picked from commit b03b9d6b2f5435afd22ac66155dd8542e55bb5da)", "pr_number": "72027", "files_changed": ["test/jit/fixtures/test_versioned_linspace_out_v7.ptl", "test/jit/fixtures/test_versioned_linspace_v7.ptl", "test/jit/fixtures_srcs/fixtures_src.py"], "labels": ["oncall: jit", "fb-exported", "cla signed", "ciflow/default"]}, "5045c18bd1": {"title": "Error if pocketfft is not found (#67909)", "body": "Summary:\nFixes https://github.com/pytorch/pytorch/issues/67842\n\ncc mruberry peterbell10\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/67909\n\nReviewed By: albanD\n\nDifferential Revision: D33759534\n\nPulled By: malfet\n\nfbshipit-source-id: 03548c95fe233b812b303ce9603c20ff9f626c39\n(cherry picked from commit 214624e254770b1b160d30b000cc244b0c8601b4)", "pr_number": "67909", "files_changed": ["cmake/Dependencies.cmake"], "labels": ["triaged", "open source", "ci/binaries", "module: fft", "cla signed", "ciflow/default", "ciflow/all", "ciflow/mobile"]}, "a1b4410964": {"title": "Add owners to custom test infra (#72080)", "body": "Summary:\nFollowup from meeting today where it is not clear who owned these processes.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/72080\n\nReviewed By: malfet\n\nDifferential Revision: D33898729\n\nPulled By: janeyx99\n\nfbshipit-source-id: 79d0e8210b8a6b9876eb50af448e6967a88d38bf\n(cherry picked from commit 57cd82ef02c8192154d644af317a51d5f6d2f9e8)", "pr_number": "72080", "files_changed": ["CODEOWNERS"], "labels": ["cla signed", "ciflow/default"]}, "d693739248": {"title": "CMake: Clean up unused definitions (#69216)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/69216\n\nCurrently `torch_cpu` has command line arguments relating to cuda\nlibraries e.g. `-DMAGMA_V2`. This happens because\n`include_directories` and `add_definitions` indescriminately change\nthe compile commands of all targets.\n\nInstead creating a proper magma target allows limiting the flags to\njust `torch_cuda`.\n\nTest Plan: Imported from OSS\n\nReviewed By: dagitses\n\nDifferential Revision: D33794174\n\nPulled By: malfet\n\nfbshipit-source-id: 762eabf3b9576bef94e8caa3ed4764c0e2c72b08\n(cherry picked from commit f7d127b654330e3b37a134200571122aab08079b)", "pr_number": "69216", "files_changed": ["aten/src/ATen/CMakeLists.txt", "cmake/Dependencies.cmake", "cmake/Modules/FindMAGMA.cmake", "third_party/sleef.BUILD"], "labels": ["open source", "cla signed", "ciflow/default"]}, "847dbb8684": {"title": "CMake: Clean up unused definitions (#69216)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/69216\n\nThis cleans up 4 pre-processor defines not used by any code:\n- HAVE_GCC_GET_CPUID\n- USE_GCC_GET_CPUID\n- USE_AVX\n- USE_AVX2\n\n`cpuid` isn't used in PyTorch any more, we only use `cpuinfo`.\n`USE_AVX*` is also not used, instead `HAVE_*_CPU_DEFINITIONS` tells\nyou which `CPU_CAPABILITY` flags are being compiled.\n\nThere is also `fbgemm`'s code path adding `third_party` as an include\npath, despite `fbgemm` having a dedicated include directory and a\nCMake setup that properly includes it.\n\nTest Plan: Imported from OSS\n\nReviewed By: albanD\n\nDifferential Revision: D33794424\n\nPulled By: malfet\n\nfbshipit-source-id: 99d504af088818d4a26c2f6ce67ec0d59a5eb703\n(cherry picked from commit 2e099d41f0e2f7d96c6013ac83223a75f4e4f862)", "pr_number": "69216", "files_changed": ["BUILD.bazel", "cmake/Dependencies.cmake", "third_party/mkl-dnn.BUILD", "third_party/sleef.BUILD"], "labels": ["open source", "cla signed", "ciflow/default"]}, "a83cf17807": {"title": "Composite compliance for gather_backward (#71766)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/71766\n\nNo need for a tensorSubclassLike check here (for functorch at least),\nchanging the zeros to new_zeros is sufficient.\n\nTest Plan: - tested with functorch\n\nReviewed By: anjali411\n\nDifferential Revision: D33772752\n\nPulled By: zou3519\n\nfbshipit-source-id: 5779a1c20b032d00a549c58ff905cf768f10467f\n(cherry picked from commit a927c664d601d0b1cbbd3cda7dc297364c1d9e94)", "pr_number": "71766", "files_changed": ["aten/src/ATen/native/TensorAdvancedIndexing.cpp"], "labels": ["cla signed", "ciflow/default"]}, "bb456d2bf7": {"title": "Split cuda: list cpp files that go in _cu library explicitly (#69082)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/69082\n\nTest Plan: Imported from OSS\n\nReviewed By: dagitses, mruberry\n\nDifferential Revision: D32723669\n\nPulled By: malfet\n\nfbshipit-source-id: e9a815b882089dcf1ba76194c728cd7c45377deb\n(cherry picked from commit 456d1ebeb8080e8082b9760fdd072390b55b9b2a)", "pr_number": "69082", "files_changed": ["aten/CMakeLists.txt", "aten/src/ATen/CMakeLists.txt", "caffe2/CMakeLists.txt", "tools/build_variables.bzl"], "labels": ["module: build", "open source", "cla signed", "ciflow/default", "release notes: build"]}, "c93d6f90c9": {"title": "Revert #62143, the new CUDNN_RNN_ALGO_PERSIST_STATIC_SMALL_H algorithm (#72089)", "body": "Summary:\nRevert \"[cuDNN] Add a new optimized cuDNN RNN algorithm for small RNN hidden_size (https://github.com/pytorch/pytorch/issues/62143)\"\n\nThis reverts commit 965b9f483ef99f98af8a5be0e751d41e5ef0efdc.\n\nThis new cudnn RNN algorithm is causing some failures in our internal testings.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/72089\n\nReviewed By: mruberry\n\nDifferential Revision: D33905226\n\nPulled By: ngimel\n\nfbshipit-source-id: 5563a2c275e697477cf79bada3b81a33f1bf2aaa\n(cherry picked from commit 35c240a8dc4ac65add84e30da1dde33402333892)", "pr_number": "72089", "files_changed": ["aten/src/ATen/native/cudnn/RNN.cpp"], "labels": ["open source", "cla signed", "ciflow/default", "topic: not user facing"]}, "1cc824ef59": {"title": "Fix old GCC ABI check in CMake package config (#72081)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/72081\n\nThis PR fixes the libstdc++ ABI check in CMake package configuration file (i.e. `TorchConfig.cmake`) The `_GLIBCXX_USE_CXX11_ABI` flag is a property of `libstdc++`, not GNU compiler collection. In its current form C++ libraries built with Clang on Linux fail since the `torch` CMake target propagates `_GLIBCXX_USE_CXX11_ABI` only when used with gcc.\nghstack-source-id: 148056323\n\nTest Plan: Built a dummy C++ library that depends on libtorch with both gcc and clang on Linux\n\nReviewed By: malfet\n\nDifferential Revision: D33899849\n\nfbshipit-source-id: 3e933b2c7a17d1fba086caa8aaec831223760882\n(cherry picked from commit 41d18c64c4e88db615ecf6f3ef973bd8f985377a)", "pr_number": "72081", "files_changed": ["caffe2/CMakeLists.txt", "cmake/TorchConfig.cmake.in"], "labels": ["cla signed", "ciflow/default"]}, "f20fa66f70": {"title": "Revert \"[fix] max_pool1d: composite compliance (#70900)\" (#71992)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/71992\n\nThis reverts commit b7222e15b6a457099b74420e29b3a39a3e8b5f1a.\n\nWe are conservatively reverting this because it broke a test in functorch.\nThe original PR added a `_max_pool1d_cpu` operator. I'm not sure if it\nis actually safe to revert this due to the addition of the new operator\n(someone may have serialized it between now and then) but because it has\nonly been two weeks this should be fine.\n\nTest Plan: - wait for tests\n\nReviewed By: jbschlosser, VitalyFedyunin\n\nDifferential Revision: D33882918\n\nPulled By: zou3519\n\nfbshipit-source-id: f146e82e6b46690376b3d8825dc7f7da62e2c7de\n(cherry picked from commit 1606333e6ce23d618863a9b0e504352bd55569bc)", "pr_number": "71992", "files_changed": ["aten/src/ATen/native/MaxPooling.cpp", "aten/src/ATen/native/native_functions.yaml", "test/forward_backward_compatibility/check_forward_backward_compatibility.py", "torch/testing/_internal/common_methods_invocations.py"], "labels": ["cla signed", "ciflow/default"]}, "58dabebcd7": {"title": "improve quantized error checking for structured kernels (#71928)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/71928\n\nTest Plan: Imported from OSS\n\nReviewed By: wconstab, bhosmer\n\nDifferential Revision: D33823417\n\nPulled By: bdhirsh\n\nfbshipit-source-id: e894b9724833b77b12963cc4bf194bc6ce526ad9\n(cherry picked from commit 6be10b79e7b3ff59aa8d6ca7cf86fc73f545933a)", "pr_number": "71928", "files_changed": ["c10/core/TensorOptions.h", "tools/codegen/dest/register_dispatch_key.py"], "labels": ["cla signed", "ciflow/default", "topic: not user facing"]}, "44e2b8da28": {"title": "Automated submodule update: FBGEMM (#72068)", "body": "Summary:\nThis is an automated pull request to update the first-party submodule for [pytorch/FBGEMM](https://github.com/pytorch/FBGEMM).\n\nNew submodule commit: https://github.com/pytorch/FBGEMM/commit/35d4dd4eb39f6fceb1bc3a3dafce1ed1f6449bd1\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/72068\n\nTest Plan: Ensure that CI jobs succeed on GitHub before landing.\n\nReviewed By: malfet\n\nDifferential Revision: D33892960\n\nfbshipit-source-id: 462b24ab3a81862bbfdc8e80fe07ea262e11829f\n(cherry picked from commit c5d2b40fa61e185fab1237c07a0ddc875bcb9203)", "pr_number": "72068", "files_changed": ["third_party/fbgemm"], "labels": ["open source", "cla signed", "ciflow/default"]}, "a5e27c45dc": {"title": "Use new_empty in dropout (#72078)", "body": "Summary:\nThis will be needed by functorch to have the expected behavior of randomness:\nDropout generates a tensor of the right size and then calls `bernoulli_` on that. In order to get the expected behavior from ensembled creation, we'll need to make sure that the generated tensor is a batched tensor.This works mostly because most tensors are created as `empty_like` but this one just creates `empty` because it needs a new shape, only for feature dropout. There is also no analogous version in CUDA because this directly calls`_dropout_impl` here (not in native_functions.yaml)\n\nThis shouldn't change the behavior outside of functorch\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/72078\n\nReviewed By: zou3519\n\nDifferential Revision: D33898338\n\nPulled By: samdow\n\nfbshipit-source-id: 9d9ed59d138d732d9647b2771ccf2ea97cffae1c\n(cherry picked from commit e51cf3ebf2c80a65296c7513576042dd58e0de28)", "pr_number": "72078", "files_changed": ["aten/src/ATen/native/Dropout.cpp"], "labels": ["cla signed", "ciflow/default", "topic: not user facing"]}, "7bb614fc71": {"title": "Simplify TensorImpl size check and fix error message (#72070)", "body": "Summary:\nToday, the enum is ignored and the generic assert within the equal function is used leading to no information in the error message when this fails.\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/72070\n\nReviewed By: bdhirsh\n\nDifferential Revision: D33893602\n\nPulled By: albanD\n\nfbshipit-source-id: 4bc644e9232cbf0bafef22d713948915eb6964ff\n(cherry picked from commit bdcc5f5f476f3b9ccd2068f365a734b7df756f02)", "pr_number": "72070", "files_changed": ["c10/core/TensorImpl.h"], "labels": ["cla signed", "ciflow/default"]}, "da0423aa0b": {"title": "[PyTorch] Use a better hash table in CUDACachingAllocator (#71667)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/71667\n\nWe have flat_hash_set because it performs better than std::unordered_set.\nghstack-source-id: 148013648\n\nReviewed By: ngimel\n\nDifferential Revision: D33720595\n\nfbshipit-source-id: aa6077c474dd6fc61ce17e24ebde4056c8bae361\n(cherry picked from commit 386082eaf1d4669c7967ba9cdf765d9d677f5cd9)", "pr_number": "71667", "files_changed": ["c10/cuda/CUDACachingAllocator.cpp"], "labels": ["cla signed", "ciflow/default"]}, "ca2ff12ea3": {"title": "[PyTorch] Remove call_once from CUDACachingAllocator (#71668)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/71668\n\nAs https://en.cppreference.com/w/cpp/thread/call_once mentions, function-local statics are probably more efficient.\nghstack-source-id: 148013646\n\nReviewed By: ngimel\n\nDifferential Revision: D33722954\n\nfbshipit-source-id: a2737c2d6dfdd23b26cbe34574b80e3da0d4b8a4\n(cherry picked from commit a6ddb24558f41aff12f76ba49a28d0a3082aec20)", "pr_number": "71668", "files_changed": ["c10/cuda/CUDACachingAllocator.cpp"], "labels": ["cla signed", "ciflow/default"]}, "4aade95029": {"title": "[PyTorch] Rework stat collection in CUDACachingAllocator (#71669)", "body": "Summary:\nPull Request resolved: https://github.com/pytorch/pytorch/pull/71669\n\nThis was relatively inefficient. Rather than looping for each type of stat we want to update, we now do one loop covering all the stats.\nghstack-source-id: 148013645\n\nReviewed By: ngimel\n\nDifferential Revision: D33725458\n\nfbshipit-source-id: 39ef5d65a73d4ef67f259de8c02c7df29487d990\n(cherry picked from commit 7ca46689b72ba7611517447a292445571bd02dd7)", "pr_number": "71669", "files_changed": ["c10/cuda/CUDACachingAllocator.cpp"], "labels": ["cla signed", "ciflow/default"]}, "1a30954f44": {"title": "CUDA TopK Optimization: use multiple block per slice (#71081)", "body": "Summary:\n# Overview\nCurrently the cuda topk implementation uses only 1 block per slice, which limits the performance for big slices. This PR addresses this issue.\n\nThere are 2 parts in the topk calculation, find the kth value (`radixFindKthValues`) in each slice, then gather topk values (`gatherTopK`) based on the kth value. `radixFindKthValues` kernel now supports multiple blocks. `gatherTopK` may also need a multiple block version (separate PR?).\n\nkthvalue, quantile, median could also use the same code (separate PR).\n\n# Benchmark\n\nBenchmark result with input `x = torch.randn((D1 (https://github.com/pytorch/pytorch/commit/2d884f226365f94833df91de532e3a31b0db310d), D2 (https://github.com/pytorch/pytorch/commit/9b53d3194c55a2094d0bbf908381ab54f89702be)), dtype=torch.float32)` and `k = 2000` on RTX 3080: https://docs.google.com/spreadsheets/d/1BAGDkTCHK1lROtjYSjuu_nLuFkwfs77VpsVPymyO8Gk/edit?usp=sharing\n\nbenchmark plot: left is multiblock, right is dispatched based on heuristics result from the above google sheet.\n<p class=\"img\">\n<img width=49%  src=\"https://user-images.githubusercontent.com/9999318/150860547-7e450ed2-df09-4292-a02a-cb0e1040eebe.png\">\n<img width=49%  src=\"https://user-images.githubusercontent.com/9999318/150860579-672b88ca-e500-4846-825c-65d31d126df4.png\">\n</p>\n\nThe performance of divide-and-conquer implementation at https://github.com/pytorch/pytorch/pull/39850 is not stable in terms of the D1 (https://github.com/pytorch/pytorch/commit/2d884f226365f94833df91de532e3a31b0db310d), D2 (https://github.com/pytorch/pytorch/commit/9b53d3194c55a2094d0bbf908381ab54f89702be) size increasing, for more detail please check the above google sheet.\n\n<p>\n<img width=49%  src=\"https://user-images.githubusercontent.com/9999318/150860563-21d5a5a3-9d6a-4cef-9031-cac4d2d8edee.png\">\n</p>\n\n# cubin binary size\nThe cubin binary size for TensorTopK.cubin (topk) and Sorting.cubin (kthvalue, quantile and etc) has been reduced by removing `#pragma unroll` at [SortingRadixSelect.cuh](https://github.com/pytorch/pytorch/pull/71081/files#diff-df06046dc4a2620f47160e1b16b8566def855c0f120a732e0d26bc1e1327bb90L321) and `largest` template argument without much performance regression.\n\nThe final binary size before and after the PR is\n```\n# master\n-rw-rw-r-- 1 richard richard  18M Jan 24 20:07 TensorTopK.cu.1.sm_86.cubin\n-rw-rw-r-- 1 richard richard  16M Jan 24 20:07 Sorting.cu.1.sm_86.cubin\n# this PR\n-rw-rw-r-- 1 richard richard 5.0M Jan 24 20:11 TensorTopK.cu.1.sm_86.cubin\n-rw-rw-r-- 1 richard richard 2.5M Jan 24 20:11 Sorting.cu.1.sm_86.cubin\n```\n\nscript to extract cubin\n```\n# build with REL_WITH_DEB_INFO=0\n# at pytorch directory\ncubin_path=build/caffe2/CMakeFiles/torch_cuda.dir/__/aten/src/ATen/native/cuda/cubin; mkdir -p $cubin_path; cd $cubin_path; find ../ -type f -name '*cu.o' -exec cuobjdump {} -xelf all \\; ; ls -lh *.cubin -S | head -70\n```\n\n# benchmark script\n```py\nimport torch\nimport time\nimport torch\nimport pandas as pd\nimport numpy as np\nimport torch.utils.benchmark as benchmark\n\ntorch.manual_seed(1)\ndtype = torch.float\ndata = []\n\nfor d1 in [1, 20, 40, 60, 80, 100, 200, 400, 800, 1000, 2000, 4000, 6000, 8000, 10000, 100000, 500000]:\n    if d1 <= 1000:\n        D2 (https://github.com/pytorch/pytorch/commit/9b53d3194c55a2094d0bbf908381ab54f89702be) = [100, 200, 300, 400, 800, 1000, 2000, 3000, 4000, 5000, 8000, 10000, 20000, 30000, 40000, 80000, 100000, 200000, 300000, 400000, 500000]\n    else:\n        D2 (https://github.com/pytorch/pytorch/commit/9b53d3194c55a2094d0bbf908381ab54f89702be) = [100, 200, 300, 400, 800, 1000, 5000, 10000, 20000, 30000]\n    for d2 in D2 (https://github.com/pytorch/pytorch/commit/9b53d3194c55a2094d0bbf908381ab54f89702be):\n        k = 2000 if d2 >= 2000 else d2 // 2\n        print(f\"----------------- D1 (https://github.com/pytorch/pytorch/commit/2d884f226365f94833df91de532e3a31b0db310d) = {d1}, D2 (https://github.com/pytorch/pytorch/commit/9b53d3194c55a2094d0bbf908381ab54f89702be) = {d2} -----------------\")\n        try:\n            x = torch.randn((d1, d2), dtype=dtype, device=\"cuda\")\n            m = benchmark.Timer(\n                stmt='x.topk(k=k, dim=1, sorted=False, largest=True)',\n                globals={'x': x, 'k': k},\n                num_threads=1,\n            ).blocked_autorange(min_run_time=1)\n            print(m)\n            time_ms = m.median * 1000\n        except RuntimeError: # OOM\n            time_ms = -1\n        data.append([d1, d2, k, time_ms])\n\ndf = pd.DataFrame(data=data, columns=['D1 (https://github.com/pytorch/pytorch/commit/2d884f226365f94833df91de532e3a31b0db310d)', 'D2 (https://github.com/pytorch/pytorch/commit/9b53d3194c55a2094d0bbf908381ab54f89702be)', 'k', 'time(ms)'])\nprint(df)\ndf.to_csv('benchmark.csv')\n```\n\nplot script could be found at: https://github.com/yueyericardo/misc/tree/master/share/topk-script\n\ncc zasdfgbnm ngimel\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/71081\n\nReviewed By: albanD\n\nDifferential Revision: D33823002\n\nPulled By: ngimel\n\nfbshipit-source-id: c0482664e9d74f7cafc559a07c6f0b564c9e3ed0\n(cherry picked from commit be367b8d076aebf53ab7511f6a8a86834c76c95b)", "pr_number": "71081", "files_changed": ["aten/src/ATen/cuda/AsmUtils.cuh", "aten/src/ATen/native/cuda/Sorting.cu", "aten/src/ATen/native/cuda/SortingRadixSelect.cuh", "aten/src/ATen/native/cuda/TensorTopK.cpp", "aten/src/ATen/native/cuda/TensorTopK.cu", "aten/src/ATen/native/cuda/TensorTopK.h", "test/test_sort_and_select.py", "torch/utils/hipify/cuda_to_hip_mappings.py"], "labels": ["triaged", "open source", "cla signed", "ciflow/default", "release notes: cuda", "topic: performance"]}, "2c3ecb435e": {"title": "Automated submodule update: FBGEMM (#72116)", "body": "Summary:\nThis is an automated pull request to update the first-party submodule for [pytorch/FBGEMM](https://github.com/pytorch/FBGEMM).\n\nNew submodule commit: https://github.com/pytorch/FBGEMM/commit/1280f817bf89153ed51642ff47b22955228f0050\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/72116\n\nTest Plan: Ensure that CI jobs succeed on GitHub before landing.\n\nReviewed By: jasonjk-park\n\nDifferential Revision: D33919076\n\nfbshipit-source-id: 8d27fd898af101494e4b54f9abfd27e6169cfd4d\n(cherry picked from commit 1731bbd676f8bc739cdb5d9b50cb151816318484)", "pr_number": "72116", "files_changed": ["third_party/fbgemm"], "labels": ["triaged", "open source", "cla signed", "ciflow/default"]}, "4567d5ded4": {"title": "Upgrade oneDNN to v2.5.2 (#71546)", "body": "Summary:\nThis PR upgrades oneDNN to v2.5.2, and includes some building support for oneDNN v2.5.2.\n\nv2.4 changes:\n- Improved performance for future Intel Xeon Scalable processor (code name Sapphire Rapids). The functionality is disabled by default and should be enabled via CPU dispatcher control.\n- Improved binary primitive performance for cases when one of the tensors is broadcasted.\n- Improved performance of reduction primitive, reorder, shuffle primitives.\n- Improved performance of depthwise convolution forward propagation for processors with Intel AVX5-12 support\n- Improved performance of forward inner product primitive for the shapes with minibatch equal to 1 for processors with Intel AVX-512 support\n- Improved performance of int8 matmul and inner product primitives for processors with Intel AVX2 and Intel DL Boost support\n\nv2.5 changes:\n- Improved performance for future Intel Xeon Scalable processors (code name Sapphire Rapids). The functionality is now enabled by default and requires Linux kernel 5.16.\n- Improved performance of matmul primitive for processors with Intel AVX-512 support.\n\nv2.5.2 changes:\n- Fixed performance regression in binary primitive with broadcast\n- Fixed segmentation fault in depthwise convolution primitive for shapes with huge spatial size for processors with Intel AVX-512 support\n\nPull Request resolved: https://github.com/pytorch/pytorch/pull/71546\n\nReviewed By: george-qi\n\nDifferential Revision: D33827108\n\nPulled By: VitalyFedyunin\n\nfbshipit-source-id: 8f5a19b331c82af5b0783f081e061e1034a93952\n(cherry picked from commit 9705212fe9b7b0838cc010d040c37d1175be83ce)", "pr_number": "71546", "files_changed": ["aten/src/ATen/Version.cpp", "cmake/Modules/FindMKLDNN.cmake", "third_party/ideep", "third_party/mkl-dnn.BUILD"], "labels": ["triaged", "open source", "cla signed", "ciflow/default", "intel priority"]}, "e305248a33": {"title": "Add logspace test modules (#72052)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/72052\n\nTest Plan: tofu_demands_tests\n\nReviewed By: cccclai\n\nDifferential Revision: D33885099\n\nfbshipit-source-id: 53eb12c6f4416d35f118a1357b714fec16d74157\n(cherry picked from commit 2608cd937975f938c6bbc9ac5b5f9bf197ff1b95)", "pr_number": "72052", "files_changed": ["test/jit/fixtures/test_versioned_logspace_out_v8.ptl", "test/jit/fixtures/test_versioned_logspace_v8.ptl", "test/jit/fixtures_srcs/fixtures_src.py", "test/jit/fixtures_srcs/generate_models.py"], "labels": ["oncall: jit", "fb-exported", "cla signed", "ciflow/default"]}, "d0f397ae61": {"title": "Avoid unnecessary copy of padding/dilation vectors in check_shape_forward (#72019)", "body": "Summary: Pull Request resolved: https://github.com/pytorch/pytorch/pull/72019\n\nTest Plan: CI + perf measurement\n\nReviewed By: marksantaniello\n\nDifferential Revision: D33846964\n\nfbshipit-source-id: d206387a6efd16005e0cda75da9fd5fac40b405b\n(cherry picked from commit 02bbdd78d2353d641622acd071b26dc7686a9e5c)", "pr_number": "72019", "files_changed": ["aten/src/ATen/native/Convolution.cpp"], "labels": ["fb-exported", "cla signed", "ciflow/default"]}}